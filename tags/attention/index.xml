<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Attention on MookStudy</title>
    <link>https://mookjsi.github.io/tags/attention/</link>
    <description>Recent content in Attention on MookStudy</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <copyright>2025 Jungmook Kang</copyright>
    <lastBuildDate>Wed, 30 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mookjsi.github.io/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Max-Margin Token Selection in Attention Mechanism</title>
      <link>https://mookjsi.github.io/posts/paper-review-maxtoken/</link>
      <pubDate>Wed, 30 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://mookjsi.github.io/posts/paper-review-maxtoken/</guid>
      <description>This post reviews the NeurIPS 2025 paper &amp;lsquo;Max-Margin Token Selection in Attention Mechanism.&amp;rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs.</description>
    </item>
  </channel>
</rss>
