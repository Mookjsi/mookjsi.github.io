<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Max-Margin Token Selection in Attention Mechanism | MookStudy</title><meta name=keywords content="Paper Review,Attention,Implicit Bias,Transformer,NeurIPS 2025"><meta name=description content="This post reviews the NeurIPS 2025 paper &lsquo;Max-Margin Token Selection in Attention Mechanism.&rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-maxtoken/><link crossorigin=anonymous href=/assets/css/stylesheet.11ee013dd5a386759d3b4c965ae95ae1ca0f4ee553d0b1703ffeb46d15507aee.css integrity="sha256-Ee4BPdWjhnWdO0yWWula4coPTuVT0LFwP/60bRVQeu4=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-maxtoken/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-maxtoken/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Max-Margin Token Selection in Attention Mechanism"><meta property="og:description" content="This post reviews the NeurIPS 2025 paper ‘Max-Margin Token Selection in Attention Mechanism.’ The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-30T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Attention"><meta property="article:tag" content="Implicit Bias"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="NeurIPS 2025"><meta name=twitter:card content="summary"><meta name=twitter:title content="Max-Margin Token Selection in Attention Mechanism"><meta name=twitter:description content="This post reviews the NeurIPS 2025 paper &lsquo;Max-Margin Token Selection in Attention Mechanism.&rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Max-Margin Token Selection in Attention Mechanism","item":"https://mookjsi.github.io/posts/paper-review-maxtoken/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Max-Margin Token Selection in Attention Mechanism","name":"Max-Margin Token Selection in Attention Mechanism","description":"This post reviews the NeurIPS 2025 paper \u0026lsquo;Max-Margin Token Selection in Attention Mechanism.\u0026rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs.","keywords":["Paper Review","Attention","Implicit Bias","Transformer","NeurIPS 2025"],"articleBody":"Hello everyone, I’m Jungmook Kang at Yonsei University. Today, I’m excited to share a review of a fascinating paper that was recently accepted to NeurIPS, which delves into the theoretical underpinnings of the attention mechanism. Let’s get started.\nThe title of the paper is “Max-Margin Token Selection in Attention Mechanism.” This work comes from our lab here at Yonsei University, and it explores the question of why attention works the way it does.\nSo, let’s begin with the motivation for this research. What prompted us to look into this?\nWe all know that the attention mechanism is incredibly effective. For a sentence like “The pizza came out of the oven and it tasted good!”, attention can correctly associate “it” with “pizza.” It works remarkably well… but the story doesn’t end there.\nWhen we look at the standard Transformer architecture, which powers models like BERT and GPT, we see a complex system of encoders and decoders with multiple layers of multi-head attention. This raises some fundamental questions: Why is this architecture so successful? And more specifically, what is happening to the model’s weights during the optimization process that leads to this success?\nAnalyzing this architecture theoretically is very challenging. The optimization problem is both non-linear, due to functions like Softmax, and non-convex. This makes it extremely difficult to describe the training dynamics with traditional methods.\nTo tackle this, we turn to the concept of “Implicit Bias.” This is a phenomenon where the optimization algorithm itself—like Gradient Descent—has a preference for a certain type of solution, even when there’s no explicit regularization term in the loss function telling it to do so.\nThis leads us to the central question of our research: What is the implicit bias of the attention model? What kind of solution does it naturally favor?\nWe can get a clue from simpler models. It’s a known result from Soudry et al. (2018) [cite_start]that when you train a linear model with logistic loss using gradient descent, the solution tends to converge towards the one you’d get from a hard-margin Support Vector Machine (SVM)[cite: 78, 79]. The goal of a hard-margin SVM is to find the hyperplane that maximizes the distance, or margin, to the nearest data points.\nSo, we formed a hypothesis: Could it be that the implicit bias of the much more complex attention model is also related to a hard-margin SVM solution?\nHere’s my core idea. [cite_start]We observe that attention has a tendency to focus on a small number of important tokens[cite: 84]. [cite_start]I believe the principle guiding this selection is related to maximizing a margin[cite: 85]. [cite_start]For instance, to distinguish between a cat and a dog, you might focus on the shape of the ’ears’ because that feature provides the clearest separation, the largest ‘margin,’ between the two classes[cite: 87].\nThis brings us to the first main part of the analysis, where we explore the idea of global and local margin maximization within the attention mechanism.\n[cite_start]To make the analysis tractable, we start with a simplified Softmax Attention model[cite: 89]. The function f(X) takes an input sequence X and computes a final output. [cite_start]The key learnable parameters are p, the query embedding, and W, the key-query weights[cite: 92, 94, 95]. [cite_start]In Transformers, p can be thought of as the embedding for the [CLS] token[cite: 98].\n[cite_start]Our training objective is to minimize the empirical risk for a binary classification task[cite: 102]. [cite_start]We use a standard setup with input sequences X and labels Y, and a decreasing loss function like logistic loss[cite: 101, 106]. [cite_start]The model’s prediction f(Xi) is a function of the learnable parameters v, p, and W[cite: 105].\nHere, I’ll just quickly go over some of the mathematical notation we use throughout the paper. [cite_start]We use standard conventions for vectors and matrices, and we’ll use simplified notations like L(p) to denote the risk when v and W are fixed[cite: 109, 110, 116].\n[cite_start]Our approach involves studying the regularization path, which is the path of the solution p as we increase its allowed norm, R[cite: 120, 121]. [cite_start]This path mirrors the trajectory of gradient descent[cite: 121]. [cite_start]We analyze a final attention model where the trainable parameters W and p jointly influence the softmax, leading to similar optimization dynamics[cite: 130].\nThis brings us to Lemma 1, which is a crucial simplification. [cite_start]It states that the weight matrix W is effectively a rank-1 matrix and its learning dynamics are completely determined by the vector p[cite: 135, 138]. [cite_start]This is very important because it means we can fix W and focus our entire analysis on the optimization of p[cite: 137, 139].\nSo, we can formally define our problem. [cite_start]We are exploring the training risk L(v, p) where the key embeddings Ki are derived from the input Xi[cite: 141, 142]. [cite_start]For the analysis, we can even treat Xi and Ki as separate entities[cite: 144].\nTo ensure our proofs hold, we need Assumption A. [cite_start]This is a standard assumption that requires our loss function l to be well-behaved—specifically, it must be strictly decreasing and have a Lipschitz-continuous derivative[cite: 146, 147]. [cite_start]Common functions like logistic loss and exponential loss satisfy this condition[cite: 148].\n[cite_start]Now, we introduce the central piece of our theory: a hard-margin SVM problem, which we call ATT-SVM[cite: 153]. [cite_start]The goal here is to find a direction p that, for each input sequence, separates one chosen token (k_iαi) from all other tokens (k_it) by a margin of at least 1[cite: 155]. [cite_start]We will show that the optimization of softmax attention is effectively trying to solve this problem[cite: 154].\n[cite_start]To formalize this, we define the score of a token (γ_it) as its contribution to the final correct prediction[cite: 159]. [cite_start]The optimal tokens are naturally the ones with the highest scores for each input[cite: 160]. [cite_start]The Globally-Optimal Max-Margin (GMM) direction, denoted p^mm*, is then the solution to our ATT-SVM problem when we choose these optimal tokens[cite: 161].\nTheorem 1 is our first main result. [cite_start]It states that the regularization path—the sequence of solutions as we increase the norm R—converges in direction to this GMM solution, p^mm*[cite: 163]. In simple terms, as the optimization progresses and the norm of p grows, its direction aligns with the direction that best separates the highest-scoring tokens from the rest.\nTheorem 2 looks at the convergence of gradient descent directly. [cite_start]It says that under our assumptions, the norm of the weight vector p will grow towards infinity during training[cite: 168]. [cite_start]Critically, for the simple case of a single training example (n=1), the direction of p converges to the GMM direction[cite: 169, 172]. [cite_start]However, for n \u003e 1, it’s possible for the optimization to get trapped in a local minimum[cite: 173].\nTo test these theorems, we designed a simple experiment. [cite_start]We created synthetic data with three tokens, where we can visualize their key embeddings in 2D and independently control their scores using a third dimension[cite: 175, 179, 184, 190]. This lets us clearly see what’s happening.\nThese plots show the results. [cite_start]In Figure (a), where the conditions for global convergence are met, we see that all gradient descent trajectories (the grey arrows) correctly converge to the GMM direction (the red dashed line)[cite: 225, 238]. [cite_start]In (b), we’ve changed the scores so that a locally optimal solution exists, and we see some trajectories get stuck there instead[cite: 226, 239]. [cite_start]Figure (c) shows the more complex scenario with multiple inputs, where finding the joint GMM solution is harder[cite: 234, 240].\nThis brings us to the idea of locally-optimal tokens. [cite_start]These are tokens that might not be the highest-scoring ones globally, but they form a stable solution for the ATT-SVM problem[cite: 243]. [cite_start]The direction associated with them is a locally-optimal max-margin (LMM) direction[cite: 243].\nTo talk about local convergence, we introduce the geometric concept of a cone. [cite_start]A cone around a direction q is simply the set of all vectors that are “close” in angle to q[cite: 247]. The diagram illustrates an initialization p(0) that lies within the cone of an LMM direction p^mm.\nTheorem 3 formalizes local convergence. [cite_start]It states that if you initialize the gradient descent p(0) within the cone of an LMM direction, the optimization will stay within that cone and ultimately converge to that specific LMM direction[cite: 260].\n[cite_start]Looking back at our experiment in Figure (b), we can now understand it better through Theorem 3. The trajectories that don’t find the global optimum are the ones that were initialized inside the cone of the locally-optimal solution (the blue square), and as the theorem predicts, they converge to it[cite: 308].\nTheorem 4 provides a tightness guarantee. It essentially says that the LMM directions are the only stable convergence points. [cite_start]If you start in any direction q that is not an LMM direction, the optimization path will eventually move away from q[cite: 312, 318].\n[cite_start]This diagram illustrates Theorem 4. If our initial parameter p(0) is in a cone (the grey one) that does not contain any LMM or GMM direction, the optimization process will not converge in that direction[cite: 326, 327]. It is implicitly forced to seek out one of the valid max-margin solutions.\n[cite_start]To summarize the first half: we’ve shown that when training only the attention parameter p, the implicit bias of gradient descent drives the solution towards a hard-margin SVM that separates tokens[cite: 329]. [cite_start]Now, what happens if we also learn the prediction head v at the same time? [cite: 330]\nThis leads to the second part of our analysis: the joint convergence of the prediction head v and the attention weights p.\n[cite_start]The high-level intuition is that the model is linear in v, so the optimization with respect to v also has an implicit bias towards a standard max-margin classifier solution[cite: 335]. [cite_start]The features for this classifier, x_i^p, are the outputs of the attention layer[cite: 336].\n[cite_start]The challenge here is that the features r that v sees are determined by p, which itself is changing during training[cite: 352]. This creates a coupled dynamic. [cite_start]We analyze this by separating the problem into cases based on whether the selected features are support vectors for the v-classifier or not[cite: 353].\nTheorem 5 addresses the case where we assume the selected tokens are all support vectors. [cite_start]We introduce Assumption C, which formalizes that if the attention doesn’t perfectly select the optimal token, the classification margin for v shrinks[cite: 355, 363]. [cite_start]Under this assumption, we find that v converges to the direction of a standard SVM solution, and p converges to the direction of our ATT-SVM solution[cite: 360, 361].\n[cite_start]This raises a natural question: Does every selected token really need to be a support vector, sitting right on the margin? [cite: 366] That seems like a very strict condition.\nIntuitively, for the data points that are not support vectors, we don’t need to maximize their margin. [cite_start]We just need to make sure they are on the correct side of the decision boundary[cite: 367]. This allows us to define a “relaxed” version of our ATT-SVM problem.\nTheorem 6 presents this more general result. [cite_start]The ATT-SVM problem is relaxed: for tokens corresponding to support vectors (i ∈ S), we require a margin of 1, but for non-support vectors (i ∈ S-bar), we only require a margin of 0 (correct classification)[cite: 370]. [cite_start]We only need Assumption C to hold for the support vectors[cite: 374]. [cite_start]The result is that v still converges to the max-margin v^mm, and p converges to the solution of this new, relaxed SVM problem, p^relax[cite: 373].\nWe ran experiments for this joint convergence case as well. [cite_start]The plots show the trajectories for the attention weights p and the classifier head v[cite: 423]. [cite_start]Plot (a) shows a case aligning with Theorem 5, where all inputs are support vectors[cite: 422]. [cite_start]Plot (b) shows a case for Theorem 6, where one input is not a support vector[cite: 422]. [cite_start]Plot (c) shows that the softmax probability for the optimal token quickly goes to 1, confirming that the attention is learning to select specific tokens as predicted[cite: 424].\n[cite_start]So, to summarize the entire theoretical part: Whether we train the attention weights p alone or jointly with the classifier head v, the implicit bias of the optimization consistently pushes the model towards a hard-margin SVM solution[cite: 426, 427]. This provides a strong theoretical explanation for why attention learns to become sparse and focus on the most discriminative tokens.\nFinally, let’s look at a few more experiments to see these principles in action on more complex tasks.\nHere, we trained a vision transformer on an image classification task. [cite_start]Figure 7 shows that as training progresses over epochs, the sparsity of the attention map increases (red curve goes down), meaning the model learns to focus on fewer, more important image patches[cite: 479, 480]. [cite_start]At the same time, the norm of the attention weights ||W|| steadily increases (blue curve), which is exactly what our theory predicts will happen as the solution converges towards an infinite-norm, max-margin boundary[cite: 481]. [cite_start]The images in Figure 6 visually show this focusing effect over time[cite: 477].\nThis final experiment shows how the choice of loss function affects the dynamics. We compare a correlation loss (l(x) = -x) with the logistic loss. [cite_start]The gradient’s magnitude depends on the token’s score γ differently for each loss[cite: 507, 513]. [cite_start]For correlation loss, larger scores get larger gradients[cite: 510], while for logistic loss, the gradient is largest for scores near zero. This results in different trajectories, but as you can see, both are ultimately guided by the same underlying max-margin principle, pushing towards a separating hyperplane.\n","wordCount":"2236","inLanguage":"en","datePublished":"2025-04-30T00:00:00Z","dateModified":"2025-04-30T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-maxtoken/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Max-Margin Token Selection in Attention Mechanism</h1><div class=post-meta><span title='2025-04-30 00:00:00 +0000 UTC'>April 30, 2025</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2236 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>Hello everyone, I&rsquo;m Jungmook Kang at Yonsei University. Today, I&rsquo;m excited to share a review of a fascinating paper that was recently accepted to NeurIPS, which delves into the theoretical underpinnings of the attention mechanism. Let&rsquo;s get started.</p><hr><p><img alt="Slide 1" loading=lazy src=/images/posts/maxtoken/slide-01.jpg></p><blockquote><p>The title of the paper is &ldquo;Max-Margin Token Selection in Attention Mechanism.&rdquo; This work comes from our lab here at Yonsei University, and it explores the question of <em>why</em> attention works the way it does.</p></blockquote><p><img alt="Slide 2" loading=lazy src=/images/posts/maxtoken/slide-02.jpg></p><blockquote><p>So, let&rsquo;s begin with the motivation for this research. What prompted us to look into this?</p></blockquote><p><img alt="Slide 3" loading=lazy src=/images/posts/maxtoken/slide-03.jpg></p><blockquote><p>We all know that the attention mechanism is incredibly effective. For a sentence like &ldquo;The pizza came out of the oven and it tasted good!&rdquo;, attention can correctly associate &ldquo;it&rdquo; with &ldquo;pizza.&rdquo; It works remarkably well&mldr; but the story doesn&rsquo;t end there.</p></blockquote><p><img alt="Slide 4" loading=lazy src=/images/posts/maxtoken/slide-04.jpg></p><blockquote><p>When we look at the standard Transformer architecture, which powers models like BERT and GPT, we see a complex system of encoders and decoders with multiple layers of multi-head attention. This raises some fundamental questions: Why is this architecture so successful? And more specifically, what is happening to the model&rsquo;s weights during the optimization process that leads to this success?</p></blockquote><p><img alt="Slide 5" loading=lazy src=/images/posts/maxtoken/slide-05.jpg></p><blockquote><p>Analyzing this architecture theoretically is very challenging. The optimization problem is both non-linear, due to functions like Softmax, and non-convex. This makes it extremely difficult to describe the training dynamics with traditional methods.</p></blockquote><p><img alt="Slide 6" loading=lazy src=/images/posts/maxtoken/slide-06.jpg></p><blockquote><p>To tackle this, we turn to the concept of <strong>&ldquo;Implicit Bias.&rdquo;</strong> This is a phenomenon where the optimization algorithm itself—like Gradient Descent—has a preference for a certain type of solution, even when there&rsquo;s no explicit regularization term in the loss function telling it to do so.</p></blockquote><p><img alt="Slide 7" loading=lazy src=/images/posts/maxtoken/slide-07.jpg></p><blockquote><p>This leads us to the central question of our research: What is the implicit bias of the attention model? What kind of solution does it naturally favor?</p></blockquote><p><img alt="Slide 8" loading=lazy src=/images/posts/maxtoken/slide-08.jpg></p><blockquote><p>We can get a clue from simpler models. It&rsquo;s a known result from Soudry et al. (2018) [cite_start]that when you train a linear model with logistic loss using gradient descent, the solution tends to converge towards the one you&rsquo;d get from a <strong>hard-margin Support Vector Machine (SVM)</strong>[cite: 78, 79]. The goal of a hard-margin SVM is to find the hyperplane that maximizes the distance, or margin, to the nearest data points.</p></blockquote><p><img alt="Slide 9" loading=lazy src=/images/posts/maxtoken/slide-09.jpg></p><blockquote><p>So, we formed a hypothesis: Could it be that the implicit bias of the much more complex attention model is also related to a hard-margin SVM solution?</p></blockquote><p><img alt="Slide 10" loading=lazy src=/images/posts/maxtoken/slide-10.jpg></p><blockquote><p>Here&rsquo;s my core idea. [cite_start]We observe that attention has a tendency to focus on a small number of important tokens[cite: 84]. [cite_start]I believe the principle guiding this selection is related to maximizing a margin[cite: 85]. [cite_start]For instance, to distinguish between a cat and a dog, you might focus on the shape of the &rsquo;ears&rsquo; because that feature provides the clearest separation, the largest &lsquo;margin,&rsquo; between the two classes[cite: 87].</p></blockquote><p><img alt="Slide 11" loading=lazy src=/images/posts/maxtoken/slide-11.jpg></p><blockquote><p>This brings us to the first main part of the analysis, where we explore the idea of global and local margin maximization within the attention mechanism.</p></blockquote><p><img alt="Slide 12" loading=lazy src=/images/posts/maxtoken/slide-12.jpg></p><blockquote><p>[cite_start]To make the analysis tractable, we start with a simplified Softmax Attention model[cite: 89]. The function <code>f(X)</code> takes an input sequence <code>X</code> and computes a final output. [cite_start]The key learnable parameters are <code>p</code>, the query embedding, and <code>W</code>, the key-query weights[cite: 92, 94, 95]. [cite_start]In Transformers, <code>p</code> can be thought of as the embedding for the <code>[CLS]</code> token[cite: 98].</p></blockquote><p><img alt="Slide 13" loading=lazy src=/images/posts/maxtoken/slide-13.jpg></p><blockquote><p>[cite_start]Our training objective is to minimize the empirical risk for a binary classification task[cite: 102]. [cite_start]We use a standard setup with input sequences <code>X</code> and labels <code>Y</code>, and a decreasing loss function like logistic loss[cite: 101, 106]. [cite_start]The model&rsquo;s prediction <code>f(Xi)</code> is a function of the learnable parameters <code>v</code>, <code>p</code>, and <code>W</code>[cite: 105].</p></blockquote><p><img alt="Slide 14" loading=lazy src=/images/posts/maxtoken/slide-14.jpg></p><blockquote><p>Here, I&rsquo;ll just quickly go over some of the mathematical notation we use throughout the paper. [cite_start]We use standard conventions for vectors and matrices, and we&rsquo;ll use simplified notations like <code>L(p)</code> to denote the risk when <code>v</code> and <code>W</code> are fixed[cite: 109, 110, 116].</p></blockquote><p><img alt="Slide 15" loading=lazy src=/images/posts/maxtoken/slide-15.jpg></p><blockquote><p>[cite_start]Our approach involves studying the <strong>regularization path</strong>, which is the path of the solution <code>p</code> as we increase its allowed norm, <code>R</code>[cite: 120, 121]. [cite_start]This path mirrors the trajectory of gradient descent[cite: 121]. [cite_start]We analyze a final attention model where the trainable parameters <code>W</code> and <code>p</code> jointly influence the softmax, leading to similar optimization dynamics[cite: 130].</p></blockquote><p><img alt="Slide 16" loading=lazy src=/images/posts/maxtoken/slide-16.jpg></p><blockquote><p>This brings us to <strong>Lemma 1</strong>, which is a crucial simplification. [cite_start]It states that the weight matrix <code>W</code> is effectively a rank-1 matrix and its learning dynamics are completely determined by the vector <code>p</code>[cite: 135, 138]. [cite_start]This is very important because it means we can fix <code>W</code> and focus our entire analysis on the optimization of <code>p</code>[cite: 137, 139].</p></blockquote><p><img alt="Slide 17" loading=lazy src=/images/posts/maxtoken/slide-17.jpg></p><blockquote><p>So, we can formally define our problem. [cite_start]We are exploring the training risk <code>L(v, p)</code> where the key embeddings <code>Ki</code> are derived from the input <code>Xi</code>[cite: 141, 142]. [cite_start]For the analysis, we can even treat <code>Xi</code> and <code>Ki</code> as separate entities[cite: 144].</p></blockquote><p><img alt="Slide 18" loading=lazy src=/images/posts/maxtoken/slide-18.jpg></p><blockquote><p>To ensure our proofs hold, we need <strong>Assumption A</strong>. [cite_start]This is a standard assumption that requires our loss function <code>l</code> to be well-behaved—specifically, it must be strictly decreasing and have a Lipschitz-continuous derivative[cite: 146, 147]. [cite_start]Common functions like logistic loss and exponential loss satisfy this condition[cite: 148].</p></blockquote><p><img alt="Slide 19" loading=lazy src=/images/posts/maxtoken/slide-19.jpg></p><blockquote><p>[cite_start]Now, we introduce the central piece of our theory: a <strong>hard-margin SVM problem</strong>, which we call <strong>ATT-SVM</strong>[cite: 153]. [cite_start]The goal here is to find a direction <code>p</code> that, for each input sequence, separates one chosen token (<code>k_iαi</code>) from all other tokens (<code>k_it</code>) by a margin of at least 1[cite: 155]. [cite_start]We will show that the optimization of softmax attention is effectively trying to solve this problem[cite: 154].</p></blockquote><p><img alt="Slide 20" loading=lazy src=/images/posts/maxtoken/slide-20.jpg></p><blockquote><p>[cite_start]To formalize this, we define the <strong>score</strong> of a token (<code>γ_it</code>) as its contribution to the final correct prediction[cite: 159]. [cite_start]The <strong>optimal tokens</strong> are naturally the ones with the highest scores for each input[cite: 160]. [cite_start]The <strong>Globally-Optimal Max-Margin (GMM) direction</strong>, denoted <code>p^mm*</code>, is then the solution to our ATT-SVM problem when we choose these optimal tokens[cite: 161].</p></blockquote><p><img alt="Slide 21" loading=lazy src=/images/posts/maxtoken/slide-21.jpg></p><blockquote><p><strong>Theorem 1</strong> is our first main result. [cite_start]It states that the regularization path—the sequence of solutions as we increase the norm <code>R</code>—converges in direction to this GMM solution, <code>p^mm*</code>[cite: 163]. In simple terms, as the optimization progresses and the norm of <code>p</code> grows, its direction aligns with the direction that best separates the highest-scoring tokens from the rest.</p></blockquote><p><img alt="Slide 22" loading=lazy src=/images/posts/maxtoken/slide-22.jpg></p><blockquote><p><strong>Theorem 2</strong> looks at the convergence of gradient descent directly. [cite_start]It says that under our assumptions, the norm of the weight vector <code>p</code> will grow towards infinity during training[cite: 168]. [cite_start]Critically, for the simple case of a single training example (<code>n=1</code>), the direction of <code>p</code> converges to the GMM direction[cite: 169, 172]. [cite_start]However, for <code>n > 1</code>, it&rsquo;s possible for the optimization to get trapped in a local minimum[cite: 173].</p></blockquote><p><img alt="Slide 23" loading=lazy src=/images/posts/maxtoken/slide-23.jpg></p><blockquote><p>To test these theorems, we designed a simple experiment. [cite_start]We created synthetic data with three tokens, where we can visualize their key embeddings in 2D and independently control their scores using a third dimension[cite: 175, 179, 184, 190]. This lets us clearly see what&rsquo;s happening.</p></blockquote><p><img alt="Slide 24" loading=lazy src=/images/posts/maxtoken/slide-24.jpg></p><blockquote><p>These plots show the results. [cite_start]In Figure (a), where the conditions for global convergence are met, we see that all gradient descent trajectories (the grey arrows) correctly converge to the GMM direction (the red dashed line)[cite: 225, 238]. [cite_start]In (b), we&rsquo;ve changed the scores so that a locally optimal solution exists, and we see some trajectories get stuck there instead[cite: 226, 239]. [cite_start]Figure (c) shows the more complex scenario with multiple inputs, where finding the joint GMM solution is harder[cite: 234, 240].</p></blockquote><p><img alt="Slide 25" loading=lazy src=/images/posts/maxtoken/slide-25.jpg></p><blockquote><p>This brings us to the idea of <strong>locally-optimal tokens</strong>. [cite_start]These are tokens that might not be the highest-scoring ones globally, but they form a stable solution for the ATT-SVM problem[cite: 243]. [cite_start]The direction associated with them is a <strong>locally-optimal max-margin (LMM)</strong> direction[cite: 243].</p></blockquote><p><img alt="Slide 26" loading=lazy src=/images/posts/maxtoken/slide-26.jpg></p><blockquote><p>To talk about local convergence, we introduce the geometric concept of a <strong>cone</strong>. [cite_start]A cone around a direction <code>q</code> is simply the set of all vectors that are &ldquo;close&rdquo; in angle to <code>q</code>[cite: 247]. The diagram illustrates an initialization <code>p(0)</code> that lies within the cone of an LMM direction <code>p^mm</code>.</p></blockquote><p><img alt="Slide 27" loading=lazy src=/images/posts/maxtoken/slide-27.jpg></p><blockquote><p><strong>Theorem 3</strong> formalizes local convergence. [cite_start]It states that if you initialize the gradient descent <code>p(0)</code> within the cone of an LMM direction, the optimization will stay within that cone and ultimately converge to that specific LMM direction[cite: 260].</p></blockquote><p><img alt="Slide 28" loading=lazy src=/images/posts/maxtoken/slide-28.jpg></p><blockquote><p>[cite_start]Looking back at our experiment in Figure (b), we can now understand it better through Theorem 3. The trajectories that don&rsquo;t find the global optimum are the ones that were initialized inside the cone of the locally-optimal solution (the blue square), and as the theorem predicts, they converge to it[cite: 308].</p></blockquote><p><img alt="Slide 29" loading=lazy src=/images/posts/maxtoken/slide-29.jpg></p><blockquote><p><strong>Theorem 4</strong> provides a tightness guarantee. It essentially says that the LMM directions are the <em>only</em> stable convergence points. [cite_start]If you start in any direction <code>q</code> that is not an LMM direction, the optimization path will eventually move away from <code>q</code>[cite: 312, 318].</p></blockquote><p><img alt="Slide 30" loading=lazy src=/images/posts/maxtoken/slide-30.jpg></p><blockquote><p>[cite_start]This diagram illustrates Theorem 4. If our initial parameter <code>p(0)</code> is in a cone (the grey one) that does not contain any LMM or GMM direction, the optimization process will not converge in that direction[cite: 326, 327]. It is implicitly forced to seek out one of the valid max-margin solutions.</p></blockquote><p><img alt="Slide 31" loading=lazy src=/images/posts/maxtoken/slide-31.jpg></p><blockquote><p>[cite_start]To summarize the first half: we&rsquo;ve shown that when training only the attention parameter <code>p</code>, the implicit bias of gradient descent drives the solution towards a hard-margin SVM that separates tokens[cite: 329]. [cite_start]Now, what happens if we also learn the prediction head <code>v</code> at the same time? [cite: 330]</p></blockquote><p><img alt="Slide 32" loading=lazy src=/images/posts/maxtoken/slide-32.jpg></p><blockquote><p>This leads to the second part of our analysis: the joint convergence of the prediction head <code>v</code> and the attention weights <code>p</code>.</p></blockquote><p><img alt="Slide 33" loading=lazy src=/images/posts/maxtoken/slide-33.jpg></p><blockquote><p>[cite_start]The high-level intuition is that the model is linear in <code>v</code>, so the optimization with respect to <code>v</code> also has an implicit bias towards a standard max-margin classifier solution[cite: 335]. [cite_start]The features for this classifier, <code>x_i^p</code>, are the outputs of the attention layer[cite: 336].</p></blockquote><p><img alt="Slide 34" loading=lazy src=/images/posts/maxtoken/slide-34.jpg></p><blockquote><p>[cite_start]The challenge here is that the features <code>r</code> that <code>v</code> sees are determined by <code>p</code>, which itself is changing during training[cite: 352]. This creates a coupled dynamic. [cite_start]We analyze this by separating the problem into cases based on whether the selected features are support vectors for the <code>v</code>-classifier or not[cite: 353].</p></blockquote><p><img alt="Slide 35" loading=lazy src=/images/posts/maxtoken/slide-35.jpg></p><blockquote><p><strong>Theorem 5</strong> addresses the case where we assume the selected tokens are all support vectors. [cite_start]We introduce <strong>Assumption C</strong>, which formalizes that if the attention doesn&rsquo;t perfectly select the optimal token, the classification margin for <code>v</code> shrinks[cite: 355, 363]. [cite_start]Under this assumption, we find that <code>v</code> converges to the direction of a standard SVM solution, and <code>p</code> converges to the direction of our ATT-SVM solution[cite: 360, 361].</p></blockquote><p><img alt="Slide 36" loading=lazy src=/images/posts/maxtoken/slide-36.jpg></p><blockquote><p>[cite_start]This raises a natural question: Does every selected token really need to be a support vector, sitting right on the margin? [cite: 366] That seems like a very strict condition.</p></blockquote><p><img alt="Slide 37" loading=lazy src=/images/posts/maxtoken/slide-37.jpg></p><blockquote><p>Intuitively, for the data points that are <em>not</em> support vectors, we don&rsquo;t need to maximize their margin. [cite_start]We just need to make sure they are on the correct side of the decision boundary[cite: 367]. This allows us to define a &ldquo;relaxed&rdquo; version of our ATT-SVM problem.</p></blockquote><p><img alt="Slide 38" loading=lazy src=/images/posts/maxtoken/slide-38.jpg></p><blockquote><p><strong>Theorem 6</strong> presents this more general result. [cite_start]The ATT-SVM problem is relaxed: for tokens corresponding to support vectors (<code>i ∈ S</code>), we require a margin of 1, but for non-support vectors (<code>i ∈ S-bar</code>), we only require a margin of 0 (correct classification)[cite: 370]. [cite_start]We only need Assumption C to hold for the support vectors[cite: 374]. [cite_start]The result is that <code>v</code> still converges to the max-margin <code>v^mm</code>, and <code>p</code> converges to the solution of this new, relaxed SVM problem, <code>p^relax</code>[cite: 373].</p></blockquote><p><img alt="Slide 39" loading=lazy src=/images/posts/maxtoken/slide-39.jpg></p><blockquote><p>We ran experiments for this joint convergence case as well. [cite_start]The plots show the trajectories for the attention weights <code>p</code> and the classifier head <code>v</code>[cite: 423]. [cite_start]Plot (a) shows a case aligning with Theorem 5, where all inputs are support vectors[cite: 422]. [cite_start]Plot (b) shows a case for Theorem 6, where one input is not a support vector[cite: 422]. [cite_start]Plot (c) shows that the softmax probability for the optimal token quickly goes to 1, confirming that the attention is learning to select specific tokens as predicted[cite: 424].</p></blockquote><p><img alt="Slide 40" loading=lazy src=/images/posts/maxtoken/slide-40.jpg></p><blockquote><p>[cite_start]So, to summarize the entire theoretical part: Whether we train the attention weights <code>p</code> alone or jointly with the classifier head <code>v</code>, the implicit bias of the optimization consistently pushes the model towards a <strong>hard-margin SVM solution</strong>[cite: 426, 427]. This provides a strong theoretical explanation for why attention learns to become sparse and focus on the most discriminative tokens.</p></blockquote><p><img alt="Slide 41" loading=lazy src=/images/posts/maxtoken/slide-41.jpg></p><blockquote><p>Finally, let&rsquo;s look at a few more experiments to see these principles in action on more complex tasks.</p></blockquote><p><img alt="Slide 42" loading=lazy src=/images/posts/maxtoken/slide-42.jpg></p><blockquote><p>Here, we trained a vision transformer on an image classification task. [cite_start]Figure 7 shows that as training progresses over epochs, the sparsity of the attention map increases (red curve goes down), meaning the model learns to focus on fewer, more important image patches[cite: 479, 480]. [cite_start]At the same time, the norm of the attention weights <code>||W||</code> steadily increases (blue curve), which is exactly what our theory predicts will happen as the solution converges towards an infinite-norm, max-margin boundary[cite: 481]. [cite_start]The images in Figure 6 visually show this focusing effect over time[cite: 477].</p></blockquote><p><img alt="Slide 43" loading=lazy src=/images/posts/maxtoken/slide-43.jpg></p><blockquote><p>This final experiment shows how the choice of loss function affects the dynamics. We compare a correlation loss (<code>l(x) = -x</code>) with the logistic loss. [cite_start]The gradient&rsquo;s magnitude depends on the token&rsquo;s score <code>γ</code> differently for each loss[cite: 507, 513]. [cite_start]For correlation loss, larger scores get larger gradients[cite: 510], while for logistic loss, the gradient is largest for scores near zero. This results in different trajectories, but as you can see, both are ultimately guided by the same underlying max-margin principle, pushing towards a separating hyperplane.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/attention/>Attention</a></li><li><a href=https://mookjsi.github.io/tags/implicit-bias/>Implicit Bias</a></li><li><a href=https://mookjsi.github.io/tags/transformer/>Transformer</a></li><li><a href=https://mookjsi.github.io/tags/neurips-2025/>NeurIPS 2025</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/paper-review-flatminima/><span class=title>« Prev</span><br><span>Simplifying Neural Nets by Discovering Flat Minima</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on x" href="https://x.com/intent/tweet/?text=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f&amp;hashtags=PaperReview%2cAttention%2cImplicitBias%2cTransformer%2cNeurIPS2025"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f&amp;title=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism&amp;summary=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f&title=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on whatsapp" href="https://api.whatsapp.com/send?text=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on telegram" href="https://telegram.me/share/url?text=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Max-Margin Token Selection in Attention Mechanism on ycombinator" href="https://news.ycombinator.com/submitlink?t=Max-Margin%20Token%20Selection%20in%20Attention%20Mechanism&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-maxtoken%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>