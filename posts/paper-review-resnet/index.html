<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ResNet - 더 깊은 신경망을 위한 잔차 학습 | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Computer Vision,ResNet,Image Recognition,CVPR 2016"><meta name=description content="CVPR 2016에서 발표된 &lsquo;Deep Residual Learning for Image Recognition&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 딥러닝의 &lsquo;성능 저하(Degradation)&rsquo; 문제를 해결한 잔차 학습(Residual Learning)의 핵심 개념, 네트워크 구조, 그리고 실험 결과를 알기 쉽게 분석합니다."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-resnet/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-resnet/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-resnet/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="ResNet - 더 깊은 신경망을 위한 잔차 학습"><meta property="og:description" content="CVPR 2016에서 발표된 ‘Deep Residual Learning for Image Recognition’ 논문에 대한 심층 리뷰입니다. 이 포스트에서는 딥러닝의 ‘성능 저하(Degradation)’ 문제를 해결한 잔차 학습(Residual Learning)의 핵심 개념, 네트워크 구조, 그리고 실험 결과를 알기 쉽게 분석합니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-16T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="ResNet"><meta property="article:tag" content="Image Recognition"><meta property="article:tag" content="CVPR 2016"><meta name=twitter:card content="summary"><meta name=twitter:title content="ResNet - 더 깊은 신경망을 위한 잔차 학습"><meta name=twitter:description content="CVPR 2016에서 발표된 &lsquo;Deep Residual Learning for Image Recognition&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 딥러닝의 &lsquo;성능 저하(Degradation)&rsquo; 문제를 해결한 잔차 학습(Residual Learning)의 핵심 개념, 네트워크 구조, 그리고 실험 결과를 알기 쉽게 분석합니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ResNet - 더 깊은 신경망을 위한 잔차 학습","item":"https://mookjsi.github.io/posts/paper-review-resnet/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ResNet - 더 깊은 신경망을 위한 잔차 학습","name":"ResNet - 더 깊은 신경망을 위한 잔차 학습","description":"CVPR 2016에서 발표된 \u0026lsquo;Deep Residual Learning for Image Recognition\u0026rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 딥러닝의 \u0026lsquo;성능 저하(Degradation)\u0026rsquo; 문제를 해결한 잔차 학습(Residual Learning)의 핵심 개념, 네트워크 구조, 그리고 실험 결과를 알기 쉽게 분석합니다.","keywords":["Paper Review","Deep Learning","Computer Vision","ResNet","Image Recognition","CVPR 2016"],"articleBody":"CVPR 2016에서 발표되어 딥러닝 역사에 한 획을 그은 논문, **“Deep Residual Learning for Image Recognition”**에 대한 전체 리뷰를 공유합니다.\n이 논문은 당시 딥러닝 모델의 큰 난제였던 ‘네트워크가 깊어질수록 오히려 성능이 저하되는’ 문제를 해결했습니다. 저자들의 해법인 **ResNet(Residual Network)**은 ‘잔차 학습’이라는 혁신적인 구조를 도입하여 이전에는 불가능했던 100층 이상의 초심층 신경망 훈련을 가능하게 했습니다.\n서론 컴퓨터가 이미지를 인식하는 기술, 예를 들어 사진 속의 고양이를 알아보는 인공지능(AI)은 ‘심층 신경망(Deep Neural Network)‘이라는 기술을 사용합니다. 이 신경망은 인간의 뇌가 정보를 처리하는 방식을 모방한 것으로, 여러 개의 층(layer)으로 이루어져 있습니다. 이론적으로는 이 층을 많이 쌓을수록, 즉 네트워크가 ‘깊어질수록’ 더 똑똑해져야 합니다. 마치 우리가 여러 단계의 사고를 거쳐 복잡한 문제를 해결하는 것과 같죠.\n하지만 실제로는 무작정 층을 깊게 쌓기만 하면 오히려 성능이 떨어지는 ‘성능 저하(Degradation)’ 문제가 발생했습니다. 신기하게도 이는 단순히 계산이 너무 복잡해져서 생기는 과적합(overfitting) 문제도 아니었습니다. 더 깊은 모델이 그보다 얕은 모델보다 훈련 데이터에 대한 오류율(training error)이 더 높게 나타나는 이상한 현상이었습니다.\n위 그림은 이 문제를 명확히 보여줍니다. CIFAR-10이라는 이미지 데이터셋으로 20층 네트워크와 56층 네트워크를 학습시킨 결과입니다. 왼쪽 그래프(training error)를 보면, 더 깊은 56층 네트워크(붉은색 선)가 20층 네트워크(노란색 선)보다 훈련 오류가 더 높습니다. 당연히 오른쪽 그래프(test error)에서도 56층 네트워크의 테스트 오류가 더 높게 나타납니다. 상식적으로 더 깊은 네트워크가 최소한 얕은 네트워크만큼의 성능은 내야 하는데, 실제로는 그렇지 못했던 것입니다.\n이 논문은 바로 이 ‘성능 저하’ 문제를 해결하기 위해 **‘깊은 잔차 학습(Deep Residual Learning)’**이라는 새로운 프레임워크를 제안합니다. 간단히 말해, 네트워크가 처음부터 정답을 완벽하게 맞추려고 애쓰는 대신, 이미 알고 있는 정보(입력값)를 바탕으로 ‘차이(residual)‘만을 학습하도록 구조를 바꾼 것입니다. 이 혁신적인 방법으로 저자들은 이전에 불가능하다고 여겨졌던 152층 깊이의 네트워크를 성공적으로 훈련시켰고, 당시 이미지 인식 대회인 ILSVRC 2015에서 1위를 차지하는 쾌거를 이루었습니다.\n기술 설명 ResNet의 핵심 아이디어는 어떻게 ‘차이’만을 학습하게 만드는 것일까요? 바로 **‘지름길 연결(Shortcut Connection)’**이라는 구조를 통해 구현됩니다.\n기존의 신경망은 입력값 x가 여러 층을 순서대로 통과하며 복잡한 함수 H(x)를 학습하려고 했습니다. 예를 들어, 고양이 사진(x)을 보고 ‘고양이’라는 정답(H(x))을 바로 찾아내려는 방식이었죠.\n하지만 ResNet은 생각을 바꿨습니다. ‘어차피 입력값 x가 있는데, 굳이 H(x) 전체를 새로 배울 필요가 있을까? 그냥 x에다가 약간의 정보만 더해서 정답을 만들면 되지 않을까?’ 라는 접근입니다. 그래서 네트워크가 학습해야 할 목표를 H(x)에서 F(x) = H(x) - x로 바꿉니다. 여기서 F(x)가 바로 입력값과 정답 사이의 ‘차이’, 즉 **잔차(residual)**입니다. 네트워크는 이 잔차 F(x)를 학습한 뒤, 원래 입력값 x를 더해 최종 결과(F(x) + x)를 만들어냅니다.\n위 그림은 이 ‘잔차 학습’의 기본 단위를 보여줍니다.\n입력값 x가 두 갈래로 나뉩니다. 한쪽은 기존처럼 가중치 층(weight layer)을 통과하며 복잡한 변환, 즉 F(x)를 학습합니다. 다른 한쪽은 아무런 변환 없이 그대로 건너뛰는 ‘지름길(shortcut)‘을 따라갑니다. 이를 **‘항등 매핑(identity mapping)’**이라고 부릅니다. 마지막에 두 결과(F(x)와 x)가 더해져 최종 출력(F(x) + x)이 됩니다. 이 구조가 왜 효과적일까요? 만약 어떤 층에서 아무것도 학습할 필요 없이 입력값을 그대로 전달하는 것이 최선인 상황이라면, 기존 네트워크는 여러 층을 거치며 입력과 출력이 똑같아지는 복잡한 변환을 학습해야만 했습니다. 이는 매우 어려운 일이었죠. 하지만 ResNet 구조에서는 네트워크가 잔차 F(x)를 그냥 ‘0’으로 만들기만 하면 됩니다. 즉, 가중치를 0으로 만들어 아무것도 바꾸지 않으면, 자연스럽게 입력 x가 그대로 출력으로 나가게 됩니다. 훨씬 쉬운 방법으로 최적의 해를 찾을 수 있는 것입니다.\n다음은 실제 네트워크 구조입니다.\nVGG-19 (왼쪽): 당시 표준으로 여겨지던 깊은 네트워크 구조입니다. 34-layer plain (중간): VGG-19의 모델을 따라 단순히 층을 깊게 쌓은 일반적인 네트워크입니다. 이 구조에서 성능 저하 문제가 발생합니다. 34-layer residual (오른쪽): 중간의 ‘plain’ 네트워크와 똑같은 구조에 ‘지름길 연결(shortcut connection)‘만 추가한 ResNet 구조입니다. 굽은 화살표들이 바로 이 지름길을 나타냅니다. 더 깊은 네트워크(50층 이상)를 효율적으로 만들기 위한 ‘병목(Bottleneck)’ 구조는 다음과 같습니다.\n기존의 블록(왼쪽)이 3x3 필터의 합성곱 층 두 개로 이루어졌다면, 병목 블록(오른쪽)은 1x1, 3x3, 1x1 합성곱 층 세 개로 구성됩니다. 첫 번째 1x1 합성곱은 채널 수를 줄여 계산량을 감소시키고(병목처럼 입구가 좁아짐), 3x3 합성곱으로 핵심적인 연산을 수행한 뒤, 마지막 1x1 합성곱으로 다시 채널 수를 원래대로 복원하는 방식입니다. 이 구조 덕분에 층은 더 깊게 쌓으면서도 전체적인 계산 복잡도는 VGG 네트워크보다 낮게 유지할 수 있었습니다. 실험 저자들은 제안한 ResNet의 효과를 증명하기 위해 이미지넷(ImageNet)과 CIFAR-10이라는 대표적인 이미지 데이터셋으로 다양한 실험을 진행했습니다.\n이미지넷(ImageNet) 분류 실험 이미지넷은 1000개의 클래스(종류)로 이루어진 128만 장의 방대한 이미지 데이터셋입니다.\n위 결과들은 ResNet이 성능 저하 문제를 어떻게 해결하는지 보여줍니다.\n왼쪽 그래프: 일반(plain) 네트워크의 경우, 34층(붉은색 선)이 18층(하늘색 선)보다 훈련 오류와 검증 오류 모두 더 높습니다. 전형적인 성능 저하 현상입니다. 오른쪽 그래프: ResNet의 경우, 상황이 역전됩니다. 34층 ResNet(붉은색 선)이 18층 ResNet(하늘색 선)보다 오류율이 훨씬 낮습니다. 깊이가 깊어질수록 성능이 향상된 것입니다. 위 표는 이 결과를 수치로 보여줍니다. 일반 네트워크는 18층(27.94%)에서 34층(28.54%)으로 갈 때 오류율이 높아졌지만, ResNet은 18층(27.88%)에서 34층(25.03%)으로 갈 때 오류율이 2.8%나 크게 감소했습니다.\n더 깊은 ResNet 모델들의 성능은 위 두 표와 같습니다. 저자들은 앞에서 설명한 ‘병목’ 구조를 사용해 50층, 101층, 그리고 무려 152층에 달하는 ResNet을 만들었습니다. 34층(25.03%)부터 시작해 50층(22.85%), 101층(21.75%), 152층(21.43%)으로 깊어질수록 top-1 오류율이 꾸준히 감소하는 것을 볼 수 있습니다. 성능 저하 문제없이 깊이의 이점을 취한 것입니다.\n특히 152층 ResNet은 단일 모델만으로 4.49%의 top-5 검증 오류율을 달성했는데, 이는 당시 다른 여러 모델을 합친 앙상블(ensemble) 결과보다도 좋은 성적이었습니다.\n최종 대회 결과, 여러 ResNet 모델을 앙상블하여 이미지넷 테스트 데이터셋에서 **3.57%**라는 경이로운 top-5 오류율을 기록하며 ILSVRC 2015 대회에서 1위를 차지했습니다.\nCIFAR-10 분석 및 1000층 이상의 네트워크 CIFAR-10 데이터셋을 이용한 실험에서도 비슷한 결과가 나타났습니다. 위 그래프는 CIFAR-10에서의 훈련 과정을 보여줍니다.\n왼쪽(plain networks): 일반 네트워크는 20층에서 56층으로 깊어질수록 오류율이 점점 높아지는 성능 저하 현상을 보입니다. 중간(ResNets): 반면, ResNet은 20층부터 110층까지 깊이가 증가할수록 오류율이 꾸준히 감소합니다. 오른쪽(110-layer vs 1202-layer): 저자들은 여기서 더 나아가 1202층이라는 극단적으로 깊은 네트워크를 훈련시키는 데 성공했습니다. 훈련 오류는 0.1% 미만으로 매우 낮았지만(오른쪽 그래프 아래쪽 선), 테스트 오류(7.93%)는 110층 모델(6.43%)보다 다소 높게 나타났습니다. 이는 작은 데이터셋에 비해 모델이 너무 거대해서 발생한 과적합(overfitting) 때문으로 분석됩니다. 위 그래프는 각 층의 응답(출력값)의 표준편차를 분석한 것입니다.\n그래프를 보면 전반적으로 ResNet(붉은색, 검은색 선)의 응답값이 일반 네트워크(노란색, 분홍색 선)보다 작습니다. 이는 ResNet의 잔차 함수가 일반적으로 ‘0’에 가까운 값을 갖는다는 가설을 뒷받침합니다. 즉, 각 층이 신호를 크게 바꾸기보다는 조금씩만 수정한다는 의미입니다. 또한 ResNet-20, 56, 110을 비교해보면, 네트워크가 깊어질수록 각 층의 응답값이 더 작아지는 경향을 보입니다. 더 많은 층이 협력하여 신호를 조금씩 점진적으로 바꾼다는 것을 시사합니다. 객체 탐지(Object Detection) 실험 ResNet은 단순히 이미지를 분류하는 것을 넘어, 이미지 속 특정 물체의 위치를 찾아내는 객체 탐지 과제에서도 뛰어난 성능을 보였습니다. 아래 표들은 기존의 VGG-16 네트워크를 ResNet-101로 교체했을 때의 성능 향상을 보여줍니다. 특히 어려운 COCO 데이터셋에서 mAP(객체 탐지 성능의 주요 척도)가 28%나 상대적으로 향상되었습니다. 이는 ResNet이 학습한 표현(representation) 자체가 매우 우수하다는 것을 증명합니다.\n결론 ‘Deep Residual Learning for Image Recognition’ 논문은 딥러닝 역사에 한 획을 그은 중요한 연구입니다. 이 논문은 다음과 같은 핵심적인 기여를 했습니다.\n‘성능 저하(Degradation)’ 문제 정의 및 해결: 이전까지는 명확하게 설명되지 않았던, 네트워크가 깊어질수록 훈련이 더 어려워지는 문제를 ‘성능 저하’로 명확히 정의하고, 이를 ‘잔차 학습(Residual Learning)‘이라는 혁신적인 아이디어로 해결했습니다. 초심층 신경망(Extremely Deep Network)의 가능성 제시: ‘지름길 연결(Shortcut Connection)‘이라는 간단하면서도 강력한 구조를 통해 152층, 나아가 1000층이 넘는 매우 깊은 신경망의 훈련을 가능하게 했습니다. 이는 딥러닝 모델의 깊이에 대한 기존의 한계를 완전히 무너뜨린 것입니다. 다양한 분야에서의 SOTA(State-of-the-art) 달성: 제안된 ResNet은 이미지 분류뿐만 아니라 객체 탐지, 분할 등 다양한 컴퓨터 비전 분야에서 압도적인 성능을 보여주며 새로운 표준 모델로 자리 잡았습니다. ","wordCount":"1090","inLanguage":"en","datePublished":"2025-09-16T00:00:00Z","dateModified":"2025-09-16T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-resnet/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">ResNet - 더 깊은 신경망을 위한 잔차 학습</h1><div class=post-meta><span title='2025-09-16 00:00:00 +0000 UTC'>September 16, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1090 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>CVPR 2016에서 발표되어 딥러닝 역사에 한 획을 그은 논문, **&ldquo;Deep Residual Learning for Image Recognition&rdquo;**에 대한 전체 리뷰를 공유합니다.</p><p>이 논문은 당시 딥러닝 모델의 큰 난제였던 &lsquo;네트워크가 깊어질수록 오히려 성능이 저하되는&rsquo; 문제를 해결했습니다. 저자들의 해법인 **ResNet(Residual Network)**은 &lsquo;잔차 학습&rsquo;이라는 혁신적인 구조를 도입하여 이전에는 불가능했던 100층 이상의 초심층 신경망 훈련을 가능하게 했습니다.</p><hr><h3 id=서론>서론<a hidden class=anchor aria-hidden=true href=#서론>#</a></h3><p>컴퓨터가 이미지를 인식하는 기술, 예를 들어 사진 속의 고양이를 알아보는 인공지능(AI)은 &lsquo;심층 신경망(Deep Neural Network)&lsquo;이라는 기술을 사용합니다. 이 신경망은 인간의 뇌가 정보를 처리하는 방식을 모방한 것으로, 여러 개의 층(layer)으로 이루어져 있습니다. 이론적으로는 이 층을 많이 쌓을수록, 즉 네트워크가 &lsquo;깊어질수록&rsquo; 더 똑똑해져야 합니다. 마치 우리가 여러 단계의 사고를 거쳐 복잡한 문제를 해결하는 것과 같죠.</p><p>하지만 실제로는 무작정 층을 깊게 쌓기만 하면 오히려 성능이 떨어지는 <strong>&lsquo;성능 저하(Degradation)&rsquo;</strong> 문제가 발생했습니다. 신기하게도 이는 단순히 계산이 너무 복잡해져서 생기는 과적합(overfitting) 문제도 아니었습니다. 더 깊은 모델이 그보다 얕은 모델보다 훈련 데이터에 대한 오류율(training error)이 더 높게 나타나는 이상한 현상이었습니다.</p><p><img alt="Figure 1: CIFAR-10에서의 20층 및 56층 일반 네트워크 오류율" loading=lazy src=/images/posts/resnet/figure1.png></p><p>위 그림은 이 문제를 명확히 보여줍니다. CIFAR-10이라는 이미지 데이터셋으로 20층 네트워크와 56층 네트워크를 학습시킨 결과입니다. 왼쪽 그래프(training error)를 보면, 더 깊은 56층 네트워크(붉은색 선)가 20층 네트워크(노란색 선)보다 훈련 오류가 더 높습니다. 당연히 오른쪽 그래프(test error)에서도 56층 네트워크의 테스트 오류가 더 높게 나타납니다. 상식적으로 더 깊은 네트워크가 최소한 얕은 네트워크만큼의 성능은 내야 하는데, 실제로는 그렇지 못했던 것입니다.</p><p>이 논문은 바로 이 &lsquo;성능 저하&rsquo; 문제를 해결하기 위해 **&lsquo;깊은 잔차 학습(Deep Residual Learning)&rsquo;**이라는 새로운 프레임워크를 제안합니다. 간단히 말해, 네트워크가 처음부터 정답을 완벽하게 맞추려고 애쓰는 대신, 이미 알고 있는 정보(입력값)를 바탕으로 &lsquo;차이(residual)&lsquo;만을 학습하도록 구조를 바꾼 것입니다. 이 혁신적인 방법으로 저자들은 이전에 불가능하다고 여겨졌던 152층 깊이의 네트워크를 성공적으로 훈련시켰고, 당시 이미지 인식 대회인 ILSVRC 2015에서 1위를 차지하는 쾌거를 이루었습니다.</p><hr><h3 id=기술-설명>기술 설명<a hidden class=anchor aria-hidden=true href=#기술-설명>#</a></h3><p>ResNet의 핵심 아이디어는 어떻게 &lsquo;차이&rsquo;만을 학습하게 만드는 것일까요? 바로 **&lsquo;지름길 연결(Shortcut Connection)&rsquo;**이라는 구조를 통해 구현됩니다.</p><p>기존의 신경망은 입력값 <code>x</code>가 여러 층을 순서대로 통과하며 복잡한 함수 <code>H(x)</code>를 학습하려고 했습니다. 예를 들어, 고양이 사진(<code>x</code>)을 보고 &lsquo;고양이&rsquo;라는 정답(<code>H(x)</code>)을 바로 찾아내려는 방식이었죠.</p><p>하지만 ResNet은 생각을 바꿨습니다. &lsquo;어차피 입력값 <code>x</code>가 있는데, 굳이 <code>H(x)</code> 전체를 새로 배울 필요가 있을까? 그냥 <code>x</code>에다가 약간의 정보만 더해서 정답을 만들면 되지 않을까?&rsquo; 라는 접근입니다. 그래서 네트워크가 학습해야 할 목표를 <code>H(x)</code>에서 <code>F(x) = H(x) - x</code>로 바꿉니다. 여기서 <code>F(x)</code>가 바로 입력값과 정답 사이의 &lsquo;차이&rsquo;, 즉 **잔차(residual)**입니다. 네트워크는 이 잔차 <code>F(x)</code>를 학습한 뒤, 원래 입력값 <code>x</code>를 더해 최종 결과(<code>F(x) + x</code>)를 만들어냅니다.</p><p><img alt="Figure 2: 잔차 학습의 기본 블록 구조" loading=lazy src=/images/posts/resnet/figure2.png></p><p>위 그림은 이 &lsquo;잔차 학습&rsquo;의 기본 단위를 보여줍니다.</p><ul><li>입력값 <code>x</code>가 두 갈래로 나뉩니다.</li><li>한쪽은 기존처럼 가중치 층(weight layer)을 통과하며 복잡한 변환, 즉 <code>F(x)</code>를 학습합니다.</li><li>다른 한쪽은 아무런 변환 없이 그대로 건너뛰는 &lsquo;지름길(shortcut)&lsquo;을 따라갑니다. 이를 **&lsquo;항등 매핑(identity mapping)&rsquo;**이라고 부릅니다.</li><li>마지막에 두 결과(<code>F(x)</code>와 <code>x</code>)가 더해져 최종 출력(<code>F(x) + x</code>)이 됩니다.</li></ul><p>이 구조가 왜 효과적일까요? 만약 어떤 층에서 아무것도 학습할 필요 없이 입력값을 그대로 전달하는 것이 최선인 상황이라면, 기존 네트워크는 여러 층을 거치며 입력과 출력이 똑같아지는 복잡한 변환을 학습해야만 했습니다. 이는 매우 어려운 일이었죠. 하지만 ResNet 구조에서는 네트워크가 잔차 <code>F(x)</code>를 그냥 &lsquo;0&rsquo;으로 만들기만 하면 됩니다. 즉, 가중치를 0으로 만들어 아무것도 바꾸지 않으면, 자연스럽게 입력 <code>x</code>가 그대로 출력으로 나가게 됩니다. 훨씬 쉬운 방법으로 최적의 해를 찾을 수 있는 것입니다.</p><p><img alt="Figure 3: VGG-19, 34층 일반 네트워크, 34층 ResNet 구조 비교" loading=lazy src=/images/posts/resnet/figure3.png></p><p>다음은 실제 네트워크 구조입니다.</p><ul><li><strong>VGG-19 (왼쪽)</strong>: 당시 표준으로 여겨지던 깊은 네트워크 구조입니다.</li><li><strong>34-layer plain (중간)</strong>: VGG-19의 모델을 따라 단순히 층을 깊게 쌓은 일반적인 네트워크입니다. 이 구조에서 성능 저하 문제가 발생합니다.</li><li><strong>34-layer residual (오른쪽)</strong>: 중간의 &lsquo;plain&rsquo; 네트워크와 똑같은 구조에 &lsquo;지름길 연결(shortcut connection)&lsquo;만 추가한 ResNet 구조입니다. 굽은 화살표들이 바로 이 지름길을 나타냅니다.</li></ul><p><img alt="Figure 5: 일반 블록과 병목 블록 구조 비교" loading=lazy src=/images/posts/resnet/figure5.png></p><p>더 깊은 네트워크(50층 이상)를 효율적으로 만들기 위한 <strong>&lsquo;병목(Bottleneck)&rsquo;</strong> 구조는 다음과 같습니다.</p><ul><li>기존의 블록(왼쪽)이 3x3 필터의 합성곱 층 두 개로 이루어졌다면, 병목 블록(오른쪽)은 1x1, 3x3, 1x1 합성곱 층 세 개로 구성됩니다.</li><li>첫 번째 1x1 합성곱은 채널 수를 줄여 계산량을 감소시키고(병목처럼 입구가 좁아짐), 3x3 합성곱으로 핵심적인 연산을 수행한 뒤, 마지막 1x1 합성곱으로 다시 채널 수를 원래대로 복원하는 방식입니다.</li><li>이 구조 덕분에 층은 더 깊게 쌓으면서도 전체적인 계산 복잡도는 VGG 네트워크보다 낮게 유지할 수 있었습니다.</li></ul><hr><h3 id=실험>실험<a hidden class=anchor aria-hidden=true href=#실험>#</a></h3><p>저자들은 제안한 ResNet의 효과를 증명하기 위해 이미지넷(ImageNet)과 CIFAR-10이라는 대표적인 이미지 데이터셋으로 다양한 실험을 진행했습니다.</p><h4 id=이미지넷imagenet-분류-실험>이미지넷(ImageNet) 분류 실험<a hidden class=anchor aria-hidden=true href=#이미지넷imagenet-분류-실험>#</a></h4><p>이미지넷은 1000개의 클래스(종류)로 이루어진 128만 장의 방대한 이미지 데이터셋입니다.</p><p><img alt="Figure 4: 이미지넷 데이터셋에서의 일반 네트워크와 ResNet 훈련 과정 비교" loading=lazy src=/images/posts/resnet/figure4.png></p><p>위 결과들은 ResNet이 성능 저하 문제를 어떻게 해결하는지 보여줍니다.</p><ul><li><strong>왼쪽 그래프</strong>: 일반(plain) 네트워크의 경우, 34층(붉은색 선)이 18층(하늘색 선)보다 훈련 오류와 검증 오류 모두 더 높습니다. 전형적인 성능 저하 현상입니다.</li><li><strong>오른쪽 그래프</strong>: ResNet의 경우, 상황이 역전됩니다. 34층 ResNet(붉은색 선)이 18층 ResNet(하늘색 선)보다 오류율이 훨씬 낮습니다. 깊이가 깊어질수록 성능이 향상된 것입니다.</li></ul><p><img alt="Table 2: 이미지넷 검증 데이터셋에서의 Top-1 오류율 비교" loading=lazy src=/images/posts/resnet/table2.png></p><p>위 표는 이 결과를 수치로 보여줍니다. 일반 네트워크는 18층(27.94%)에서 34층(28.54%)으로 갈 때 오류율이 높아졌지만, ResNet은 18층(27.88%)에서 34층(25.03%)으로 갈 때 오류율이 2.8%나 크게 감소했습니다.</p><p><img alt="Table 3: 다양한 깊이의 ResNet 모델 성능 비교" loading=lazy src=/images/posts/resnet/table3.png></p><p><img alt="Table 4: 단일 모델 성능 비교" loading=lazy src=/images/posts/resnet/table4.png></p><p>더 깊은 ResNet 모델들의 성능은 위 두 표와 같습니다. 저자들은 앞에서 설명한 &lsquo;병목&rsquo; 구조를 사용해 50층, 101층, 그리고 무려 152층에 달하는 ResNet을 만들었습니다. 34층(25.03%)부터 시작해 50층(22.85%), 101층(21.75%), 152층(21.43%)으로 깊어질수록 top-1 오류율이 꾸준히 감소하는 것을 볼 수 있습니다. 성능 저하 문제없이 깊이의 이점을 취한 것입니다.</p><p>특히 <strong>152층 ResNet</strong>은 단일 모델만으로 4.49%의 top-5 검증 오류율을 달성했는데, 이는 당시 다른 여러 모델을 합친 앙상블(ensemble) 결과보다도 좋은 성적이었습니다.</p><p>최종 대회 결과, 여러 ResNet 모델을 앙상블하여 이미지넷 테스트 데이터셋에서 **3.57%**라는 경이로운 top-5 오류율을 기록하며 ILSVRC 2015 대회에서 1위를 차지했습니다.</p><p><img alt="Table 5: 앙상블 모델 성능 및 ILSVRC 2015 대회 결과" loading=lazy src=/images/posts/resnet/table5.png></p><h4 id=cifar-10-분석-및-1000층-이상의-네트워크>CIFAR-10 분석 및 1000층 이상의 네트워크<a hidden class=anchor aria-hidden=true href=#cifar-10-분석-및-1000층-이상의-네트워크>#</a></h4><p><img alt="Figure 6: CIFAR-10 데이터셋에서의 훈련 과정" loading=lazy src=/images/posts/resnet/figure6.png></p><p>CIFAR-10 데이터셋을 이용한 실험에서도 비슷한 결과가 나타났습니다. 위 그래프는 CIFAR-10에서의 훈련 과정을 보여줍니다.</p><ul><li><strong>왼쪽(plain networks)</strong>: 일반 네트워크는 20층에서 56층으로 깊어질수록 오류율이 점점 높아지는 성능 저하 현상을 보입니다.</li><li><strong>중간(ResNets)</strong>: 반면, ResNet은 20층부터 110층까지 깊이가 증가할수록 오류율이 꾸준히 감소합니다.</li><li><strong>오른쪽(110-layer vs 1202-layer)</strong>: 저자들은 여기서 더 나아가 1202층이라는 극단적으로 깊은 네트워크를 훈련시키는 데 성공했습니다. 훈련 오류는 0.1% 미만으로 매우 낮았지만(오른쪽 그래프 아래쪽 선), 테스트 오류(7.93%)는 110층 모델(6.43%)보다 다소 높게 나타났습니다. 이는 작은 데이터셋에 비해 모델이 너무 거대해서 발생한 과적합(overfitting) 때문으로 분석됩니다.</li></ul><p><img alt="Figure 7: CIFAR-10에서의 층별 응답 표준편차 분석" loading=lazy src=/images/posts/resnet/figure7.png></p><p>위 그래프는 각 층의 응답(출력값)의 표준편차를 분석한 것입니다.</p><ul><li>그래프를 보면 전반적으로 ResNet(붉은색, 검은색 선)의 응답값이 일반 네트워크(노란색, 분홍색 선)보다 작습니다. 이는 ResNet의 잔차 함수가 일반적으로 &lsquo;0&rsquo;에 가까운 값을 갖는다는 가설을 뒷받침합니다. 즉, 각 층이 신호를 크게 바꾸기보다는 조금씩만 수정한다는 의미입니다.</li><li>또한 ResNet-20, 56, 110을 비교해보면, 네트워크가 깊어질수록 각 층의 응답값이 더 작아지는 경향을 보입니다. 더 많은 층이 협력하여 신호를 조금씩 점진적으로 바꾼다는 것을 시사합니다.</li></ul><h4 id=객체-탐지object-detection-실험>객체 탐지(Object Detection) 실험<a hidden class=anchor aria-hidden=true href=#객체-탐지object-detection-실험>#</a></h4><p><img alt="Table 7/8: PASCAL VOC / COCO 데이터셋에서의 객체 탐지 성능(mAP)" loading=lazy src=/images/posts/resnet/table7_8.png></p><p>ResNet은 단순히 이미지를 분류하는 것을 넘어, 이미지 속 특정 물체의 위치를 찾아내는 객체 탐지 과제에서도 뛰어난 성능을 보였습니다. 아래 표들은 기존의 VGG-16 네트워크를 ResNet-101로 교체했을 때의 성능 향상을 보여줍니다. 특히 어려운 COCO 데이터셋에서 mAP(객체 탐지 성능의 주요 척도)가 28%나 상대적으로 향상되었습니다. 이는 ResNet이 학습한 표현(representation) 자체가 매우 우수하다는 것을 증명합니다.</p><hr><h3 id=결론>결론<a hidden class=anchor aria-hidden=true href=#결론>#</a></h3><p>&lsquo;Deep Residual Learning for Image Recognition&rsquo; 논문은 딥러닝 역사에 한 획을 그은 중요한 연구입니다. 이 논문은 다음과 같은 핵심적인 기여를 했습니다.</p><ol><li><strong>&lsquo;성능 저하(Degradation)&rsquo; 문제 정의 및 해결</strong>: 이전까지는 명확하게 설명되지 않았던, 네트워크가 깊어질수록 훈련이 더 어려워지는 문제를 &lsquo;성능 저하&rsquo;로 명확히 정의하고, 이를 &lsquo;잔차 학습(Residual Learning)&lsquo;이라는 혁신적인 아이디어로 해결했습니다.</li><li><strong>초심층 신경망(Extremely Deep Network)의 가능성 제시</strong>: &lsquo;지름길 연결(Shortcut Connection)&lsquo;이라는 간단하면서도 강력한 구조를 통해 152층, 나아가 1000층이 넘는 매우 깊은 신경망의 훈련을 가능하게 했습니다. 이는 딥러닝 모델의 깊이에 대한 기존의 한계를 완전히 무너뜨린 것입니다.</li><li><strong>다양한 분야에서의 SOTA(State-of-the-art) 달성</strong>: 제안된 ResNet은 이미지 분류뿐만 아니라 객체 탐지, 분할 등 다양한 컴퓨터 비전 분야에서 압도적인 성능을 보여주며 새로운 표준 모델로 자리 잡았습니다.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://mookjsi.github.io/tags/resnet/>ResNet</a></li><li><a href=https://mookjsi.github.io/tags/image-recognition/>Image Recognition</a></li><li><a href=https://mookjsi.github.io/tags/cvpr-2016/>CVPR 2016</a></li></ul><nav class=paginav><a class=next href=https://mookjsi.github.io/posts/trpoppo/><span class=title>Next »</span><br><span>Advanced Policy Gradient Methods: TRPO & PPO</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on x" href="https://x.com/intent/tweet/?text=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f&amp;hashtags=PaperReview%2cDeepLearning%2cComputerVision%2cResNet%2cImageRecognition%2cCVPR2016"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f&amp;title=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5&amp;summary=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f&title=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on whatsapp" href="https://api.whatsapp.com/send?text=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on telegram" href="https://telegram.me/share/url?text=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ResNet - 더 깊은 신경망을 위한 잔차 학습 on ycombinator" href="https://news.ycombinator.com/submitlink?t=ResNet%20-%20%eb%8d%94%20%ea%b9%8a%ec%9d%80%20%ec%8b%a0%ea%b2%bd%eb%a7%9d%ec%9d%84%20%ec%9c%84%ed%95%9c%20%ec%9e%94%ec%b0%a8%20%ed%95%99%ec%8a%b5&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-resnet%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>