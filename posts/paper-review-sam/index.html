<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Segment Anything 논문 리뷰 | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Computer Vision,Image Segmentation,Foundation Model,SAM,SA-1B,ICCV 2023"><meta name=description content="Meta AI의 &lsquo;Segment Anything&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트는 이미지 분할(Image Segmentation) 분야에 &lsquo;파운데이션 모델&rsquo;이라는 새로운 패러다임을 제시한 SAM(Segment Anything Model)의 핵심 개념, 즉 프롬프트 기반 분할 과업, 효율적인 모델 구조, 그리고 11억 개의 마스크를 포함하는 SA-1B 데이터셋 구축을 위한 데이터 엔진에 대해 상세히 분석합니다."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-sam/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-sam/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-sam/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Segment Anything 논문 리뷰"><meta property="og:description" content="Meta AI의 ‘Segment Anything’ 논문에 대한 심층 리뷰입니다. 이 포스트는 이미지 분할(Image Segmentation) 분야에 ‘파운데이션 모델’이라는 새로운 패러다임을 제시한 SAM(Segment Anything Model)의 핵심 개념, 즉 프롬프트 기반 분할 과업, 효율적인 모델 구조, 그리고 11억 개의 마스크를 포함하는 SA-1B 데이터셋 구축을 위한 데이터 엔진에 대해 상세히 분석합니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-30T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Segmentation"><meta property="article:tag" content="Foundation Model"><meta property="article:tag" content="SAM"><meta name=twitter:card content="summary"><meta name=twitter:title content="Segment Anything 논문 리뷰"><meta name=twitter:description content="Meta AI의 &lsquo;Segment Anything&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트는 이미지 분할(Image Segmentation) 분야에 &lsquo;파운데이션 모델&rsquo;이라는 새로운 패러다임을 제시한 SAM(Segment Anything Model)의 핵심 개념, 즉 프롬프트 기반 분할 과업, 효율적인 모델 구조, 그리고 11억 개의 마스크를 포함하는 SA-1B 데이터셋 구축을 위한 데이터 엔진에 대해 상세히 분석합니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Segment Anything 논문 리뷰","item":"https://mookjsi.github.io/posts/paper-review-sam/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Segment Anything 논문 리뷰","name":"Segment Anything 논문 리뷰","description":"Meta AI의 \u0026lsquo;Segment Anything\u0026rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트는 이미지 분할(Image Segmentation) 분야에 \u0026lsquo;파운데이션 모델\u0026rsquo;이라는 새로운 패러다임을 제시한 SAM(Segment Anything Model)의 핵심 개념, 즉 프롬프트 기반 분할 과업, 효율적인 모델 구조, 그리고 11억 개의 마스크를 포함하는 SA-1B 데이터셋 구축을 위한 데이터 엔진에 대해 상세히 분석합니다.","keywords":["Paper Review","Deep Learning","Computer Vision","Image Segmentation","Foundation Model","SAM","SA-1B","ICCV 2023"],"articleBody":"이 논문은 이미지 분할(Image Segmentation) 분야에 ‘파운데이션 모델(Foundation Model)’ 이라는 새로운 패러다임을 제시하는 “Segment Anything (SA)“를 소개합니다. 이미지 분할이란, 이미지 속 각 픽셀이 어떤 객체에 속하는지 구분하여 영역을 나누는 기술입니다. 예를 들어, 고양이 사진에서 픽셀 단위로 ‘여기는 고양이’, ‘여기는 배경’이라고 알려주는 것과 같습니다.\n최근 자연어 처리(NLP) 분야에서는 GPT와 같이 웹 규모의 방대한 데이터로 사전 훈련된 **‘파운데이션 모델’**이 등장하여, 특정 작업에 대한 추가 훈련(fine-tuning) 없이도 ‘프롬프트(prompt)‘만으로 다양한 작업을 놀랍도록 잘 수행해내고 있습니다. 이 논문의 저자들은 이러한 성공에 영감을 받아, 이미지 분할 분야에도 적용할 수 있는 범용 모델을 만들고자 했습니다.\n이를 위해 저자들은 다음 세 가지 핵심 요소를 새롭게 정의하고 개발했습니다.\n새로운 과업(Task): 프롬프트 기반 분할 (Promptable Segmentation) 새로운 모델(Model): Segment Anything Model (SAM) 새로운 데이터셋(Data): Segment Anything 1-Billion (SA-1B) 이 세 가지는 서로 맞물려 돌아가는 톱니바퀴처럼 작용합니다. 좋은 모델(SAM)을 만들기 위해 방대한 데이터(SA-1B)가 필요하고, 이 데이터를 효율적으로 구축하기 위해 좋은 모델(SAM)을 ‘데이터 엔진’으로 활용했으며, 이 모든 것을 가능하게 하는 것이 바로 프롬프트 기반 분할입니다.\n(a) 과업: 이 그림은 ‘프롬프트 기반 분할’이라는 새로운 task를 보여줍니다. 사용자가 이미지 위에 점을 찍거나(points), 네모 박스를 그리거나(box), 심지어 “검은 귀를 가진 고양이\"처럼 텍스트(text)로 프롬프트를 주면, 모델이 그에 해당하는 객체의 마스크(mask)를 출력합니다.\n(b) 모델: 이것이 바로 SAM(Segment Anything Model)의 구조입니다. 이미지를 분석하는 무거운 ‘이미지 인코더’, 사용자의 프롬프트를 이해하는 ‘프롬프트 인코더’, 그리고 이 둘을 합쳐 최종 마스크를 빠르게 만들어내는 가벼운 ‘마스크 디코더’로 구성됩니다.\n(c) 데이터: 거대한 SA-1B 데이터셋을 구축하기 위한 ‘데이터 엔진’을 보여줍니다. 모델을 이용해 데이터 주석 작업을 돕고(annotate), 그 데이터로 모델을 다시 학습시키는(train) 과정을 반복하여 11억 개가 넘는 마스크 데이터를 구축했습니다.\n기술 설명 1. 과업: 프롬프트 기반 분할 (Promptable Segmentation) 기존의 분할 모델들은 ‘고양이만 찾아내라’ 또는 ‘사람과 자동차만 구분해라’와 같이 정해진 종류의 객체만 분할할 수 있었습니다. 하지만 이 논문에서 제안하는 프롬프트 기반 분할은 “어떤 프롬프트가 주어지든 그에 해당하는 유효한 분할 마스크를 반환\"하는 것을 목표로 합니다.\n여기서 ‘유효한(valid)’ 마스크라는 점이 중요합니다. 예를 들어, 파란 셔츠를 입은 사람의 사진에서 셔츠 부분에 점 하나를 프롬프트로 찍었다고 가정해봅시다. 이 점은 ‘파란 셔츠’를 의미할 수도 있고, ‘셔츠를 입고 있는 사람 전체’를 의미할 수도 있습니다. 이렇게 프롬프트가 모호할 경우, 모델은 셔츠 마스크와 사람 마스크 둘 중 하나 이상의 합리적인 마스크를 출력해야 합니다.\n이 그림은 SAM이 어떻게 모호성을 처리하는지 잘 보여줍니다. 각 열은 하나의 이미지에 대한 결과입니다. 초록색 점(green circle)이라는 단 하나의 모호한 프롬프트가 주어졌을 때, SAM은 세 개의 서로 다른 유효한 마스크를 생성합니다. 예를 들어 첫 번째 열의 타조 이미지에서 목 부분에 점을 찍자, SAM은 타조의 머리, 목, 그리고 몸 전체에 해당하는 세 가지 마스크를 모두 제시합니다. 이를 통해 사용자는 자신이 원하는 결과를 선택할 수 있습니다.\n2. 모델: SAM (Segment Anything Model) SAM은 프롬프트 기반 분할 테스크를 실시간으로 수행하기 위해 매우 효율적인 구조로 설계되었습니다.\n이미지 인코더 (Image Encoder): 무거운 Vision Transformer(ViT) 기반의 인코더로, 입력된 이미지 전체를 한 번만 처리하여 고차원의 ‘이미지 임베딩(image embedding)‘을 생성합니다. 이 과정은 계산량이 많지만, 이미지당 한 번만 수행하면 되므로 여러 프롬프트를 처리할 때 비용을 절약할 수 있습니다.\n프롬프트 인코더 (Prompt Encoder): 점, 상자, 텍스트, 마스크 등 다양한 형태의 프롬프트를 이미지 임베딩과 결합할 수 있는 벡터 형태로 변환합니다. 예를 들어, 점이나 상자는 위치 정보를 담은 인코딩을 사용하고, 텍스트는 CLIP과 같은 언어 모델의 인코더를 활용합니다.\n마스크 디코더 (Mask Decoder): 이미지 임베딩과 프롬프트 임베딩을 입력받아 최종 마스크를 출력하는 가볍고 빠른 모듈입니다. 이 디코더는 웹 브라우저에서도 약 50ms 만에 작동할 정도로 매우 효율적이어서, 사용자가 프롬프트를 바꾸는 대로 실시간으로 결과를 보여주는 상호작용이 가능합니다. 또한, 모호한 프롬프트에 대응하기 위해 3개의 마스크와 각 마스크의 신뢰도 점수(예상 IoU)를 함께 출력합니다.\n모델 학습에는 Focal Loss와 Dice Loss를 결합한 손실 함수가 사용되었습니다.\n이 논문에서 사용된 손실 함수는 $L = \\alpha L_{focal} + \\beta L_{dice}$ 형태로 표현할 수 있습니다.\nFocal Loss ($L_{focal}$): 주로 객체 탐지에서 클래스 불균형 문제를 해결하기 위해 제안된 손실 함수입니다. 일반적인 Cross-Entropy 손실 함수에 $(1-p_t)^\\gamma$ 항을 추가하여, 모델이 이미 잘 맞추는 쉬운 샘플(예: 배경 픽셀)에는 적은 가중치를 부여하고, 맞추기 어려운 어려운 샘플(예: 객체의 경계 픽셀)에 집중하도록 만듭니다. 이를 통해 경계가 명확한 마스크를 학습하는 데 도움을 줍니다.\nDice Loss ($L_{dice}$): 예측 마스크와 실제 마스크 간의 겹치는 영역(Intersection over Union, IoU)을 직접적으로 최대화하도록 설계된 손실 함수입니다. Dice 계수 $D = \\frac{2|A \\cap B|}{|A| + |B|}$를 기반으로 하며, 손실 함수는 $L_{dice} = 1 - D$로 정의됩니다. 이 손실 함수는 분할 영역의 크기에 덜 민감하고, 심한 불균형 상황에서도 안정적인 학습을 돕습니다.\n이 두 손실 함수를 결합함으로써, 픽셀 단위의 정확성(Focal Loss)과 영역 단위의 유사성(Dice Loss)을 모두 고려하여 고품질의 마스크를 생성하도록 모델을 훈련시킵니다.\n3. 데이터 엔진 및 SA-1B 데이터셋 고품질의 파운데이션 모델을 훈련시키기 위해서는 방대하고 다양한 데이터가 필수적이지만, 이미지 분할 분야에는 웹에서 바로 수집할 수 있는 대규모 데이터가 존재하지 않았습니다. 이 문제를 해결하기 위해 저자들은 ‘데이터 엔진(Data Engine)’ 이라는 독창적인 방법을 고안했습니다.\n데이터 엔진은 3단계로 진행되었습니다.\n보조-수동 단계 (Assisted-manual): 초기 SAM 모델을 이용해 전문 작업자가 대화형으로 마스크를 생성합니다. 모델이 대략적인 마스크를 제안하면, 작업자는 클릭 몇 번으로 수정하여 빠르고 정확하게 데이터를 만듭니다. 반자동 단계 (Semi-automatic): 모델이 발전함에 따라, 이제 모델이 이미지에서 확실한 객체들을 먼저 자동으로 찾아냅니다. 작업자는 모델이 놓친, 더 작거나 까다로운 객체들에 집중하여 마스크를 추가함으로써 데이터의 다양성을 높입니다. 완전 자동 단계 (Fully automatic): 최종 단계에서는 모델이 충분히 똑똑해져서 사람의 개입 없이도 고품질의 마스크를 대량으로 생성할 수 있게 됩니다. 저자들은 이미지에 $32 \\times 32$ 격자 형태의 점 프롬프트를 자동으로 입력하고, 모호성 처리 기능을 활용하여 이미지 당 평균 100여 개의 마스크를 자동으로 생성했습니다. 이제부터 각 데이터 엔진 단계별로 실제 예시 상황과 과정을 구체적으로 살펴보겠습니다.\n1. 보조-수동 단계 (Assisted-manual) 이 단계는 똑똑한 보조 도구를 활용해 사람이 수동으로 데이터를 만드는 과정입니다.\n예시 상황: 전문 작업자에게 공원에 있는 강아지 사진 한 장이 주어졌습니다. 작업 방식: 작업자는 수작업으로 강아지 외곽선을 그리는 대신, 강아지 몸통 중앙에 점(point) 하나를 클릭합니다. 초기 SAM 모델이 즉시 그 점을 기반으로 “이것이 강아지인 것 같다\"고 추측하며 대략적인 강아지 마스크를 생성해 보여줍니다. 이 마스크는 꼬리가 잘려있고, 배경의 풀 일부가 포함되어 있을 수 있습니다. 작업자는 모델의 예측을 수정하기 위해 잘린 꼬리 부분에 점을 하나 더 클릭하고(여기도 포함해줘!), 잘못 포함된 풀 부분에 점을 클릭합니다(여기는 빼줘!). 클릭할 때마다 SAM은 실시간으로 마스크를 업데이트하고, 몇 번의 클릭만으로 픽셀 수준까지 정확한 강아지 마스크가 완성됩니다. 결과: 이 방식을 통해 처음부터 손으로 그리는 것보다 6.5배 빠르게 고품질 마스크 데이터를 수집할 수 있었습니다. 이 단계에서 수집된 데이터로 SAM 모델을 반복적으로 재학습시켜 성능을 점차 향상시켰습니다. 2. 반자동 단계 (Semi-automatic) 1단계를 거쳐 똑똑해진 모델이 먼저 쉬운 작업을 처리하고, 사람은 더 어려운 작업에 집중하는 협업 단계입니다.\n예시 상황: 작업자에게 다양한 물건이 있는 책상 사진이 주어졌습니다. 작업 방식: 작업자가 이미지를 열면, 1단계 데이터로 학습되어 더 강력해진 SAM이 이미지를 먼저 분석합니다. 모델은 자신이 90% 이상 확신하는 명확한 객체(예: 모니터, 키보드, 마우스)들의 마스크를 미리 자동으로 생성해 놓습니다. 🤖 작업자는 이미 생성된 마스크들을 검토하고, 모델이 놓친 더 작거나 복잡하고 애매한 객체(예: 엉켜있는 케이블, 펜, 책상 뒤편의 작은 화분)에만 집중하여 마스크를 추가합니다. 결과: 이 단계의 목표는 데이터의 다양성을 높이는 것입니다. 모델이 쉬운 것만 학습하지 않도록, 사람이 까다로운 객체 데이터를 보충해 줌으로써 모델이 “어떤 것이든(Anything)” 분할할 수 있는 능력을 키우게 됩니다. 3. 완전 자동 단계 (Fully automatic) 최종 단계에서는 사람의 개입 없이, 가장 강력해진 SAM 모델이 스스로 모든 데이터를 생성합니다.\n예시 상황: 모델에게 수많은 과일이 진열된 시장 사진이 주어졌습니다. 작업 방식: 시스템이 이미지 전체에 32x32 간격의 촘촘한 격자(grid) 점 프롬프트를 자동으로 입력합니다. SAM은 각 점에 대해 모호성 처리 기능을 최대한 활용합니다. 예를 들어, 포도송이의 포도알 하나에 점이 찍혔다면, 모델은 다음과 같이 여러 개의 유효한 마스크를 동시에 생성합니다. 마스크 1: 포도알 하나 마스크 2: 포도알이 속한 포도송이 전체 마스크 3: 포도송이가 놓인 과일 상자 전체 이렇게 생성된 수많은 마스크들 중에서, 모델이 스스로 예측한 신뢰도 점수(IoU)가 높고 안정적인 마스크만 남기고 나머지는 필터링합니다. 결과: 이 자동화된 과정을 통해 이미지 한 장당 평균 100여 개의 고품질 마스크를 사람의 도움 없이 대량으로 얻을 수 있었고, 최종적으로 11억 개 이상의 마스크로 구성된 SA-1B 데이터셋을 구축할 수 있었습니다. 이 과정을 통해 탄생한 것이 바로 SA-1B 데이터셋입니다. 이 데이터셋은 1100만 개의 고해상도 이미지와 총 11억 개가 넘는 분할 마스크로 구성되어 있으며, 이는 기존에 가장 컸던 분할 데이터셋보다 마스크 수 기준으로 400배 이상 많은 규모입니다.\n이 그림은 SA-1B 데이터셋에 포함된 다양한 이미지와 그 위에 오버레이된 마스크들을 보여줍니다. 위쪽으로 갈수록 이미지 당 마스크 수가 적고(\u003c50개), 아래로 갈수록 마스크 수가 많은(\u003e500개) 이미지들입니다. 이를 통해 데이터셋이 단순한 이미지부터 매우 복잡하고 객체가 많은 이미지까지 폭넓게 포함하고 있음을 알 수 있습니다.\nFigure 6은 SA-1B 데이터셋의 마스크 특성을 다른 주요 데이터셋(LVIS, COCO 등)과 비교합니다. SA-1B는 이미지당 마스크 수가 압도적으로 많고(왼쪽 그래프), 이로 인해 상대적으로 작거나 중간 크기의 마스크 비율이 높습니다(중간 그래프).\nFigure 7은 데이터셋에 포함된 이미지들의 추정된 지리적 분포를 보여줍니다. 기존 데이터셋들이 북미나 특정 지역에 편중된 경향이 있었던 반면, SA-1B는 아시아, 유럽 등 훨씬 더 다양한 국가의 이미지를 포함하여 지리적 다양성을 확보했습니다.\n실험 (Experiments) 저자들은 SAM이 훈련 데이터에서 보지 못한 새로운 종류의 이미지나 작업에 대해서도 얼마나 잘 작동하는지, 즉 제로샷 전이(Zero-shot Transfer) 성능을 평가하기 위해 다양한 실험을 진행했습니다.\n1. 단일 점 기반 마스크 생성 평가 가장 핵심적인 실험으로, 이미지에 단 하나의 점을 프롬프트로 주었을 때 얼마나 유효한 마스크를 생성하는지 평가했습니다.\n(a) 자동 평가(mIoU): 23개의 다양한 데이터셋에서 기존 최강의 대화형 분할 모델인 RITM과 성능을 비교했습니다. SAM은 23개 중 16개 데이터셋에서 더 높은 성능(IoU)을 보였습니다. 특히 ‘SAM (oracle)‘은 SAM이 출력한 3개의 마스크 중 정답과 가장 가까운 것을 선택했을 때의 성능인데, 이 경우 모든 데이터셋에서 RITM을 압도했습니다. 이는 단일 점 프롬프트가 모호하기 때문에, 자동 평가 지표만으로는 모델의 실제 성능을 제대로 측정하기 어렵다는 것을 보여줍니다.\n(b) 사람 평가: 전문 작업자가 직접 마스크의 품질을 1~10점으로 평가한 결과입니다. 모든 데이터셋에서 사람들은 SAM이 생성한 마스크의 품질이 RITM보다 월등히 높다고 평가했습니다. 이는 SAM이 IoU 점수로는 측정되지 않는, 시각적으로 더 자연스럽고 정확한 마스크를 생성한다는 것을 의미합니다.\n2. 제로샷 엣지 검출, 객체 제안 및 인스턴스 분할 SAM은 분할 모델이지만, 그 능력을 응용하여 다른 컴퓨터 비전 작업에도 제로샷으로 적용했습니다.\nSAM이 생성한 여러 마스크들의 경계를 종합하여 엣지 맵을 만들었습니다. 그 결과, 엣지 검출을 위해 전혀 훈련되지 않았음에도 불구하고, Figure 10에서 보듯이 매우 합리적인 엣지 맵을 생성했습니다. Table 3의 정량 평가에서는 최신 전문 모델보다는 성능이 낮았지만, 초기의 딥러닝 기반 엣지 검출 모델(HED)과 비슷한 수준의 성능을 보여주며 SAM의 범용성을 입증했습니다.\n이미지 내에 객체가 있을 만한 영역을 제안하는 작업에서, LVIS 데이터셋으로 훈련된 강력한 탐지 모델(ViTDet-H)과 성능을 비교했습니다. 놀랍게도 SAM은 중간 크기, 큰 크기, 그리고 희귀한 객체에 대해서는 전문 모델보다 더 높은 재현율(Average Recall)을 기록했습니다.\n객체 탐지기가 찾아낸 박스를 프롬프트로 주어 객체를 분할하는 실험입니다. Table 5의 자동 평가 점수(AP)는 전문 모델인 ViTDet-H보다 낮았지만, Figure 11의 사람 평가에서는 SAM의 마스크 품질이 더 높게 평가되었습니다. 이는 ViTDet-H가 COCO나 LVIS 데이터셋의 고유한 주석 편향(예: 마스크에 구멍이 없는 등)까지 학습하여 점수를 높인 반면, SAM은 그런 편향 없이 더 보편적으로 고품질의 마스크를 생성하기 때문으로 분석됩니다.\n3. 제로샷 텍스트-마스크 변환 개념 증명 단계의 실험으로, 텍스트 프롬프트를 사용해 객체를 분할하는 가능성을 보여주었습니다.\nSAM은 “a wheel(바퀴)” 같은 간단한 텍스트뿐만 아니라, “beaver tooth grille(비버 이빨 그릴)“과 같은 구체적이고 미묘한 표현까지 이해하고 해당 객체를 정확하게 분할해냈습니다. 텍스트만으로 실패할 경우, 점 프롬프트를 추가하여 정확도를 높일 수도 있습니다.\n","wordCount":"1691","inLanguage":"en","datePublished":"2025-09-30T00:00:00Z","dateModified":"2025-09-30T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-sam/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Segment Anything 논문 리뷰</h1><div class=post-meta><span title='2025-09-30 00:00:00 +0000 UTC'>September 30, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1691 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>이 논문은 이미지 분할(Image Segmentation) 분야에 <strong>&lsquo;파운데이션 모델(Foundation Model)&rsquo;</strong> 이라는 새로운 패러다임을 제시하는 &ldquo;Segment Anything (SA)&ldquo;를 소개합니다. 이미지 분할이란, 이미지 속 각 픽셀이 어떤 객체에 속하는지 구분하여 영역을 나누는 기술입니다. 예를 들어, 고양이 사진에서 픽셀 단위로 &lsquo;여기는 고양이&rsquo;, &lsquo;여기는 배경&rsquo;이라고 알려주는 것과 같습니다.</p><p>최근 자연어 처리(NLP) 분야에서는 GPT와 같이 웹 규모의 방대한 데이터로 사전 훈련된 **&lsquo;파운데이션 모델&rsquo;**이 등장하여, 특정 작업에 대한 추가 훈련(fine-tuning) 없이도 &lsquo;프롬프트(prompt)&lsquo;만으로 다양한 작업을 놀랍도록 잘 수행해내고 있습니다. 이 논문의 저자들은 이러한 성공에 영감을 받아, 이미지 분할 분야에도 적용할 수 있는 범용 모델을 만들고자 했습니다.</p><p>이를 위해 저자들은 다음 세 가지 핵심 요소를 새롭게 정의하고 개발했습니다.</p><ol><li><strong>새로운 과업(Task):</strong> 프롬프트 기반 분할 (Promptable Segmentation)</li><li><strong>새로운 모델(Model):</strong> Segment Anything Model (SAM)</li><li><strong>새로운 데이터셋(Data):</strong> Segment Anything 1-Billion (SA-1B)</li></ol><p>이 세 가지는 서로 맞물려 돌아가는 톱니바퀴처럼 작용합니다. 좋은 모델(SAM)을 만들기 위해 방대한 데이터(SA-1B)가 필요하고, 이 데이터를 효율적으로 구축하기 위해 좋은 모델(SAM)을 &lsquo;데이터 엔진&rsquo;으로 활용했으며, 이 모든 것을 가능하게 하는 것이 바로 프롬프트 기반 분할입니다.</p><hr><p><img alt="Figure 1: Segment Anything 프로젝트 개요" loading=lazy src=/images/posts/segment-anything/figure1.png></p><p><strong>(a) 과업:</strong> 이 그림은 &lsquo;프롬프트 기반 분할&rsquo;이라는 새로운 task를 보여줍니다. 사용자가 이미지 위에 점을 찍거나(points), 네모 박스를 그리거나(box), 심지어 &ldquo;검은 귀를 가진 고양이"처럼 텍스트(text)로 프롬프트를 주면, 모델이 그에 해당하는 객체의 마스크(mask)를 출력합니다.</p><p><strong>(b) 모델:</strong> 이것이 바로 SAM(Segment Anything Model)의 구조입니다. 이미지를 분석하는 무거운 &lsquo;이미지 인코더&rsquo;, 사용자의 프롬프트를 이해하는 &lsquo;프롬프트 인코더&rsquo;, 그리고 이 둘을 합쳐 최종 마스크를 빠르게 만들어내는 가벼운 &lsquo;마스크 디코더&rsquo;로 구성됩니다.</p><p><strong>(c) 데이터:</strong> 거대한 SA-1B 데이터셋을 구축하기 위한 &lsquo;데이터 엔진&rsquo;을 보여줍니다. 모델을 이용해 데이터 주석 작업을 돕고(annotate), 그 데이터로 모델을 다시 학습시키는(train) 과정을 반복하여 11억 개가 넘는 마스크 데이터를 구축했습니다.</p><hr><h3 id=기술-설명>기술 설명<a hidden class=anchor aria-hidden=true href=#기술-설명>#</a></h3><h4 id=1-과업-프롬프트-기반-분할-promptable-segmentation>1. 과업: 프롬프트 기반 분할 (Promptable Segmentation)<a hidden class=anchor aria-hidden=true href=#1-과업-프롬프트-기반-분할-promptable-segmentation>#</a></h4><p>기존의 분할 모델들은 &lsquo;고양이만 찾아내라&rsquo; 또는 &lsquo;사람과 자동차만 구분해라&rsquo;와 같이 정해진 종류의 객체만 분할할 수 있었습니다. 하지만 이 논문에서 제안하는 <strong>프롬프트 기반 분할</strong>은 &ldquo;어떤 프롬프트가 주어지든 그에 해당하는 유효한 분할 마스크를 반환"하는 것을 목표로 합니다.</p><p>여기서 &lsquo;유효한(valid)&rsquo; 마스크라는 점이 중요합니다. 예를 들어, 파란 셔츠를 입은 사람의 사진에서 셔츠 부분에 점 하나를 프롬프트로 찍었다고 가정해봅시다. 이 점은 &lsquo;파란 셔츠&rsquo;를 의미할 수도 있고, &lsquo;셔츠를 입고 있는 사람 전체&rsquo;를 의미할 수도 있습니다. 이렇게 프롬프트가 모호할 경우, 모델은 셔츠 마스크와 사람 마스크 둘 중 하나 이상의 합리적인 마스크를 출력해야 합니다.</p><p><img alt="Figure 3: 모호한 프롬프트에 대한 SAM의 다중 마스크 생성" loading=lazy src=/images/posts/segment-anything/figure3.png></p><p>이 그림은 SAM이 어떻게 모호성을 처리하는지 잘 보여줍니다. 각 열은 하나의 이미지에 대한 결과입니다. 초록색 점(green circle)이라는 단 하나의 모호한 프롬프트가 주어졌을 때, SAM은 세 개의 서로 다른 유효한 마스크를 생성합니다. 예를 들어 첫 번째 열의 타조 이미지에서 목 부분에 점을 찍자, SAM은 타조의 머리, 목, 그리고 몸 전체에 해당하는 세 가지 마스크를 모두 제시합니다. 이를 통해 사용자는 자신이 원하는 결과를 선택할 수 있습니다.</p><h4 id=2-모델-sam-segment-anything-model>2. 모델: SAM (Segment Anything Model)<a hidden class=anchor aria-hidden=true href=#2-모델-sam-segment-anything-model>#</a></h4><p>SAM은 프롬프트 기반 분할 테스크를 실시간으로 수행하기 위해 매우 효율적인 구조로 설계되었습니다.</p><p><img alt="Figure 4: SAM 모델 구조 개요" loading=lazy src=/images/posts/segment-anything/figure4.png></p><ul><li><p><strong>이미지 인코더 (Image Encoder):</strong> 무거운 Vision Transformer(ViT) 기반의 인코더로, 입력된 이미지 전체를 한 번만 처리하여 고차원의 &lsquo;이미지 임베딩(image embedding)&lsquo;을 생성합니다. 이 과정은 계산량이 많지만, 이미지당 한 번만 수행하면 되므로 여러 프롬프트를 처리할 때 비용을 절약할 수 있습니다.</p></li><li><p><strong>프롬프트 인코더 (Prompt Encoder):</strong> 점, 상자, 텍스트, 마스크 등 다양한 형태의 프롬프트를 이미지 임베딩과 결합할 수 있는 벡터 형태로 변환합니다. 예를 들어, 점이나 상자는 위치 정보를 담은 인코딩을 사용하고, 텍스트는 CLIP과 같은 언어 모델의 인코더를 활용합니다.</p></li><li><p><strong>마스크 디코더 (Mask Decoder):</strong> 이미지 임베딩과 프롬프트 임베딩을 입력받아 최종 마스크를 출력하는 가볍고 빠른 모듈입니다. 이 디코더는 웹 브라우저에서도 약 50ms 만에 작동할 정도로 매우 효율적이어서, 사용자가 프롬프트를 바꾸는 대로 실시간으로 결과를 보여주는 상호작용이 가능합니다. 또한, 모호한 프롬프트에 대응하기 위해 3개의 마스크와 각 마스크의 신뢰도 점수(예상 IoU)를 함께 출력합니다.</p></li></ul><p>모델 학습에는 <strong>Focal Loss</strong>와 <strong>Dice Loss</strong>를 결합한 손실 함수가 사용되었습니다.</p><p>이 논문에서 사용된 손실 함수는 $L = \alpha L_{focal} + \beta L_{dice}$ 형태로 표현할 수 있습니다.</p><ul><li><p><strong>Focal Loss ($L_{focal}$):</strong> 주로 객체 탐지에서 클래스 불균형 문제를 해결하기 위해 제안된 손실 함수입니다. 일반적인 Cross-Entropy 손실 함수에 $(1-p_t)^\gamma$ 항을 추가하여, 모델이 이미 잘 맞추는 쉬운 샘플(예: 배경 픽셀)에는 적은 가중치를 부여하고, 맞추기 어려운 어려운 샘플(예: 객체의 경계 픽셀)에 집중하도록 만듭니다. 이를 통해 경계가 명확한 마스크를 학습하는 데 도움을 줍니다.</p></li><li><p><strong>Dice Loss ($L_{dice}$):</strong> 예측 마스크와 실제 마스크 간의 겹치는 영역(Intersection over Union, IoU)을 직접적으로 최대화하도록 설계된 손실 함수입니다. Dice 계수 $D = \frac{2|A \cap B|}{|A| + |B|}$를 기반으로 하며, 손실 함수는 $L_{dice} = 1 - D$로 정의됩니다. 이 손실 함수는 분할 영역의 크기에 덜 민감하고, 심한 불균형 상황에서도 안정적인 학습을 돕습니다.</p></li></ul><p>이 두 손실 함수를 결합함으로써, 픽셀 단위의 정확성(Focal Loss)과 영역 단위의 유사성(Dice Loss)을 모두 고려하여 고품질의 마스크를 생성하도록 모델을 훈련시킵니다.</p><h4 id=3-데이터-엔진-및-sa-1b-데이터셋>3. 데이터 엔진 및 SA-1B 데이터셋<a hidden class=anchor aria-hidden=true href=#3-데이터-엔진-및-sa-1b-데이터셋>#</a></h4><p>고품질의 파운데이션 모델을 훈련시키기 위해서는 방대하고 다양한 데이터가 필수적이지만, 이미지 분할 분야에는 웹에서 바로 수집할 수 있는 대규모 데이터가 존재하지 않았습니다. 이 문제를 해결하기 위해 저자들은 <strong>&lsquo;데이터 엔진(Data Engine)&rsquo;</strong> 이라는 독창적인 방법을 고안했습니다.</p><p>데이터 엔진은 3단계로 진행되었습니다.</p><ol><li><strong>보조-수동 단계 (Assisted-manual):</strong> 초기 SAM 모델을 이용해 전문 작업자가 대화형으로 마스크를 생성합니다. 모델이 대략적인 마스크를 제안하면, 작업자는 클릭 몇 번으로 수정하여 빠르고 정확하게 데이터를 만듭니다.</li><li><strong>반자동 단계 (Semi-automatic):</strong> 모델이 발전함에 따라, 이제 모델이 이미지에서 확실한 객체들을 먼저 자동으로 찾아냅니다. 작업자는 모델이 놓친, 더 작거나 까다로운 객체들에 집중하여 마스크를 추가함으로써 데이터의 다양성을 높입니다.</li><li><strong>완전 자동 단계 (Fully automatic):</strong> 최종 단계에서는 모델이 충분히 똑똑해져서 사람의 개입 없이도 고품질의 마스크를 대량으로 생성할 수 있게 됩니다. 저자들은 이미지에 $32 \times 32$ 격자 형태의 점 프롬프트를 자동으로 입력하고, 모호성 처리 기능을 활용하여 이미지 당 평균 100여 개의 마스크를 자동으로 생성했습니다.</li></ol><hr><p>이제부터 각 데이터 엔진 단계별로 실제 예시 상황과 과정을 구체적으로 살펴보겠습니다.</p><hr><h4 id=1-보조-수동-단계-assisted-manual>1. 보조-수동 단계 (Assisted-manual)<a hidden class=anchor aria-hidden=true href=#1-보조-수동-단계-assisted-manual>#</a></h4><p>이 단계는 똑똑한 보조 도구를 활용해 사람이 수동으로 데이터를 만드는 과정입니다.</p><ul><li><strong>예시 상황:</strong> 전문 작업자에게 <strong>공원에 있는 강아지 사진 한 장</strong>이 주어졌습니다.</li><li><strong>작업 방식:</strong><ol><li>작업자는 수작업으로 강아지 외곽선을 그리는 대신, <strong>강아지 몸통 중앙에 점(point) 하나</strong>를 클릭합니다.</li><li>초기 SAM 모델이 즉시 그 점을 기반으로 &ldquo;이것이 강아지인 것 같다"고 추측하며 <strong>대략적인 강아지 마스크</strong>를 생성해 보여줍니다. 이 마스크는 꼬리가 잘려있고, 배경의 풀 일부가 포함되어 있을 수 있습니다.</li><li>작업자는 모델의 예측을 수정하기 위해 <strong>잘린 꼬리 부분에 점을 하나 더 클릭</strong>하고(여기도 포함해줘!), <strong>잘못 포함된 풀 부분에 점을 클릭</strong>합니다(여기는 빼줘!).</li><li>클릭할 때마다 SAM은 실시간으로 마스크를 업데이트하고, 몇 번의 클릭만으로 픽셀 수준까지 정확한 강아지 마스크가 완성됩니다.</li></ol></li><li><strong>결과:</strong> 이 방식을 통해 처음부터 손으로 그리는 것보다 <strong>6.5배 빠르게</strong> 고품질 마스크 데이터를 수집할 수 있었습니다. 이 단계에서 수집된 데이터로 SAM 모델을 반복적으로 재학습시켜 성능을 점차 향상시켰습니다.</li></ul><hr><h4 id=2-반자동-단계-semi-automatic>2. 반자동 단계 (Semi-automatic)<a hidden class=anchor aria-hidden=true href=#2-반자동-단계-semi-automatic>#</a></h4><p>1단계를 거쳐 똑똑해진 모델이 먼저 쉬운 작업을 처리하고, 사람은 더 어려운 작업에 집중하는 협업 단계입니다.</p><ul><li><strong>예시 상황:</strong> 작업자에게 <strong>다양한 물건이 있는 책상 사진</strong>이 주어졌습니다.</li><li><strong>작업 방식:</strong><ol><li>작업자가 이미지를 열면, 1단계 데이터로 학습되어 더 강력해진 SAM이 <strong>이미지를 먼저 분석</strong>합니다.</li><li>모델은 자신이 <strong>90% 이상 확신하는 명확한 객체</strong>(예: 모니터, 키보드, 마우스)들의 마스크를 <strong>미리 자동으로 생성</strong>해 놓습니다. 🤖</li><li>작업자는 이미 생성된 마스크들을 검토하고, 모델이 놓친 <strong>더 작거나 복잡하고 애매한 객체</strong>(예: 엉켜있는 케이블, 펜, 책상 뒤편의 작은 화분)에만 집중하여 마스크를 추가합니다.</li></ol></li><li><strong>결과:</strong> 이 단계의 목표는 <strong>데이터의 다양성을 높이는 것</strong>입니다. 모델이 쉬운 것만 학습하지 않도록, 사람이 까다로운 객체 데이터를 보충해 줌으로써 모델이 &ldquo;어떤 것이든(Anything)&rdquo; 분할할 수 있는 능력을 키우게 됩니다.</li></ul><hr><h4 id=3-완전-자동-단계-fully-automatic>3. 완전 자동 단계 (Fully automatic)<a hidden class=anchor aria-hidden=true href=#3-완전-자동-단계-fully-automatic>#</a></h4><p>최종 단계에서는 사람의 개입 없이, 가장 강력해진 SAM 모델이 스스로 모든 데이터를 생성합니다.</p><ul><li><strong>예시 상황:</strong> 모델에게 <strong>수많은 과일이 진열된 시장 사진</strong>이 주어졌습니다.</li><li><strong>작업 방식:</strong><ol><li>시스템이 이미지 전체에 <strong>32x32 간격의 촘촘한 격자(grid) 점 프롬프트</strong>를 자동으로 입력합니다.</li><li>SAM은 각 점에 대해 <strong>모호성 처리 기능</strong>을 최대한 활용합니다. 예를 들어, 포도송이의 포도알 하나에 점이 찍혔다면, 모델은 다음과 같이 여러 개의 유효한 마스크를 동시에 생성합니다.<ul><li>마스크 1: 포도알 하나</li><li>마스크 2: 포도알이 속한 포도송이 전체</li><li>마스크 3: 포도송이가 놓인 과일 상자 전체</li></ul></li><li>이렇게 생성된 수많은 마스크들 중에서, 모델이 스스로 예측한 신뢰도 점수(IoU)가 높고 안정적인 마스크만 남기고 나머지는 필터링합니다.</li></ol></li><li><strong>결과:</strong> 이 자동화된 과정을 통해 이미지 한 장당 <strong>평균 100여 개의 고품질 마스크</strong>를 사람의 도움 없이 대량으로 얻을 수 있었고, 최종적으로 <strong>11억 개 이상의 마스크</strong>로 구성된 SA-1B 데이터셋을 구축할 수 있었습니다.</li></ul><p>이 과정을 통해 탄생한 것이 바로 <strong>SA-1B 데이터셋</strong>입니다. 이 데이터셋은 1100만 개의 고해상도 이미지와 총 11억 개가 넘는 분할 마스크로 구성되어 있으며, 이는 기존에 가장 컸던 분할 데이터셋보다 마스크 수 기준으로 400배 이상 많은 규모입니다.</p><p><img alt="Figure 2: SA-1B 데이터셋 예시" loading=lazy src=/images/posts/segment-anything/figure2.png></p><p>이 그림은 SA-1B 데이터셋에 포함된 다양한 이미지와 그 위에 오버레이된 마스크들을 보여줍니다. 위쪽으로 갈수록 이미지 당 마스크 수가 적고(&lt;50개), 아래로 갈수록 마스크 수가 많은(>500개) 이미지들입니다. 이를 통해 데이터셋이 단순한 이미지부터 매우 복잡하고 객체가 많은 이미지까지 폭넓게 포함하고 있음을 알 수 있습니다.</p><p><img alt="Figure 6 & 7: 데이터셋 특성 분석" loading=lazy src=/images/posts/segment-anything/figure6_7.png></p><p><strong>Figure 6</strong>은 SA-1B 데이터셋의 마스크 특성을 다른 주요 데이터셋(LVIS, COCO 등)과 비교합니다. SA-1B는 이미지당 마스크 수가 압도적으로 많고(왼쪽 그래프), 이로 인해 상대적으로 작거나 중간 크기의 마스크 비율이 높습니다(중간 그래프).</p><p><strong>Figure 7</strong>은 데이터셋에 포함된 이미지들의 추정된 지리적 분포를 보여줍니다. 기존 데이터셋들이 북미나 특정 지역에 편중된 경향이 있었던 반면, SA-1B는 아시아, 유럽 등 훨씬 더 다양한 국가의 이미지를 포함하여 지리적 다양성을 확보했습니다.</p><hr><h3 id=실험-experiments>실험 (Experiments)<a hidden class=anchor aria-hidden=true href=#실험-experiments>#</a></h3><p>저자들은 SAM이 훈련 데이터에서 보지 못한 새로운 종류의 이미지나 작업에 대해서도 얼마나 잘 작동하는지, 즉 <strong>제로샷 전이(Zero-shot Transfer)</strong> 성능을 평가하기 위해 다양한 실험을 진행했습니다.</p><h4 id=1-단일-점-기반-마스크-생성-평가>1. 단일 점 기반 마스크 생성 평가<a hidden class=anchor aria-hidden=true href=#1-단일-점-기반-마스크-생성-평가>#</a></h4><p>가장 핵심적인 실험으로, 이미지에 단 하나의 점을 프롬프트로 주었을 때 얼마나 유효한 마스크를 생성하는지 평가했습니다.</p><p><img alt="Figure 9: 단일 점 프롬프트 평가 결과" loading=lazy src=/images/posts/segment-anything/figure9.png></p><ul><li><p><strong>(a) 자동 평가(mIoU):</strong> 23개의 다양한 데이터셋에서 기존 최강의 대화형 분할 모델인 RITM과 성능을 비교했습니다. SAM은 23개 중 16개 데이터셋에서 더 높은 성능(IoU)을 보였습니다. 특히 &lsquo;SAM (oracle)&lsquo;은 SAM이 출력한 3개의 마스크 중 정답과 가장 가까운 것을 선택했을 때의 성능인데, 이 경우 모든 데이터셋에서 RITM을 압도했습니다. 이는 단일 점 프롬프트가 모호하기 때문에, 자동 평가 지표만으로는 모델의 실제 성능을 제대로 측정하기 어렵다는 것을 보여줍니다.</p></li><li><p><strong>(b) 사람 평가:</strong> 전문 작업자가 직접 마스크의 품질을 1~10점으로 평가한 결과입니다. 모든 데이터셋에서 사람들은 SAM이 생성한 마스크의 품질이 RITM보다 월등히 높다고 평가했습니다. 이는 SAM이 IoU 점수로는 측정되지 않는, 시각적으로 더 자연스럽고 정확한 마스크를 생성한다는 것을 의미합니다.</p></li></ul><h4 id=2-제로샷-엣지-검출-객체-제안-및-인스턴스-분할>2. 제로샷 엣지 검출, 객체 제안 및 인스턴스 분할<a hidden class=anchor aria-hidden=true href=#2-제로샷-엣지-검출-객체-제안-및-인스턴스-분할>#</a></h4><p>SAM은 분할 모델이지만, 그 능력을 응용하여 다른 컴퓨터 비전 작업에도 제로샷으로 적용했습니다.</p><p><img alt="Table 3 & Figure 10: 제로샷 엣지 검출" loading=lazy src=/images/posts/segment-anything/figure10_table3.png></p><p>SAM이 생성한 여러 마스크들의 경계를 종합하여 엣지 맵을 만들었습니다. 그 결과, 엣지 검출을 위해 전혀 훈련되지 않았음에도 불구하고, <strong>Figure 10</strong>에서 보듯이 매우 합리적인 엣지 맵을 생성했습니다. <strong>Table 3</strong>의 정량 평가에서는 최신 전문 모델보다는 성능이 낮았지만, 초기의 딥러닝 기반 엣지 검출 모델(HED)과 비슷한 수준의 성능을 보여주며 SAM의 범용성을 입증했습니다.</p><p><img alt="Table 4: 제로샷 객체 제안" loading=lazy src=/images/posts/segment-anything/table4.png></p><p>이미지 내에 객체가 있을 만한 영역을 제안하는 작업에서, LVIS 데이터셋으로 훈련된 강력한 탐지 모델(ViTDet-H)과 성능을 비교했습니다. 놀랍게도 SAM은 중간 크기, 큰 크기, 그리고 희귀한 객체에 대해서는 전문 모델보다 더 높은 재현율(Average Recall)을 기록했습니다.</p><p><img alt="Table 5 & Figure 11: 제로샷 인스턴스 분할" loading=lazy src=/images/posts/segment-anything/figure11_table5.png></p><p>객체 탐지기가 찾아낸 박스를 프롬프트로 주어 객체를 분할하는 실험입니다. <strong>Table 5</strong>의 자동 평가 점수(AP)는 전문 모델인 ViTDet-H보다 낮았지만, <strong>Figure 11</strong>의 사람 평가에서는 SAM의 마스크 품질이 더 높게 평가되었습니다. 이는 ViTDet-H가 COCO나 LVIS 데이터셋의 고유한 주석 편향(예: 마스크에 구멍이 없는 등)까지 학습하여 점수를 높인 반면, SAM은 그런 편향 없이 더 보편적으로 고품질의 마스크를 생성하기 때문으로 분석됩니다.</p><h4 id=3-제로샷-텍스트-마스크-변환>3. 제로샷 텍스트-마스크 변환<a hidden class=anchor aria-hidden=true href=#3-제로샷-텍스트-마스크-변환>#</a></h4><p>개념 증명 단계의 실험으로, 텍스트 프롬프트를 사용해 객체를 분할하는 가능성을 보여주었습니다.</p><p><img alt="Figure 12: 제로샷 텍스트-마스크 변환 예시" loading=lazy src=/images/posts/segment-anything/figure12.png></p><p>SAM은 &ldquo;a wheel(바퀴)&rdquo; 같은 간단한 텍스트뿐만 아니라, &ldquo;beaver tooth grille(비버 이빨 그릴)&ldquo;과 같은 구체적이고 미묘한 표현까지 이해하고 해당 객체를 정확하게 분할해냈습니다. 텍스트만으로 실패할 경우, 점 프롬프트를 추가하여 정확도를 높일 수도 있습니다.</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://mookjsi.github.io/tags/image-segmentation/>Image Segmentation</a></li><li><a href=https://mookjsi.github.io/tags/foundation-model/>Foundation Model</a></li><li><a href=https://mookjsi.github.io/tags/sam/>SAM</a></li><li><a href=https://mookjsi.github.io/tags/sa-1b/>SA-1B</a></li><li><a href=https://mookjsi.github.io/tags/iccv-2023/>ICCV 2023</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/paper-review-simclr/><span class=title>« Prev</span><br><span>SimCLR 논문 리뷰</span>
</a><a class=next href=https://mookjsi.github.io/posts/paper-review-detr/><span class=title>Next »</span><br><span>DETR: End-to-End Object Detection with Transformers</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on x" href="https://x.com/intent/tweet/?text=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f&amp;hashtags=PaperReview%2cDeepLearning%2cComputerVision%2cImageSegmentation%2cFoundationModel%2cSAM%2cSA-1B%2cICCV2023"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f&amp;title=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;summary=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f&title=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on whatsapp" href="https://api.whatsapp.com/send?text=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on telegram" href="https://telegram.me/share/url?text=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Segment Anything 논문 리뷰 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Segment%20Anything%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-sam%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>