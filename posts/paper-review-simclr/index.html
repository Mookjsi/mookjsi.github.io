<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SimCLR 논문 리뷰 | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Computer Vision,Self-Supervised Learning,Contrastive Learning,SimCLR,ICML 2020"><meta name=description content="ICML 2020에서 발표된 &lsquo;A Simple Framework for Contrastive Learning of Visual Representations&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 레이블 없는 데이터로부터 컴퓨터가 스스로 이미지의 특징을 학습하는 자기 지도 학습(Self-Supervised Learning) 방법론인 SimCLR의 핵심 아이디어, 프레임워크 구성 요소, 그리고 실험 결과를 쉽게 풀어 설명합니다."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-simclr/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-simclr/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-simclr/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="SimCLR 논문 리뷰"><meta property="og:description" content="ICML 2020에서 발표된 ‘A Simple Framework for Contrastive Learning of Visual Representations’ 논문에 대한 심층 리뷰입니다. 이 포스트에서는 레이블 없는 데이터로부터 컴퓨터가 스스로 이미지의 특징을 학습하는 자기 지도 학습(Self-Supervised Learning) 방법론인 SimCLR의 핵심 아이디어, 프레임워크 구성 요소, 그리고 실험 결과를 쉽게 풀어 설명합니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-28T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Self-Supervised Learning"><meta property="article:tag" content="Contrastive Learning"><meta property="article:tag" content="SimCLR"><meta name=twitter:card content="summary"><meta name=twitter:title content="SimCLR 논문 리뷰"><meta name=twitter:description content="ICML 2020에서 발표된 &lsquo;A Simple Framework for Contrastive Learning of Visual Representations&rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 레이블 없는 데이터로부터 컴퓨터가 스스로 이미지의 특징을 학습하는 자기 지도 학습(Self-Supervised Learning) 방법론인 SimCLR의 핵심 아이디어, 프레임워크 구성 요소, 그리고 실험 결과를 쉽게 풀어 설명합니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"SimCLR 논문 리뷰","item":"https://mookjsi.github.io/posts/paper-review-simclr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SimCLR 논문 리뷰","name":"SimCLR 논문 리뷰","description":"ICML 2020에서 발표된 \u0026lsquo;A Simple Framework for Contrastive Learning of Visual Representations\u0026rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 레이블 없는 데이터로부터 컴퓨터가 스스로 이미지의 특징을 학습하는 자기 지도 학습(Self-Supervised Learning) 방법론인 SimCLR의 핵심 아이디어, 프레임워크 구성 요소, 그리고 실험 결과를 쉽게 풀어 설명합니다.","keywords":["Paper Review","Deep Learning","Computer Vision","Self-Supervised Learning","Contrastive Learning","SimCLR","ICML 2020"],"articleBody":"이 논문은 사람이 일일이 사진에 “이건 고양이야”, “저건 강아지야\"라고 알려주지 않아도, 컴퓨터가 스스로 이미지의 특징을 학습하는 방법에 대한 연구입니다. 특히 아주 간단하면서도 효과적인 방법을 제시하여 큰 주목을 받았습니다.\n컴퓨터에게 사진을 주고 이게 뭔지 맞혀보라고 시키는 걸 **지도 학습(Supervised Learning)**이라 합니다. 정답을 알려주면서 공부시키는 거죠. 하지만 세상에는 정답이 없는 사진이 훨씬 많습니다. 이 정답 없는 사진들로 컴퓨터를 똑똑하게 만들 순 없을까요? 이게 바로 **‘자기 지도 학습(Self-Supervised Learning)’**의 목표입니다.\nSimCLR이 사용하는 방법은 **‘대조 학습(Contrastive Learning)’**이라는 건데요, 아주 간단한 아이디어에서 출발합니다.\n예시: 강아지 사진이 한 장 있다고 상상해 봅시다.\n이 사진을 가지고 약간 다른 버전의 사진 두 장을 만듭니다. 예를 들어, 한 장은 강아지 얼굴만 확대(crop)하고, 다른 한 장은 색감을 살짝 바꾸는(color distortion) 거죠. 컴퓨터에게 이 두 사진은 ‘같은 강아지’에서 나온 긍정적 쌍(positive pair) 이라고 알려줍니다. 그리고 동시에, 전혀 다른 사진들(예: 고양이, 자동차, 나무 사진)을 보여주면서, 이것들은 방금 본 강아지 사진과 다른 부정적 쌍(negative pair) 이라고 알려줍니다. 이 과정을 수많은 사진으로 반복하면, 컴퓨터는 점차 ‘어떤 특징’이 같은 대상을 나타내고, ‘어떤 특징’이 다른 대상을 나타내는지 스스로 배우게 됩니다. 즉, ‘강아지다움’이 무엇인지 그 본질적인 **표현(representation)**을 학습하게 되는 거죠. SimCLR은 이 간단한 원리를 매우 효과적으로 구현한 프레임워크입니다.\n참고: Stochastic Optimization Review (표현 학습 관련 발표 자료) 1. 서론 (Introduction) 연구의 시작은 “사람의 정답 없이 어떻게 컴퓨터가 **시각적 표현(visual representation)**을 배울 수 있을까?“라는 오랜 질문에서 출발합니다. 기존의 방법들은 복잡한 구조를 필요로 하거나, 대량의 이전 데이터를 저장해두는 메모리 뱅크(memory bank) 같은 부가적인 장치가 필요했습니다.\nFigure 1은 SimCLR이 나오기 전까지의 여러 자기 지도 학습 모델들의 성능을 보여주는 그래프입니다. 가로축은 모델의 크기(Number of Parameters), 세로축은 이미지 분류 정확도(ImageNet Top-1 Accuracy)를 나타냅니다. 이 그래프에서 **SimCLR(별 모양 ★)**은 다른 모델들(점 모양 •)에 비해 훨씬 적은 파라미터로 더 높은 정확도를 달성하며, 심지어 사람이 정답을 알려주고 학습시킨 **지도 학습 모델(Supervised, 회색 십자가)**의 성능과 맞먹는 것을 보여줍니다. 이는 매우 간단한 프레임워크가 기존의 복잡한 방법들을 뛰어넘을 수 있음을 시사합니다.\n이 논문은 SimCLR의 성공 비결이 다음 세 가지 핵심 요소의 조합에 있음을 체계적인 실험을 통해 보여줍니다.\n데이터 증강 기법의 조합이 효과적인 표현을 학습하는 데 매우 중요하다. 모델의 최종 표현(representation)과 손실 함수(loss function) 사이에 **비선형 변환(nonlinear transformation)**을 추가하는 것이 성능을 크게 향상시킨다. 대조 학습은 지도 학습보다 더 큰 **배치 크기(batch size)**와 더 긴 학습 시간으로부터 더 큰 이득을 얻는다. 2. 기술 설명 (Technical Description) SimCLR 프레임워크는 네 가지 주요 구성 요소로 이루어져 있습니다.\nFigure 2는 이 전체 과정을 아주 잘 보여주는 그림입니다. 하나의 이미지 $x$에서 시작해서, 두 개의 서로 다른 데이터 증강(augmentation) $t$와 $t’$를 적용하여 두 개의 새로운 이미지 $\\tilde{x}_i$와 $\\tilde{x}_j$를 만듭니다. 이 둘은 서로 연관된 **‘긍정적 쌍’**이 됩니다. 이 두 이미지는 동일한 인코더 네트워크(encoder network) $f(\\cdot)$를 통과하여 각각의 표현 벡터(representation vector) $h_i$와 $h_j$로 변환됩니다. 그 후, 이 표현 벡터들은 또 다른 작은 신경망인 투영 헤드(projection head) $g(\\cdot)$를 거쳐 최종적으로 $z_i$와 $z_j$라는 벡터로 변환됩니다. 학습의 목표는 이 $z_i$와 $z_j$ 사이의 **유사도(agreement)**를 최대화하는 것입니다. 학습이 끝나면 투영 헤드 $g(\\cdot)$는 버리고, 표현 벡터 $h$를 다른 작업(예: 이미지 분류)에 사용합니다.\n데이터 증강 (Data Augmentation): 하나의 이미지로부터 두 개의 연관된 뷰(view)를 생성하는 과정입니다. 이 논문에서는 무작위 자르기 후 원래 크기로 복원(random cropping and resize), 색상 왜곡(color distortion), **가우시안 블러(Gaussian blur)**를 순차적으로 적용합니다. 어떤 증강 기법을 어떻게 조합하는지가 모델 성능의 핵심입니다.\n기본 인코더 (Base Encoder): 증강된 이미지로부터 표현 벡터 $h_i$를 추출하는 신경망입니다. 이 논문에서는 주로 ResNet 구조를 사용하며, 특별한 구조적 제약이 없어 어떤 신경망이든 사용할 수 있습니다. $h_i = f(\\tilde{x}_i)$ 입니다.\n투영 헤드 (Projection Head): 인코더에서 나온 표현 벡터 $h$를 **대조 손실(contrastive loss)**을 계산하는 공간으로 매핑하는 작은 신경망 $g(\\cdot)$입니다. 이 논문에서는 하나의 **은닉층(hidden layer)**을 가진 **MLP(Multi-Layer Perceptron)**를 사용합니다. $z_i = g(h_i)$ 이며, $h_i$가 아닌 $z_i$에 손실 함수를 적용하는 것이 성능 향상에 큰 도움이 된다는 것을 발견했습니다.\n대조 손실 함수 (Contrastive Loss Function): 주어진 ‘긍정적 쌍’($z_i$, $z_j$)의 유사도는 높이고, 나머지 모든 ‘부정적 쌍’과의 유사도는 낮추도록 학습시키는 함수입니다. 이 논문에서는 NT-Xent (Normalized Temperature-scaled Cross Entropy) 손실을 사용합니다.\nAlgorithm 1은 이 SimCLR 학습 과정을 단계별로 요약한 것입니다. N개의 이미지를 배치로 가져와 각 이미지마다 두 개의 증강된 뷰를 만들고, 인코더와 투영 헤드를 통과시켜 2N개의 $z$ 벡터를 얻습니다. 그런 다음 모든 벡터 쌍 간의 유사도($s_{i,j}$)를 계산하고, 이를 바탕으로 손실($l(i,j)$)을 계산하여 네트워크 $f$와 $g$를 업데이트합니다.\n수식에 대해 좀 더 깊이 들어가 보겠습니다. NT-Xent 손실 함수는 다음과 같이 정의됩니다. $$ l_{ij} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{I}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)} $$ $z_i$와 $z_j$는 각각 증강된 이미지 $\\tilde{x}_i$와 $\\tilde{x}_j$로부터 추출된 $l_2$ 정규화된(normalized) 표현 벡터입니다. 이 둘은 ‘긍정적 쌍’입니다. $\\text{sim}(u, v) = u^T v / (||u|| ||v||)$는 두 벡터 간의 **코사인 유사도(cosine similarity)**를 계산합니다. $\\tau$ (타우)는 온도(temperature) 하이퍼파라미터로, 유사도 분포를 얼마나 뾰족하게 만들지 조절합니다. $\\tau$가 작을수록 모델은 어려운 부정적 예시(hard negatives, 즉 긍정적 예시와 유사도가 높은 부정적 예시)에 더 집중하게 됩니다. 이는 손실 함수의 기울기(gradient)에서 부정적 예시들에 대한 가중치를 조절하는 효과를 낳습니다. 분자 $\\exp(\\text{sim}(z_i, z_j)/\\tau)$는 긍정적 쌍 간의 유사도를 지수적으로 증폭시킨 값입니다. 분모 $\\sum_{k=1}^{2N} \\mathbb{I}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)$는 $z_i$를 기준으로, 자기 자신을 제외한 배치 내의 모든 $2N-1$개의 다른 벡터(긍정적 쌍인 $z_j$ 1개와 부정적 쌍 $2N-2$개 포함)와의 유사도를 지수적으로 증폭시켜 합한 값입니다. 결론적으로 이 수식은 Softmax 함수의 형태를 띠며, 2N개의 벡터 중에서 $z_i$의 짝인 $z_j$를 올바르게 분류하는 다중 클래스 분류 문제로 볼 수 있습니다. 로그를 취하고 음수를 붙여, 정답(긍정적 쌍)의 확률을 최대화(손실을 최소화)하도록 합니다. 이것이 바로 Cross-Entropy 손실의 기본 원리입니다.\n3. 실험 (Experiments) SimCLR의 각 설계 요소가 성능에 어떤 영향을 미치는지 검증하기 위해 방대한 실험을 수행했습니다.\n데이터 증강의 효과 Figure 3은 **무작위 자르기(random cropping)**만으로도 이미지의 전체적인 모습을 보는 ‘global view’와 특정 부분을 보는 ’local view’ (B → A), 또는 인접한 부분을 보는 ‘adjacent view’ (D → C)를 예측하는 다양한 대조 학습 과제를 만들 수 있음을 보여줍니다.\nFigure 4는 실험에 사용된 다양한 데이터 증강 기법들을 시각적으로 보여줍니다. (a) 원본 이미지부터 시작해 자르기, 색상 왜곡, 회전, 노이즈, 블러 등 다양한 변환을 적용한 모습을 확인할 수 있습니다.\nFigure 5는 이 증강 기법들의 조합 효과를 분석한 히트맵입니다. 대각선은 단일 변환의 성능, 비대각선은 두 변환 조합의 성능을 나타냅니다. 이 표에서 어떤 단일 변환도 좋은 성능을 내지 못하지만, 두 가지 변환을 조합했을 때(특히 ‘Crop’과 ‘Color’의 조합) 성능이 극적으로 향상됨을 알 수 있습니다.\nFigure 6은 왜 색상 왜곡이 중요한지 설명합니다. 색상 왜곡이 없으면(a), 같은 이미지에서 잘라낸 조각들은 비슷한 색상 분포(히스토그램)를 가집니다. 모델이 이 ‘쉬운 길’을 택해 색상만으로 정답을 맞히는 꼼수를 부릴 수 있습니다. 색상 왜곡을 추가하면(b), 이런 꼼수가 불가능해져 모델이 더 일반화 가능한 특징을 배우게 됩니다.\nTable 1은 색상 증강의 강도에 따른 성능 변화를 보여줍니다. 자기 지도 학습(SimCLR)에서는 색상 증강을 강하게 할수록 성능이 향상되지만, 지도 학습에서는 오히려 성능이 저하되기도 합니다. 이는 자기 지도 학습이 지도 학습보다 더 강한 데이터 증강을 필요로 함을 의미합니다.\n아키텍처의 영향 Figure 7은 모델의 깊이와 너비가 커질수록 성능이 향상됨을 보여줍니다. 흥미로운 점은 모델이 커질수록, 자기 지도 학습 모델(파란 점, 빨간 별)과 지도 학습 모델(녹색 십자가) 간의 성능 격차가 줄어든다는 것입니다. 이는 자기 지도 학습이 큰 모델로부터 더 많은 혜택을 본다는 것을 시사합니다.\nFigure 8은 **투영 헤드 $g(\\cdot)$**의 구조에 따른 성능을 비교합니다. 비선형 MLP 헤드(‘Non-linear’)가 선형 헤드(‘Linear’)나 헤드가 없는 경우(‘None’)보다 월등히 좋은 성능을 보입니다.\n$h$와 $g(h)$가 어떤 증강(색상, 회전 등)이 적용되었는지 예측하는 실험을 했을 때, $h$가 훨씬 더 많은 정보를 담고 있는 것으로 나타났습니다. 즉, $g(h)$는 대조 학습 과제에 불필요한 정보(예: 색상, 방향)를 제거하는 역할을 하며, 이로 인해 그 이전 단계인 $h$에는 더 풍부한 정보가 보존되는 것입니다.\n손실 함수와 배치 크기 Table 2는 NT-Xent 손실 함수를 다른 대조 손실 함수들(NT-Logistic, Margin Triplet)과 수식 및 기울기 측면에서 비교합니다. NT-Xent는 온도 $\\tau$와 $l_2$ 정규화를 통해 어려운 부정적 예시에 가중치를 두는 반면, 다른 손실 함수들은 그렇지 않다는 차이가 있습니다.\nTable 4는 실제 성능 비교 결과로, NT-Xent가 다른 손실 함수들보다 훨씬 우수한 성능을 보임을 확인시켜 줍니다.\nTable 5는 $l_2$ 정규화와 온도 $\\tau$의 중요성을 보여줍니다. 정규화를 사용하지 않거나, 적절한 온도를 설정하지 않으면 성능이 크게 저하됩니다.\nFigure 9는 배치 크기와 학습 에포크(epoch) 수에 따른 성능 변화를 보여줍니다. 학습 초기에는 배치 크기가 클수록 성능이 좋지만, 학습을 오래 진행할수록 그 차이가 줄어듭니다. 대조 학습에서는 큰 배치가 더 많은 부정적 예시를 제공하므로 수렴을 돕는 효과가 있습니다.\n최신 기술과의 비교 (SOTA) Table 6은 ImageNet 데이터셋에서의 선형 평가(linear evaluation) 결과를 다른 자기 지도 학습 모델들과 비교한 표입니다. SimCLR은 ResNet-50 (4x) 모델을 사용하여 76.5%의 Top-1 정확도를 달성, 이전 최고 성능을 크게 뛰어넘었으며 지도 학습 모델의 성능과 동등한 수준에 도달했습니다.\nTable 7은 레이블이 적은 데이터(1% 또는 10%)로 모델을 **미세 조정(fine-tuning)**하는 준지도 학습(semi-supervised learning) 성능을 비교합니다. 여기서도 SimCLR은 다른 방법들을 큰 차이로 능가했습니다.\nTable 8은 다른 12개의 다양한 이미지 데이터셋으로의 전이 학습(transfer learning) 성능을 보여줍니다. SimCLR로 사전 학습된 모델은 많은 데이터셋에서 지도 학습으로 사전 학습된 모델과 대등하거나 더 나은 성능을 보였습니다.\n4. 결론 (Conclusion) 이 논문은 SimCLR이라는 매우 간단하면서도 강력한 대조 학습 프레임워크를 제시했습니다. SimCLR의 성공은 어느 하나의 새로운 발견이 아닌, 기존에 알려진 요소들을 체계적으로 연구하고 최적으로 조합한 결과입니다.\n핵심적인 발견은 다음과 같습니다.\n데이터 증강 기법의 강력한 조합(특히 무작위 자르기와 색상 왜곡)은 효과적인 표현 학습에 필수적이다. 인코더 뒤에 비선형 투영 헤드를 추가하고, 헤드를 통과하기 전의 표현을 사용하는 것이 성능을 크게 향상시킨다. **정규화된 Cross Entropy 손실 함수(NT-Xent)**와 적절한 온도 파라미터, 그리고 매우 큰 배치 크기에서의 학습이 매우 효과적이다. SimCLR은 복잡한 구조나 메모리 뱅크 없이도 최첨단 성능을 달성함으로써, 자기 지도 학습의 잠재력이 여전히 과소평가되고 있음을 보여주었습니다. 이 연구는 이후의 수많은 자기 지도 학습 연구에 큰 영감을 주었습니다.\n","wordCount":"1435","inLanguage":"en","datePublished":"2025-10-28T00:00:00Z","dateModified":"2025-10-28T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-simclr/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">SimCLR 논문 리뷰</h1><div class=post-meta><span title='2025-10-28 00:00:00 +0000 UTC'>October 28, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1435 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>이 논문은 사람이 일일이 사진에 &ldquo;이건 고양이야&rdquo;, &ldquo;저건 강아지야"라고 알려주지 않아도, 컴퓨터가 스스로 이미지의 특징을 학습하는 방법에 대한 연구입니다. 특히 아주 간단하면서도 효과적인 방법을 제시하여 큰 주목을 받았습니다.</p><p>컴퓨터에게 사진을 주고 이게 뭔지 맞혀보라고 시키는 걸 **지도 학습(Supervised Learning)**이라 합니다. 정답을 알려주면서 공부시키는 거죠. 하지만 세상에는 정답이 없는 사진이 훨씬 많습니다. 이 정답 없는 사진들로 컴퓨터를 똑똑하게 만들 순 없을까요? 이게 바로 **&lsquo;자기 지도 학습(Self-Supervised Learning)&rsquo;**의 목표입니다.</p><p>SimCLR이 사용하는 방법은 **&lsquo;대조 학습(Contrastive Learning)&rsquo;**이라는 건데요, 아주 간단한 아이디어에서 출발합니다.</p><p><strong>예시:</strong> 강아지 사진이 한 장 있다고 상상해 봅시다.</p><ol><li>이 사진을 가지고 약간 다른 버전의 사진 두 장을 만듭니다. 예를 들어, 한 장은 강아지 얼굴만 확대(crop)하고, 다른 한 장은 색감을 살짝 바꾸는(color distortion) 거죠.</li><li>컴퓨터에게 이 두 사진은 &lsquo;같은 강아지&rsquo;에서 나온 <strong>긍정적 쌍(positive pair)</strong> 이라고 알려줍니다.</li><li>그리고 동시에, 전혀 다른 사진들(예: 고양이, 자동차, 나무 사진)을 보여주면서, 이것들은 방금 본 강아지 사진과 다른 <strong>부정적 쌍(negative pair)</strong> 이라고 알려줍니다.</li></ol><p>이 과정을 수많은 사진으로 반복하면, 컴퓨터는 점차 &lsquo;어떤 특징&rsquo;이 같은 대상을 나타내고, &lsquo;어떤 특징&rsquo;이 다른 대상을 나타내는지 스스로 배우게 됩니다. 즉, &lsquo;강아지다움&rsquo;이 무엇인지 그 본질적인 **표현(representation)**을 학습하게 되는 거죠. SimCLR은 이 간단한 원리를 매우 효과적으로 구현한 프레임워크입니다.</p><ul><li>참고: <a href=/posts/paper-review-stochastic/>Stochastic Optimization Review</a> (표현 학습 관련 발표 자료)</li></ul><hr><h3 id=1-서론-introduction>1. 서론 (Introduction)<a hidden class=anchor aria-hidden=true href=#1-서론-introduction>#</a></h3><p>연구의 시작은 &ldquo;사람의 정답 없이 어떻게 컴퓨터가 **시각적 표현(visual representation)**을 배울 수 있을까?&ldquo;라는 오랜 질문에서 출발합니다. 기존의 방법들은 복잡한 구조를 필요로 하거나, 대량의 이전 데이터를 저장해두는 <strong>메모리 뱅크(memory bank)</strong> 같은 부가적인 장치가 필요했습니다.</p><p><img alt="Figure 1: SimCLR 이전 자기 지도 학습 모델들의 성능 그래프" loading=lazy src=/images/posts/simclr/figure1.png></p><p><strong>Figure 1</strong>은 SimCLR이 나오기 전까지의 여러 자기 지도 학습 모델들의 성능을 보여주는 그래프입니다. 가로축은 모델의 크기(Number of Parameters), 세로축은 이미지 분류 정확도(ImageNet Top-1 Accuracy)를 나타냅니다. 이 그래프에서 **SimCLR(별 모양 ★)**은 다른 모델들(점 모양 •)에 비해 훨씬 적은 파라미터로 더 높은 정확도를 달성하며, 심지어 사람이 정답을 알려주고 학습시킨 **지도 학습 모델(Supervised, 회색 십자가)**의 성능과 맞먹는 것을 보여줍니다. 이는 매우 간단한 프레임워크가 기존의 복잡한 방법들을 뛰어넘을 수 있음을 시사합니다.</p><p>이 논문은 SimCLR의 성공 비결이 다음 세 가지 핵심 요소의 조합에 있음을 체계적인 실험을 통해 보여줍니다.</p><ul><li><strong>데이터 증강 기법의 조합</strong>이 효과적인 표현을 학습하는 데 매우 중요하다.</li><li>모델의 최종 표현(representation)과 손실 함수(loss function) 사이에 **비선형 변환(nonlinear transformation)**을 추가하는 것이 성능을 크게 향상시킨다.</li><li>대조 학습은 지도 학습보다 더 큰 **배치 크기(batch size)**와 더 긴 학습 시간으로부터 더 큰 이득을 얻는다.</li></ul><hr><h3 id=2-기술-설명-technical-description>2. 기술 설명 (Technical Description)<a hidden class=anchor aria-hidden=true href=#2-기술-설명-technical-description>#</a></h3><p>SimCLR 프레임워크는 네 가지 주요 구성 요소로 이루어져 있습니다.</p><p><img alt="Figure 2: SimCLR 프레임워크 전체 과정" loading=lazy src=/images/posts/simclr/figure2.png></p><p><strong>Figure 2</strong>는 이 전체 과정을 아주 잘 보여주는 그림입니다.
하나의 이미지 $x$에서 시작해서, 두 개의 서로 다른 <strong>데이터 증강(augmentation)</strong> $t$와 $t&rsquo;$를 적용하여 두 개의 새로운 이미지 $\tilde{x}_i$와 $\tilde{x}_j$를 만듭니다. 이 둘은 서로 연관된 **&lsquo;긍정적 쌍&rsquo;**이 됩니다. 이 두 이미지는 동일한 <strong>인코더 네트워크(encoder network)</strong> $f(\cdot)$를 통과하여 각각의 <strong>표현 벡터(representation vector)</strong> $h_i$와 $h_j$로 변환됩니다. 그 후, 이 표현 벡터들은 또 다른 작은 신경망인 <strong>투영 헤드(projection head)</strong> $g(\cdot)$를 거쳐 최종적으로 $z_i$와 $z_j$라는 벡터로 변환됩니다. 학습의 목표는 이 $z_i$와 $z_j$ 사이의 **유사도(agreement)**를 최대화하는 것입니다. 학습이 끝나면 투영 헤드 $g(\cdot)$는 버리고, 표현 벡터 $h$를 다른 작업(예: 이미지 분류)에 사용합니다.</p><ol><li><p><strong>데이터 증강 (Data Augmentation):</strong> 하나의 이미지로부터 두 개의 연관된 뷰(view)를 생성하는 과정입니다. 이 논문에서는 <strong>무작위 자르기 후 원래 크기로 복원(random cropping and resize)</strong>, <strong>색상 왜곡(color distortion)</strong>, **가우시안 블러(Gaussian blur)**를 순차적으로 적용합니다. 어떤 증강 기법을 어떻게 조합하는지가 모델 성능의 핵심입니다.</p></li><li><p><strong>기본 인코더 (Base Encoder):</strong> 증강된 이미지로부터 표현 벡터 $h_i$를 추출하는 신경망입니다. 이 논문에서는 주로 <strong>ResNet</strong> 구조를 사용하며, 특별한 구조적 제약이 없어 어떤 신경망이든 사용할 수 있습니다. $h_i = f(\tilde{x}_i)$ 입니다.</p></li><li><p><strong>투영 헤드 (Projection Head):</strong> 인코더에서 나온 표현 벡터 $h$를 **대조 손실(contrastive loss)**을 계산하는 공간으로 매핑하는 작은 신경망 $g(\cdot)$입니다. 이 논문에서는 하나의 **은닉층(hidden layer)**을 가진 **MLP(Multi-Layer Perceptron)**를 사용합니다. $z_i = g(h_i)$ 이며, $h_i$가 아닌 $z_i$에 손실 함수를 적용하는 것이 성능 향상에 큰 도움이 된다는 것을 발견했습니다.</p></li><li><p><strong>대조 손실 함수 (Contrastive Loss Function):</strong> 주어진 &lsquo;긍정적 쌍&rsquo;($z_i$, $z_j$)의 유사도는 높이고, 나머지 모든 &lsquo;부정적 쌍&rsquo;과의 유사도는 낮추도록 학습시키는 함수입니다. 이 논문에서는 <strong>NT-Xent (Normalized Temperature-scaled Cross Entropy)</strong> 손실을 사용합니다.</p></li></ol><p><img alt="Algorithm 1: SimCLR의 주요 학습 알고리즘" loading=lazy src=/images/posts/simclr/algorithm1.png>
<strong>Algorithm 1</strong>은 이 SimCLR 학습 과정을 단계별로 요약한 것입니다.
N개의 이미지를 배치로 가져와 각 이미지마다 두 개의 증강된 뷰를 만들고, 인코더와 투영 헤드를 통과시켜 2N개의 $z$ 벡터를 얻습니다. 그런 다음 모든 벡터 쌍 간의 유사도($s_{i,j}$)를 계산하고, 이를 바탕으로 손실($l(i,j)$)을 계산하여 네트워크 $f$와 $g$를 업데이트합니다.</p><p>수식에 대해 좀 더 깊이 들어가 보겠습니다. NT-Xent 손실 함수는 다음과 같이 정의됩니다.
$$
l_{ij} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{I}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}
$$</p><ul><li>$z_i$와 $z_j$는 각각 증강된 이미지 $\tilde{x}_i$와 $\tilde{x}_j$로부터 추출된 $l_2$ <strong>정규화된(normalized)</strong> 표현 벡터입니다. 이 둘은 &lsquo;긍정적 쌍&rsquo;입니다.</li><li>$\text{sim}(u, v) = u^T v / (||u|| ||v||)$는 두 벡터 간의 **코사인 유사도(cosine similarity)**를 계산합니다.</li><li>$\tau$ (타우)는 <strong>온도(temperature)</strong> 하이퍼파라미터로, 유사도 분포를 얼마나 뾰족하게 만들지 조절합니다. $\tau$가 작을수록 모델은 <strong>어려운 부정적 예시(hard negatives</strong>, 즉 긍정적 예시와 유사도가 높은 부정적 예시)에 더 집중하게 됩니다. 이는 손실 함수의 기울기(gradient)에서 부정적 예시들에 대한 가중치를 조절하는 효과를 낳습니다.</li><li>분자 $\exp(\text{sim}(z_i, z_j)/\tau)$는 <strong>긍정적 쌍</strong> 간의 유사도를 지수적으로 증폭시킨 값입니다.</li><li>분모 $\sum_{k=1}^{2N} \mathbb{I}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)$는 $z_i$를 기준으로, 자기 자신을 제외한 배치 내의 모든 $2N-1$개의 다른 벡터(긍정적 쌍인 $z_j$ 1개와 부정적 쌍 $2N-2$개 포함)와의 유사도를 지수적으로 증폭시켜 합한 값입니다.</li></ul><p>결론적으로 이 수식은 <strong>Softmax</strong> 함수의 형태를 띠며, 2N개의 벡터 중에서 $z_i$의 짝인 $z_j$를 올바르게 분류하는 다중 클래스 분류 문제로 볼 수 있습니다. 로그를 취하고 음수를 붙여, 정답(긍정적 쌍)의 확률을 최대화(손실을 최소화)하도록 합니다. 이것이 바로 <strong>Cross-Entropy 손실</strong>의 기본 원리입니다.</p><hr><h3 id=3-실험-experiments>3. 실험 (Experiments)<a hidden class=anchor aria-hidden=true href=#3-실험-experiments>#</a></h3><p>SimCLR의 각 설계 요소가 성능에 어떤 영향을 미치는지 검증하기 위해 방대한 실험을 수행했습니다.</p><h4 id=데이터-증강의-효과>데이터 증강의 효과<a hidden class=anchor aria-hidden=true href=#데이터-증강의-효과>#</a></h4><p><img alt="Figure 3: 무작위 자르기를 통한 대조 학습 과제 생성 예시" loading=lazy src=/images/posts/simclr/figure3.png></p><p><strong>Figure 3</strong>은 **무작위 자르기(random cropping)**만으로도 이미지의 전체적인 모습을 보는 &lsquo;global view&rsquo;와 특정 부분을 보는 &rsquo;local view&rsquo; (B → A), 또는 인접한 부분을 보는 &lsquo;adjacent view&rsquo; (D → C)를 예측하는 다양한 대조 학습 과제를 만들 수 있음을 보여줍니다.</p><p><img alt="Figure 4: 사용된 다양한 데이터 증강 기법 시각화" loading=lazy src=/images/posts/simclr/figure4.png></p><p><strong>Figure 4</strong>는 실험에 사용된 다양한 데이터 증강 기법들을 시각적으로 보여줍니다. (a) 원본 이미지부터 시작해 자르기, 색상 왜곡, 회전, 노이즈, 블러 등 다양한 변환을 적용한 모습을 확인할 수 있습니다.</p><p><img alt="Figure 5: 데이터 증강 기법 조합 효과 분석 히트맵" loading=lazy src=/images/posts/simclr/figure5.png></p><p><strong>Figure 5</strong>는 이 증강 기법들의 조합 효과를 분석한 히트맵입니다. 대각선은 단일 변환의 성능, 비대각선은 두 변환 조합의 성능을 나타냅니다. 이 표에서 어떤 단일 변환도 좋은 성능을 내지 못하지만, 두 가지 변환을 조합했을 때(특히 <strong>&lsquo;Crop&rsquo;과 &lsquo;Color&rsquo;의 조합</strong>) 성능이 극적으로 향상됨을 알 수 있습니다.</p><p><img alt="Figure 6: 색상 왜곡의 중요성 설명" loading=lazy src=/images/posts/simclr/figure6.png></p><p><strong>Figure 6</strong>은 왜 <strong>색상 왜곡</strong>이 중요한지 설명합니다. 색상 왜곡이 없으면(a), 같은 이미지에서 잘라낸 조각들은 비슷한 색상 분포(히스토그램)를 가집니다. 모델이 이 &lsquo;쉬운 길&rsquo;을 택해 색상만으로 정답을 맞히는 꼼수를 부릴 수 있습니다. 색상 왜곡을 추가하면(b), 이런 꼼수가 불가능해져 모델이 더 일반화 가능한 특징을 배우게 됩니다.</p><p><img alt="Table 1: 색상 증강 강도에 따른 성능 변화" loading=lazy src=/images/posts/simclr/table1.png>
<strong>Table 1</strong>은 색상 증강의 강도에 따른 성능 변화를 보여줍니다. 자기 지도 학습(SimCLR)에서는 색상 증강을 강하게 할수록 성능이 향상되지만, 지도 학습에서는 오히려 성능이 저하되기도 합니다. 이는 자기 지도 학습이 지도 학습보다 더 강한 데이터 증강을 필요로 함을 의미합니다.</p><h4 id=아키텍처의-영향>아키텍처의 영향<a hidden class=anchor aria-hidden=true href=#아키텍처의-영향>#</a></h4><p><img alt="Figure 7: 모델 크기에 따른 성능 향상 그래프" loading=lazy src=/images/posts/simclr/figure7.png></p><p><strong>Figure 7</strong>은 모델의 깊이와 너비가 커질수록 성능이 향상됨을 보여줍니다. 흥미로운 점은 모델이 커질수록, 자기 지도 학습 모델(파란 점, 빨간 별)과 지도 학습 모델(녹색 십자가) 간의 성능 격차가 줄어든다는 것입니다. 이는 <strong>자기 지도 학습이 큰 모델로부터 더 많은 혜택을 본다</strong>는 것을 시사합니다.</p><p><img alt="Figure 8: 투영 헤드 구조에 따른 성능 비교" loading=lazy src=/images/posts/simclr/figure8.png></p><p><strong>Figure 8</strong>은 **투영 헤드 $g(\cdot)$**의 구조에 따른 성능을 비교합니다. 비선형 MLP 헤드(&lsquo;Non-linear&rsquo;)가 선형 헤드(&lsquo;Linear&rsquo;)나 헤드가 없는 경우(&lsquo;None&rsquo;)보다 월등히 좋은 성능을 보입니다.</p><p><img alt="Table 3: 표현 h와 g(h)에 담긴 정보 비교" loading=lazy src=/images/posts/simclr/table3.png>
$h$와 $g(h)$가 어떤 증강(색상, 회전 등)이 적용되었는지 예측하는 실험을 했을 때, $h$가 훨씬 더 많은 정보를 담고 있는 것으로 나타났습니다. 즉, $g(h)$는 대조 학습 과제에 불필요한 정보(예: 색상, 방향)를 제거하는 역할을 하며, 이로 인해 그 이전 단계인 $h$에는 더 풍부한 정보가 보존되는 것입니다.</p><h4 id=손실-함수와-배치-크기>손실 함수와 배치 크기<a hidden class=anchor aria-hidden=true href=#손실-함수와-배치-크기>#</a></h4><p><img alt="Table 2: NT-Xent와 다른 대조 손실 함수 비교" loading=lazy src=/images/posts/simclr/table2.png>
<strong>Table 2</strong>는 <strong>NT-Xent 손실 함수</strong>를 다른 대조 손실 함수들(NT-Logistic, Margin Triplet)과 수식 및 기울기 측면에서 비교합니다. NT-Xent는 온도 $\tau$와 $l_2$ 정규화를 통해 어려운 부정적 예시에 가중치를 두는 반면, 다른 손실 함수들은 그렇지 않다는 차이가 있습니다.</p><p><img alt="Table 4: 손실 함수별 성능 비교" loading=lazy src=/images/posts/simclr/table4.png>
<strong>Table 4</strong>는 실제 성능 비교 결과로, NT-Xent가 다른 손실 함수들보다 훨씬 우수한 성능을 보임을 확인시켜 줍니다.</p><p><img alt="Table 5: 정규화와 온도 설정의 영향" loading=lazy src=/images/posts/simclr/table5.png>
<strong>Table 5</strong>는 $l_2$ 정규화와 온도 $\tau$의 중요성을 보여줍니다. 정규화를 사용하지 않거나, 적절한 온도를 설정하지 않으면 성능이 크게 저하됩니다.</p><p><img alt="Figure 9: 배치 크기와 학습 에포크에 따른 성능 변화" loading=lazy src=/images/posts/simclr/figure9.png></p><p><strong>Figure 9</strong>는 배치 크기와 학습 에포크(epoch) 수에 따른 성능 변화를 보여줍니다. 학습 초기에는 배치 크기가 클수록 성능이 좋지만, 학습을 오래 진행할수록 그 차이가 줄어듭니다. 대조 학습에서는 <strong>큰 배치가 더 많은 부정적 예시를 제공</strong>하므로 수렴을 돕는 효과가 있습니다.</p><h4 id=최신-기술과의-비교-sota>최신 기술과의 비교 (SOTA)<a hidden class=anchor aria-hidden=true href=#최신-기술과의-비교-sota>#</a></h4><p><img alt="Table 6: ImageNet 선형 평가 비교" loading=lazy src=/images/posts/simclr/table6.png>
<strong>Table 6</strong>은 ImageNet 데이터셋에서의 <strong>선형 평가(linear evaluation)</strong> 결과를 다른 자기 지도 학습 모델들과 비교한 표입니다. SimCLR은 ResNet-50 (4x) 모델을 사용하여 <strong>76.5%의 Top-1 정확도</strong>를 달성, 이전 최고 성능을 크게 뛰어넘었으며 지도 학습 모델의 성능과 동등한 수준에 도달했습니다.</p><p><img alt="Table 7: 준지도 학습 성능 비교" loading=lazy src=/images/posts/simclr/table7.png>
<strong>Table 7</strong>은 레이블이 적은 데이터(1% 또는 10%)로 모델을 **미세 조정(fine-tuning)**하는 <strong>준지도 학습(semi-supervised learning)</strong> 성능을 비교합니다. 여기서도 SimCLR은 다른 방법들을 큰 차이로 능가했습니다.</p><p><img alt="Table 8: 다양한 데이터셋 전이 학습 성능" loading=lazy src=/images/posts/simclr/table8.png>
<strong>Table 8</strong>은 다른 12개의 다양한 이미지 데이터셋으로의 <strong>전이 학습(transfer learning)</strong> 성능을 보여줍니다. SimCLR로 사전 학습된 모델은 많은 데이터셋에서 지도 학습으로 사전 학습된 모델과 대등하거나 더 나은 성능을 보였습니다.</p><hr><h3 id=4-결론-conclusion>4. 결론 (Conclusion)<a hidden class=anchor aria-hidden=true href=#4-결론-conclusion>#</a></h3><p>이 논문은 <strong>SimCLR</strong>이라는 매우 간단하면서도 강력한 대조 학습 프레임워크를 제시했습니다. SimCLR의 성공은 어느 하나의 새로운 발견이 아닌, 기존에 알려진 요소들을 체계적으로 연구하고 최적으로 조합한 결과입니다.</p><p>핵심적인 발견은 다음과 같습니다.</p><ul><li><strong>데이터 증강 기법의 강력한 조합</strong>(특히 무작위 자르기와 색상 왜곡)은 효과적인 표현 학습에 필수적이다.</li><li>인코더 뒤에 <strong>비선형 투영 헤드</strong>를 추가하고, 헤드를 통과하기 전의 표현을 사용하는 것이 성능을 크게 향상시킨다.</li><li>**정규화된 Cross Entropy 손실 함수(NT-Xent)**와 적절한 온도 파라미터, 그리고 <strong>매우 큰 배치 크기</strong>에서의 학습이 매우 효과적이다.</li></ul><p>SimCLR은 복잡한 구조나 메모리 뱅크 없이도 최첨단 성능을 달성함으로써, 자기 지도 학습의 잠재력이 여전히 과소평가되고 있음을 보여주었습니다. 이 연구는 이후의 수많은 자기 지도 학습 연구에 큰 영감을 주었습니다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://mookjsi.github.io/tags/self-supervised-learning/>Self-Supervised Learning</a></li><li><a href=https://mookjsi.github.io/tags/contrastive-learning/>Contrastive Learning</a></li><li><a href=https://mookjsi.github.io/tags/simclr/>SimCLR</a></li><li><a href=https://mookjsi.github.io/tags/icml-2020/>ICML 2020</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/paper-review-gan/><span class=title>« Prev</span><br><span>Generative Adversarial Nets (GAN) 논문 리뷰</span>
</a><a class=next href=https://mookjsi.github.io/posts/paper-review-sam/><span class=title>Next »</span><br><span>Segment Anything 논문 리뷰</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on x" href="https://x.com/intent/tweet/?text=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f&amp;hashtags=PaperReview%2cDeepLearning%2cComputerVision%2cSelf-SupervisedLearning%2cContrastiveLearning%2cSimCLR%2cICML2020"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f&amp;title=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;summary=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f&title=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on whatsapp" href="https://api.whatsapp.com/send?text=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on telegram" href="https://telegram.me/share/url?text=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SimCLR 논문 리뷰 on ycombinator" href="https://news.ycombinator.com/submitlink?t=SimCLR%20%eb%85%bc%eb%ac%b8%20%eb%a6%ac%eb%b7%b0&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-simclr%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>