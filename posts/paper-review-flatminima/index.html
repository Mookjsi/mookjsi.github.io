<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Simplifying Neural Nets by Discovering Flat Minima | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Optimization,Generalization,NIPS 1994"><meta name=description content="A detailed review of the classic 1994 paper by Hochreiter & Schmidhuber, &lsquo;Simplifying Neural Nets by Discovering Flat Minima&rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-flatminima/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-flatminima/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-flatminima/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Simplifying Neural Nets by Discovering Flat Minima"><meta property="og:description" content="A detailed review of the classic 1994 paper by Hochreiter & Schmidhuber, ‘Simplifying Neural Nets by Discovering Flat Minima’. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-15T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-15T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Generalization"><meta property="article:tag" content="NIPS 1994"><meta name=twitter:card content="summary"><meta name=twitter:title content="Simplifying Neural Nets by Discovering Flat Minima"><meta name=twitter:description content="A detailed review of the classic 1994 paper by Hochreiter & Schmidhuber, &lsquo;Simplifying Neural Nets by Discovering Flat Minima&rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Simplifying Neural Nets by Discovering Flat Minima","item":"https://mookjsi.github.io/posts/paper-review-flatminima/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Simplifying Neural Nets by Discovering Flat Minima","name":"Simplifying Neural Nets by Discovering Flat Minima","description":"A detailed review of the classic 1994 paper by Hochreiter \u0026amp; Schmidhuber, \u0026lsquo;Simplifying Neural Nets by Discovering Flat Minima\u0026rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization.","keywords":["Paper Review","Deep Learning","Optimization","Generalization","NIPS 1994"],"articleBody":"This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the geometry of the loss landscape and a model’s ability to generalize.\nThe core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, flat regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.\nThis is the title slide for my presentation on “Simplifying neural nets by discovering flat minima,” which I prepared for our lab meeting.\nFirst, let’s credit the authors: Sepp Hochreiter and Jürgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?\nHere are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).\nThis quote from the authors perfectly captures the paper’s central thesis: “Find wide, not sharp, minima - and your network generalizes for free.” It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.\nMy presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.\nSo, let’s start with the motivation. Why was this research necessary?\nTo understand the paper’s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network’s weights (so-called “priors”). This paper marked a shift, arguing that we should focus on the geometry of the error surface rather than imposing assumptions about the weights themselves.\nThis 3D plot illustrates the core concept perfectly. On the right, we see a “sharp” minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a “flat” minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.\nHere’s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It’s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It’s much easier to find a good spot, and moving around a little doesn’t drastically change your altitude (or your model’s error). This robustness is linked to lower model complexity.\nSo, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a “good” prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.\nThe “Flat Minima” approach offers solutions to these problems. First, it doesn’t require any pre-chosen priors; the geometry of the solution space itself defines what a “simple” model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.\nNow, let’s formalize the problem by defining the tasks and architectures this method applies to.\nThe basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.\nThe model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).\nBuilding on that, we can define the set of “acceptable” solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors w for which the training MSE is less than or equal to this tolerance.\nNow we get to the core of the algorithm’s construction. For a given weight vector w, we define a “box” around it. For each individual weight $w_{ij}$, we find the maximum amount $\\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\\Delta w_{ij}$ for each weight.\nThese intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the “Flat Minima” as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.\nSo, how do we actually find these large-volume minima? That brings us to the algorithm itself.\nThe main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\\Delta w$. A larger volume signifies a flatter, more desirable minimum.\nThis slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.\nMaximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.\nThis new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.\nTo build the algorithm, we first need to mathematically define “flatness”. We start by defining the change in the network’s output, $ED(w, \\delta w)$, that results from a small change in weights, $\\delta w$.\nTo make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \\delta w)$, in terms of the original output and the first derivatives (gradients).\nSubstituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.\nThis leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don’t lead to large output changes.\nThe second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as “spherical” as possible by ensuring that perturbations to each weight contribute equally to the total output change.\nHere is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.\nBy rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).\nThis slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.\nBy solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\\Delta w_{ij}|$. This formula depends on the network’s output gradients.\nLet’s quickly recap the algorithm’s goal. We aim to maximize the box volume $\\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.\nNow that we have a formula for the size of the box edges, $\\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network’s derivatives. This slide shows that connection, approximating the log of $\\Delta w_{ij}$ with the log of the derived formula.\nThis slide restates the formula for the size of the perturbation $|\\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.\nPlugging the expression for $\\Delta w_{ij}$ into our cost function $B(w, D_0) = -\\sum \\log \\Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the “flatness” of the minimum.\nThe complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\\lambda$ balances the two.\nTo minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network’s output, which would typically be very expensive to compute.\nHowever, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$—the same as standard backpropagation—using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.\nTo recap the key advantages: the “Flat Minima” method is appealing because it uses geometry instead of priors to define simplicity, it’s computationally efficient, and it naturally performs network pruning for better generalization.\nThe paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.\nIn the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.\nThe results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.\nThe second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.\nThe final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.\nThe results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.\n","wordCount":"1878","inLanguage":"en","datePublished":"2025-05-15T00:00:00Z","dateModified":"2025-05-15T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-flatminima/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Simplifying Neural Nets by Discovering Flat Minima</h1><div class=post-meta><span title='2025-05-15 00:00:00 +0000 UTC'>May 15, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1878 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the <em>geometry</em> of the loss landscape and a model&rsquo;s ability to generalize.</p><p>The core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, <em>flat</em> regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.</p><hr><p><img alt="Slide 1" loading=lazy src=/images/posts/flatminima/slide-01.jpg></p><blockquote><p>This is the title slide for my presentation on &ldquo;Simplifying neural nets by discovering flat minima,&rdquo; which I prepared for our lab meeting.</p></blockquote><p><img alt="Slide 2" loading=lazy src=/images/posts/flatminima/slide-02.jpg></p><blockquote><p>First, let&rsquo;s credit the authors: Sepp Hochreiter and Jürgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?</p></blockquote><p><img alt="Slide 3" loading=lazy src=/images/posts/flatminima/slide-03.jpg></p><blockquote><p>Here are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).</p></blockquote><p><img alt="Slide 4" loading=lazy src=/images/posts/flatminima/slide-04.jpg></p><blockquote><p>This quote from the authors perfectly captures the paper&rsquo;s central thesis: &ldquo;Find wide, not sharp, minima - and your network generalizes for free.&rdquo; It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.</p></blockquote><p><img alt="Slide 5" loading=lazy src=/images/posts/flatminima/slide-05.jpg></p><blockquote><p>My presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.</p></blockquote><p><img alt="Slide 6" loading=lazy src=/images/posts/flatminima/slide-06.jpg></p><blockquote><p>So, let&rsquo;s start with the motivation. Why was this research necessary?</p></blockquote><p><img alt="Slide 7" loading=lazy src=/images/posts/flatminima/slide-07.jpg></p><blockquote><p>To understand the paper&rsquo;s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network&rsquo;s weights (so-called &ldquo;priors&rdquo;). This paper marked a shift, arguing that we should focus on the <em>geometry</em> of the error surface rather than imposing assumptions about the weights themselves.</p></blockquote><p><img alt="Slide 8" loading=lazy src=/images/posts/flatminima/slide-08.jpg></p><blockquote><p>This 3D plot illustrates the core concept perfectly. On the right, we see a &ldquo;sharp&rdquo; minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a &ldquo;flat&rdquo; minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.</p></blockquote><p><img alt="Slide 9" loading=lazy src=/images/posts/flatminima/slide-09.jpg></p><blockquote><p>Here&rsquo;s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It&rsquo;s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It&rsquo;s much easier to find a good spot, and moving around a little doesn&rsquo;t drastically change your altitude (or your model&rsquo;s error). This robustness is linked to lower model complexity.</p></blockquote><p><img alt="Slide 10" loading=lazy src=/images/posts/flatminima/slide-10.jpg></p><blockquote><p>So, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a &ldquo;good&rdquo; prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.</p></blockquote><p><img alt="Slide 11" loading=lazy src=/images/posts/flatminima/slide-11.jpg></p><blockquote><p>The &ldquo;Flat Minima&rdquo; approach offers solutions to these problems. First, it doesn&rsquo;t require any pre-chosen priors; the geometry of the solution space itself defines what a &ldquo;simple&rdquo; model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.</p></blockquote><p><img alt="Slide 12" loading=lazy src=/images/posts/flatminima/slide-12.jpg></p><blockquote><p>Now, let&rsquo;s formalize the problem by defining the tasks and architectures this method applies to.</p></blockquote><p><img alt="Slide 13" loading=lazy src=/images/posts/flatminima/slide-13.jpg></p><blockquote><p>The basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.</p></blockquote><p><img alt="Slide 14" loading=lazy src=/images/posts/flatminima/slide-14.jpg></p><blockquote><p>The model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).</p></blockquote><p><img alt="Slide 15" loading=lazy src=/images/posts/flatminima/slide-15.jpg></p><blockquote><p>Building on that, we can define the set of &ldquo;acceptable&rdquo; solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors <code>w</code> for which the training MSE is less than or equal to this tolerance.</p></blockquote><p><img alt="Slide 16" loading=lazy src=/images/posts/flatminima/slide-16.jpg></p><blockquote><p>Now we get to the core of the algorithm&rsquo;s construction. For a given weight vector <code>w</code>, we define a &ldquo;box&rdquo; around it. For each individual weight $w_{ij}$, we find the maximum amount $\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\Delta w_{ij}$ for each weight.</p></blockquote><p><img alt="Slide 17" loading=lazy src=/images/posts/flatminima/slide-17.jpg></p><blockquote><p>These intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the &ldquo;Flat Minima&rdquo; as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.</p></blockquote><p><img alt="Slide 18" loading=lazy src=/images/posts/flatminima/slide-18.jpg></p><blockquote><p>So, how do we actually find these large-volume minima? That brings us to the algorithm itself.</p></blockquote><p><img alt="Slide 19" loading=lazy src=/images/posts/flatminima/slide-19.jpg></p><blockquote><p>The main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\Delta w$. A larger volume signifies a flatter, more desirable minimum.</p></blockquote><p><img alt="Slide 20" loading=lazy src=/images/posts/flatminima/slide-20.jpg></p><blockquote><p>This slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.</p></blockquote><p><img alt="Slide 21" loading=lazy src=/images/posts/flatminima/slide-21.jpg></p><blockquote><p>Maximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.</p></blockquote><p><img alt="Slide 22" loading=lazy src=/images/posts/flatminima/slide-22.jpg></p><blockquote><p>This new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.</p></blockquote><p><img alt="Slide 23" loading=lazy src=/images/posts/flatminima/slide-23.jpg></p><blockquote><p>To build the algorithm, we first need to mathematically define &ldquo;flatness&rdquo;. We start by defining the change in the network&rsquo;s output, $ED(w, \delta w)$, that results from a small change in weights, $\delta w$.</p></blockquote><p><img alt="Slide 24" loading=lazy src=/images/posts/flatminima/slide-24.jpg></p><blockquote><p>To make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \delta w)$, in terms of the original output and the first derivatives (gradients).</p></blockquote><p><img alt="Slide 25" loading=lazy src=/images/posts/flatminima/slide-25.jpg></p><blockquote><p>Substituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.</p></blockquote><p><img alt="Slide 26" loading=lazy src=/images/posts/flatminima/slide-26.jpg></p><blockquote><p>This leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don&rsquo;t lead to large output changes.</p></blockquote><p><img alt="Slide 27" loading=lazy src=/images/posts/flatminima/slide-27.jpg></p><blockquote><p>The second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as &ldquo;spherical&rdquo; as possible by ensuring that perturbations to each weight contribute equally to the total output change.</p></blockquote><p><img alt="Slide 28" loading=lazy src=/images/posts/flatminima/slide-28.jpg></p><blockquote><p>Here is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.</p></blockquote><p><img alt="Slide 29" loading=lazy src=/images/posts/flatminima/slide-29.jpg></p><blockquote><p>By rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).</p></blockquote><p><img alt="Slide 30" loading=lazy src=/images/posts/flatminima/slide-30.jpg></p><blockquote><p>This slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.</p></blockquote><p><img alt="Slide 31" loading=lazy src=/images/posts/flatminima/slide-31.jpg></p><blockquote><p>By solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\Delta w_{ij}|$. This formula depends on the network&rsquo;s output gradients.</p></blockquote><p><img alt="Slide 32" loading=lazy src=/images/posts/flatminima/slide-32.jpg></p><blockquote><p>Let&rsquo;s quickly recap the algorithm&rsquo;s goal. We aim to maximize the box volume $\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.</p></blockquote><p><img alt="Slide 33" loading=lazy src=/images/posts/flatminima/slide-33.jpg></p><blockquote><p>Now that we have a formula for the size of the box edges, $\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network&rsquo;s derivatives. This slide shows that connection, approximating the log of $\Delta w_{ij}$ with the log of the derived formula.</p></blockquote><p><img alt="Slide 34" loading=lazy src=/images/posts/flatminima/slide-34.jpg></p><blockquote><p>This slide restates the formula for the size of the perturbation $|\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.</p></blockquote><p><img alt="Slide 35" loading=lazy src=/images/posts/flatminima/slide-35.jpg></p><blockquote><p>Plugging the expression for $\Delta w_{ij}$ into our cost function $B(w, D_0) = -\sum \log \Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the &ldquo;flatness&rdquo; of the minimum.</p></blockquote><p><img alt="Slide 36" loading=lazy src=/images/posts/flatminima/slide-36.jpg></p><blockquote><p>The complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\lambda$ balances the two.</p></blockquote><p><img alt="Slide 37" loading=lazy src=/images/posts/flatminima/slide-37.jpg></p><blockquote><p>To minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network&rsquo;s output, which would typically be very expensive to compute.</p></blockquote><p><img alt="Slide 38" loading=lazy src=/images/posts/flatminima/slide-38.jpg></p><blockquote><p>However, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$—the same as standard backpropagation—using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.</p></blockquote><p><img alt="Slide 39" loading=lazy src=/images/posts/flatminima/slide-39.jpg></p><blockquote><p>To recap the key advantages: the &ldquo;Flat Minima&rdquo; method is appealing because it uses geometry instead of priors to define simplicity, it&rsquo;s computationally efficient, and it naturally performs network pruning for better generalization.</p></blockquote><p><img alt="Slide 40" loading=lazy src=/images/posts/flatminima/slide-40.jpg></p><blockquote><p>The paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.</p></blockquote><p><img alt="Slide 41" loading=lazy src=/images/posts/flatminima/slide-41.jpg></p><blockquote><p>In the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.</p></blockquote><p><img alt="Slide 42" loading=lazy src=/images/posts/flatminima/slide-42.jpg></p><blockquote><p>The results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.</p></blockquote><p><img alt="Slide 43" loading=lazy src=/images/posts/flatminima/slide-43.jpg></p><blockquote><p>The second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.</p></blockquote><p><img alt="Slide 44" loading=lazy src=/images/posts/flatminima/slide-44.jpg></p><blockquote><p>The final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.</p></blockquote><p><img alt="Slide 45" loading=lazy src=/images/posts/flatminima/slide-45.jpg></p><blockquote><p>The results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/optimization/>Optimization</a></li><li><a href=https://mookjsi.github.io/tags/generalization/>Generalization</a></li><li><a href=https://mookjsi.github.io/tags/nips-1994/>NIPS 1994</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/paper-review-promptriever/><span class=title>« Prev</span><br><span>Promptriever - Instruction-Trained Retrievers</span>
</a><a class=next href=https://mookjsi.github.io/posts/paper-review-maxtoken/><span class=title>Next »</span><br><span>Max-Margin Token Selection in Attention Mechanism</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on x" href="https://x.com/intent/tweet/?text=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f&amp;hashtags=PaperReview%2cDeepLearning%2cOptimization%2cGeneralization%2cNIPS1994"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f&amp;title=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima&amp;summary=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f&title=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on whatsapp" href="https://api.whatsapp.com/send?text=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on telegram" href="https://telegram.me/share/url?text=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simplifying Neural Nets by Discovering Flat Minima on ycombinator" href="https://news.ycombinator.com/submitlink?t=Simplifying%20Neural%20Nets%20by%20Discovering%20Flat%20Minima&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flatminima%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>