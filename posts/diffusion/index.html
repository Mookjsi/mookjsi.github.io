<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Diffusion Models: From VAEs to DDPM Derivation | MookStudy</title><meta name=keywords content="Generative Models,Diffusion Models,VAE,DDPM,Deep Learning,Math"><meta name=description content="A comprehensive breakdown of Diffusion Models, starting from the limitations of Hierarchical VAEs, covering the thermodynamics inspiration, and detailing the full mathematical derivation of the ELBO and the simplified noise-prediction loss used in DDPM. This post is based on a presentation I gave at a YBIGTA (Big data society in Yonsei) session."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/diffusion/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/diffusion/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/diffusion/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Diffusion Models: From VAEs to DDPM Derivation"><meta property="og:description" content="A comprehensive breakdown of Diffusion Models, starting from the limitations of Hierarchical VAEs, covering the thermodynamics inspiration, and detailing the full mathematical derivation of the ELBO and the simplified noise-prediction loss used in DDPM. This post is based on a presentation I gave at a YBIGTA (Big data society in Yonsei) session."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-25T00:00:00+00:00"><meta property="article:tag" content="Generative Models"><meta property="article:tag" content="Diffusion Models"><meta property="article:tag" content="VAE"><meta property="article:tag" content="DDPM"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Math"><meta name=twitter:card content="summary"><meta name=twitter:title content="Diffusion Models: From VAEs to DDPM Derivation"><meta name=twitter:description content="A comprehensive breakdown of Diffusion Models, starting from the limitations of Hierarchical VAEs, covering the thermodynamics inspiration, and detailing the full mathematical derivation of the ELBO and the simplified noise-prediction loss used in DDPM. This post is based on a presentation I gave at a YBIGTA (Big data society in Yonsei) session."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Diffusion Models: From VAEs to DDPM Derivation","item":"https://mookjsi.github.io/posts/diffusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Diffusion Models: From VAEs to DDPM Derivation","name":"Diffusion Models: From VAEs to DDPM Derivation","description":"A comprehensive breakdown of Diffusion Models, starting from the limitations of Hierarchical VAEs, covering the thermodynamics inspiration, and detailing the full mathematical derivation of the ELBO and the simplified noise-prediction loss used in DDPM. This post is based on a presentation I gave at a YBIGTA (Big data society in Yonsei) session.","keywords":["Generative Models","Diffusion Models","VAE","DDPM","Deep Learning","Math"],"articleBody":"\nSlide 1: Preface Topic: Diffusion Models (In Process) Sources referenced: Deep Generative Modeling, Jacob M. Tomczak, 2022 Denoising Diffusion Probabilistic Models, Ho et al., 2020 Deep Unsupervised Learning using Nonequilibrium Thermodynamics, Sohl-Dickstein et al., 2015 Slide 2: Introduction - Data Representation An example of how we perceive data versus how a machine might process features. Human view: “How cute! This adorable little dog has a white soft coat and big eyes with excitement.” Visual Data: An image of a white dog. Slide 3: Computer Representation Computer view: The image is represented as a large tensor of pixel values (e.g., RGB arrays like [200, 189, 165]...). Slide 4: Latent Variable Space (1) Concept: Introduction to Latent Variable Space. We want to map the high-dimensional raw pixel data (Real Data, $x$) to a more meaningful, compressed representation. The descriptive features (“How cute”, “white soft coat”) represent the abstract information we want to capture. Slide 5: Latent Variable Space (2) $x$ (Real Data): High-dimensional pixel array. Latent Variable: A compressed vector (e.g., [1.23, -0.85, 0.44, ..., 0.03]) that encodes the semantic features of the data. Slide 6: Latent Variable Space (3) Visualizing the relationship: The raw data array corresponds to the latent vector. Slide 7: Mapping Semantics Latent Variable: Represents features like “Dog”, “Cute”, “White”, “soft”, “big eyes”, “excitement”. These abstract concepts are encoded into the numeric vector $z$. Slide 8: The Generative Goal Goal: To establish a two-way mapping. $x \\to z$: Encoding the image into latent features. $z \\to x$: Generating the image from latent features. Slide 9: The Process Make nice $z$: Generate a meaningful latent vector from semantic descriptions or distributions. Reconstruct image: Use $z$ to create $x$. Slide 10: Requirements for a Good Model Make nice $z$: The latent space should be well-structured (e.g., following a standard normal distribution). Reconstruct image well: The generated image should look like the real data. Slide 11: Example - Hierarchical VAE Hierarchical VAE: An extension of standard VAEs. Uses multiple layers of latent variables ($z_1, z_2, …$) to capture features at different levels of abstraction. Slide 12: Hierarchical VAE Structure Structure: A (Generative/Top-down): $z_2 \\to z_1 \\to x$. B (Inference/Bottom-up): $x \\to z_1 \\to z_2$. Objective: Forces the model to use $z$ to create an Inductive Bias. Slide 13: The Problem - Posterior Collapse Even with hierarchy, Posterior Collapse remains a problem. Issue: The variational posterior has a bottom-up path, but the generative part has a top-down path. Sometimes the model ignores the latent code $z$ and relies solely on the powerful decoder (autoregressive part), making $z$ meaningless. Slide 14: Analyzing Posterior Collapse ELBO Equation: $$ELBO(x) = E_{q(z_1:z_2|x)}[\\ln p(x|z_1)] - KL[q(z_1|x)||p(z_1|z_2)] - KL[q(z_2|z_1)||p(z_2)]$$ Collapse condition: If $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$, then $q$ doesn’t encode any meaningful information about the data. Slide 15: Hierarchical VAE (Top-down Inference) Solution attempt: Make the variational posterior top-down as well. Mechanism: Use residual connections ($r_1, r_2$) and delta parameters ($\\Delta\\mu, \\Delta\\sigma^2$) to guide the generation. Benefit: Forces the model to use $z$. Creates stronger inductive bias. Relieves Posterior Collapse because both Variational Posterior \u0026 Generative parts have a top-down path. Note: $q(z_2|z_1) \\approx p(z_2)$ cannot happen easily here. Slide 16: Introduction to Diffusion Based Deep Generative Models Comparing the structures. Diffusion Models: B (Bottom-up): Add Gaussian noise repeatedly. A (Top-down): Denoise \u0026 generation. Slide 17: Diffusion Process Visualization Forward (B): Start with image $x$ -\u003e add noise -\u003e $z_1$ -\u003e … -\u003e $z_5$ (pure noise). Reverse (A): Start with noise $z_5$ -\u003e denoise -\u003e … -\u003e generate $x$. Slide 18: VAEs vs. DDGMs (Structural Comparison) VAEs: A (Generative): Top-down ($z_2 \\to z_1 \\to x$). B (Inference): Bottom-up ($x \\to z_1 \\to z_2$). Structure has opposite paths. DDGMs (Diffusion): A (Generative): Top-down ($z_2 \\to z_1 \\to x$). B (Inference): Bottom-up ($x \\to z_1 \\to z_2$). Same structure visually, but distinct operational paths. Slide 19: Addressing Posterior Collapse in DDGMs Question: Does Posterior Collapse happen in DDGMs? VAE: Often suffers from $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$. DDGM: The structure is similar (opposite paths), so we need to check. Slide 20: Top-Down Fix in VAEs Recall slide 15: VAEs fixed collapse by making both paths top-down using ResNet-like connections. Does Diffusion do this? Slide 21: Diffusion’s Approach Diffusion models have the standard opposite path structure (Forward vs Reverse). Does Posterior Collapse remain? $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$? Answer: No. Slide 22: Why No Posterior Collapse? In Diffusion, the forward process (Bottom-up B) is fixed to add noise. It forces the data to become Gaussian noise. It is not a learned parameter that can “collapse” to zero information; it is a fixed physical process. Slide 23: Paper Introduction Paper: Deep Unsupervised Learning using Nonequilibrium Thermodynamics (ICML 2015). Authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli. Slide 24: Motivation - Nonequilibrium Thermodynamics Physical Analogy: A drop of ink diffusing in water. Equation: $\\frac{\\partial p(x,t)}{\\partial t} = D\\nabla^2 p(x,t)$ Forward Process: $dx_t = -\\frac{1}{2}\\beta x_t dt + \\beta dW_t$ Organized structure (Ink drop) $\\to$ Disorganized structure (Uniform distribution). Slide 25: Our Objective Map the diffusion process to image generation. Sequence: $x \\to z_1 \\to z_2 \\to z_3 \\to z_4 \\to z_5$ (Noise). We want to reverse this. Slide 26: Defining the Target Distribution We want to learn the distribution $p_\\theta(x)$ that generates the cat image. Problem: How can we know the distribution of $x$ from $z$? Slide 27: Formulation Strategy We need to write down $p_\\theta(x)$ with respect to latent variables $z$. Slide 28: Formulation (Top-down) - Marginalization Express $p_\\theta(x)$ by marginalizing over all latent steps: $$p_{\\theta}(x) = \\int p_{\\theta}(x, z_{1:T}) dz_{1:T}$$ Slide 29: Joint Distribution Expansion Expand the joint probability using the chain rule (Markov property): $$p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ Slide 30: Detailed Expansion Since $p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_{1:T})p_{\\theta}(z_{1:T})$ By Markov property (dependency only on previous step): $$= p_{\\theta}(x|z_1) p_{\\theta}(z_1|z_2) p_{\\theta}(z_2|z_3) \\cdots p_{\\theta}(z_{T-1}|z_T) p_{\\theta}(z_T)$$ $$= p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ Slide 31: Top-down Path Formula Top-down Integral: $$p_{\\theta}(x) = \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ This represents the path from Noise ($z_T$) back to Image ($x$). Slide 32: Formulation (Bottom-up) Now, how do we make $z$ from $x$ (Forward process)? Sequence: $x \\to z_1 \\to z_2 \\dots \\to z_5$. Slide 33: Forward Posterior Define the approximate posterior $Q$: $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 34: Forward Chain Expansion Breaking down the joint posterior: $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) q_{\\phi}(z_2|z_1, x) \\cdots q_{\\phi}(z_T|z_{1:T-1}, x)$$ Utilizing Markov property: $$= q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 35: Defining the Transition Kernel The forward step is defined as adding Gaussian noise: $$q_{\\phi}(z_i|z_{i-1}) = \\mathcal{N}(z_i | \\sqrt{1-\\beta_i}z_{i-1}, \\beta_i I)$$ Slide 36: Reparameterization Using the reparameterization trick: $$z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$ Slide 37: Learning $\\beta$? Question: Do we have to learn the schedule of $\\beta_i$? Slide 38: Fixed Variance Answer: It could be learned, but fixed $\\beta$ also works well! References: Sohl-Dickstein et al., Ho et al. Slide 39: Overall Formulation Summary Bottom-up (Encoder/Forward): $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ $$z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon$$ Top-down (Decoder/Reverse): $$p_{\\theta}(x) = \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ Slide 40: Where are the Learnable Parameters? Bottom-up: Fixed (Gaussian noise). Top-down: Parameters $\\theta$ exist here! We need to learn the reverse process. Slide 41: Learning Parameters We learn parameters $\\theta$ by maximizing likelihood. Slide 42: Intractability Objective: Maximize $\\ln p_{\\theta}(x)$. $$\\ln p_{\\theta}(x) = \\ln \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ Problem: Can we calculate this integral? No, it’s too expensive (intractable). Slide 43: The Trick However, the forward process $Q_{\\phi}(z_{1:T}|x)$ is “easy peasy” (just adding noise). We can use this to approximate the objective. Slide 44: Importance Sampling Setup Multiply and divide by $Q$: $$= \\ln \\int Q_{\\phi}(z_{1:T}|x) \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} dz_{1:T}$$ Slide 45: Expectation Form Convert integral to expectation: $$= \\ln E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ This is related to Importance Sampling. Slide 46: Jensen’s Inequality We use Jensen’s Inequality: $f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]$ for convex $f$ (or $\\ge$ for concave like $\\ln$). $$\\ge E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\ln \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ Slide 47: The Lower Bound We now have a lower bound on the log-likelihood: $$\\ln p_{\\theta}(x) \\ge E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\ln \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ Slide 48: Expanding the Terms Substitute the definitions of $p_{\\theta}$ (reverse) and $Q_{\\phi}$ (forward): $$p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 49: Log Expansion (1) Expanding the log term inside the expectation: $$E_{Q_{\\phi}} [\\ln p_{\\theta}(x|z_1) + \\sum \\ln p_{\\theta} - \\sum \\ln q_{\\phi} - \\ln q_{\\phi}(z_1|x)]$$ Slide 50: Log Expansion (2) Grouping terms: $$= E_{Q_{\\phi}} [\\ln p_{\\theta}(x|z_1) + \\ln p_{\\theta}(z_1|z_2) + \\sum_{i=2}^{T-1} \\ln p_{\\theta}(z_i|z_{i+1}) + \\ln p_{\\theta}(z_T)$$ $$- \\sum_{i=2}^{T-1} \\ln q_{\\phi}(z_i|z_{i-1}) - \\ln q_{\\phi}(z_T|z_{T-1}) - \\ln q_{\\phi}(z_1|x)]$$ Slide 51: Regrouping for KL Divergence We want to pair matching $p$ and $q$ terms to form KL Divergences. Grouping $(p_{\\theta}(z_i|z_{i+1})$ and $q_{\\phi}(z_i|z_{i-1}))$. Slide 52: KL Divergence Recap Definition: $KL(q||p) = \\mathbb{E}_q [\\ln q - \\ln p]$ We will use this to simplify the expectation terms. Slide 53: Applying KL Definition $E_{Q_{\\phi}} [\\ln p - \\ln q] = - E_{Q_{\\phi}} [\\ln q - \\ln p] = - KL(q||p)$ Slide 54: Identifying Terms (1) Looking at the expanded equation again to match terms. Slide 55: Identifying Terms (2) The sum term becomes a sum of KL divergences: $$\\sum_{i=2}^{T-1} E [\\ln p_{\\theta}(z_i|z_{i+1}) - \\ln q_{\\phi}(z_i|z_{i-1})] = - \\sum_{i=2}^{T-1} KL(q_{\\phi}(z_i|z_{i-1}) || p_{\\theta}(z_i|z_{i+1}))$$ Slide 56: Identifying Terms (3) Placing the summation term back into the equation. Slide 57: Identifying Terms (4) Dealing with the $z_T$ term: $$E [\\ln p_{\\theta}(z_T) - \\ln q_{\\phi}(z_T|z_{T-1})] = - KL(q_{\\phi}(z_T|z_{T-1}) || p_{\\theta}(z_T))$$ Slide 58: Identifying Terms (5) Only the $z_1$ term and reconstruction term remain. Slide 59: Identifying Terms (6) The $z_1$ term: $$E [\\ln p_{\\theta}(z_1|z_2) - \\ln q_{\\phi}(z_1|x)] = - KL(q_{\\phi}(z_1|x) || p_{\\theta}(z_1|z_2))$$ Slide 60: Final Grouping All terms are now converted to Expectations or KL divergences. Slide 61: The ELBO Equation Evidence Lower Bound (ELBO): $$ \\begin{aligned} L_{ELBO} \u0026= \\mathbb{E}_{Q_{\\phi}} [\\ln p_{\\theta}(x|z_1)] \\\\ \u0026\\quad - \\sum_{i=2}^{T-1} KL\\big(q_{\\phi}(z_i \\mid z_{i-1}) \\,\\|\\, p_{\\theta}(z_i \\mid z_{i+1})\\big) \\\\ \u0026\\quad - KL\\big(q_{\\phi}(z_T \\mid z_{T-1}) \\,\\|\\, p_{\\theta}(z_T)\\big) \\\\ \u0026\\quad - KL\\big(q_{\\phi}(z_1 \\mid x) \\,\\|\\, p_{\\theta}(z_1 \\mid z_2)\\big) \\end{aligned} $$ Slide 62: Usefulness of ELBO This gives us a computable lower bound on the true log-likelihood. We maximize this ELBO to learn the parameters. Slide 63: The Reconstruction Term Focus on the first term: $\\mathbb{E}{Q{\\phi}} [\\ln p_{\\theta}(x|z_1)]$ What distribution should we use for $p_{\\theta}(x|z_1)$? Slide 64: Gaussian Reconstruction Assume Gaussian distribution: $p(x|z_1) = \\mathcal{N}(x | \\tanh(NN(z_1)), I)$ The log-likelihood of a Gaussian is proportional to the negative Mean Squared Error (MSE). $\\ln p(x|z_1) = -MSE(x, \\tanh(NN(z_1))) + const$ Slide 65: Final ELBO for Implementation Result: $$L_{ELBO} = E[-MSE] - \\sum KL - KL - KL$$ Now we can implement this in code since it consists of MSE and KL divergence terms, which are differentiable. Slide 66: Revisiting the VAE Comparison Why did we go through all this math? To check stability and Posterior Collapse again. Slide 67: Posterior Collapse in DDGMs? VAE: Collapsed if $q(z|x) \\approx \\mathcal{N}(0,1)$. DDGM: The goal is to make $z$ become Gaussian noise! So “collapse” in the VAE sense is actually the objective of the forward diffusion process. Slide 68: DDPM Paper Paper: Denoising Diffusion Probabilistic Models (NeurIPS 2020). Authors: Jonathan Ho, Ajay Jain, Pieter Abbeel. This paper simplified the training of diffusion models significantly. Slide 69: Original Model Recap Bottom-up (Forward): Gaussian noise steps. Top-down (Reverse): Learned denoising. Slide 70: Efficiency Problem Do we have to sample iteratively $i=1…T$ just to get the forward noise $z_T$? $z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon$ Slide 71: Cumulation Trick (1) We can skip steps. $z_2 = \\sqrt{\\alpha_2}z_1 + \\sqrt{1-\\alpha_2}\\epsilon_2$ Substitute $z_1$: $z_2 = \\sqrt{\\alpha_2}(\\sqrt{\\alpha_1}x + \\sqrt{1-\\alpha_1}\\epsilon_1) + \\sqrt{1-\\alpha_2}\\epsilon_2$ (Where $\\alpha_i = 1 - \\beta_i$) Slide 72: Cumulation Trick (2) Combining Gaussians: $z_t = \\sqrt{\\alpha_t}z_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_t$ By induction, we can express $z_t$ directly from $x$. Slide 73: Cumulation Trick (3) Result: $$z_t = \\sqrt{\\bar{\\alpha}_t}x + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$$ Where $\\bar{\\alpha}t = \\prod{s=1}^t \\alpha_s$. Mean: $\\sqrt{\\bar{\\alpha}_t}x$ Variance: $(1-\\bar{\\alpha}_t)I$ Slide 74: Closed Form Forward Step We can sample $z_t$ at any timestep $t$ in one go: $$q(z_t|x) = \\mathcal{N}(z_t | \\sqrt{\\bar{\\alpha}_t}x, (1-\\bar{\\alpha}_t)I)$$ Slide 75: New Model Formulation Forward: Directly sample $z_t$ using the cumulation trick. Reverse: Same learned process. Slide 76: DDPM Learning Parameters We revisit the loss function derivation with this new trick in mind. Objective: Maximize $\\ln p_{\\theta}(x)$. Slide 77: Deriving DDPM Loss (1) Using the same variational bound logic: $$= E_Q [\\ln \\frac{p(z_T) \\prod p(z_{t-1}|z_t) p(x|z_1)}{Q(z_T|x) \\prod Q(z_{t-1}|z_t, x)}]$$ Slide 78: Deriving DDPM Loss (2) Grouping terms into KL divergences again. Terminology: $L_T$: KL between $Q(z_T|x)$ and $p(z_T)$ (Prior matching). $L_{t-1}$: KL between $Q(z_{t-1}|z_t, x)$ and $p_{\\theta}(z_{t-1}|z_t)$ (Denoising matching). $L_0$: Reconstruction log likelihood. Slide 79: Loss Terms Definition $L_T = D_{KL}(Q_{\\phi}(z_T|x) || p(z_T))$ $L_{t-1} = D_{KL}(Q_{\\phi}(z_{t-1}|z_t, x) || p_{\\theta}(z_{t-1}|z_t))$ $L_0 = -E_{Q}[ \\ln p_{\\theta}(x|z_1) ]$ Total Loss: Minimize $\\sum L$ terms. Slide 80: The Posterior $q(z_{t-1}|z_t, x_0)$ This term $Q_{\\phi}(z_{t-1}|z_t, x)$ is tractable! It is a Gaussian distribution $\\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)$. Mean $\\tilde{\\mu}_t$: A weighted combination of $x_0$ and $z_t$. Variance $\\tilde{\\beta}_t$: Function of $\\beta$ and $\\bar{\\alpha}$. Slide 81: Posterior Derivation Proof involves Gaussian conditioning formulas. Combining the forward equations allows us to solve for the distribution of $z_{t-1}$ given $z_t$ and $x_0$. Slide 82: Loss Summation We sum the expectations of the loss terms. Slide 83: Expectation Simplification $L_T$ relies on $z_T$. $L_{t-1}$ relies on $z_t, z_{t-1}$. By Markov chain property, we calculate expectations appropriately. Slide 84: Minimization Objective Minimize $\\sum_{t=1}^T \\mathbb{E}_{q}[L_t]$. Slide 85: Randomized Time Sampling Instead of summing all $T$ terms every step, we can sample $t \\sim Uniform(1, \\dots, T)$. Minimize $E_{t} [E_{q}[L_t]]$. Slide 86: Gradient Descent We can take the gradient of this expected loss. Slide 87: Efficient Training OOM: Calculating all $T$ steps is memory intensive. Solution: Only calculate one random $t$ per optimization step. Slide 88: Analyzing the Loss Let’s look at the full ELBO equation again. Slide 89: Positivity of KL KL divergences are non-negative. Since our models are not perfect approximations, these KL terms will be strictly positive. Slide 90: Accumulation of Error As $T$ grows (many timesteps), there are many KL terms. This makes the ELBO value very small (large negative). Slide 91: Unstable Learning Summing many KL terms can lead to variance issues and unstable learning. Slide 92: Changing the Goal We focus on the specific term: $L_{t-1} = D_{KL}(Q(z_{t-1}|z_t, x) || p_{\\theta}(z_{t-1}|z_t))$. Slide 93: Matching Distributions Our goal is to make the learned reverse process $p_{\\theta}(z_{t-1}|z_t)$ approximate the true posterior $q_{\\phi}(z_{t-1}|z_t, x)$. Slide 94: Gaussian Matching Both distributions are Gaussian. $q \\sim \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)$ $p_{\\theta} \\sim \\mathcal{N}(\\mu_{\\theta}, \\sigma_t^2 I)$ (Usually $\\sigma_t^2$ is fixed to $\\beta_t$ or $\\tilde{\\beta}_t$). Slide 95: KL between Gaussians The KL divergence between two Gaussians with diagonal covariance is proportional to the MSE of their means. Minimize: $$ \\frac{1}{2\\sigma_t^2} \\left\\lVert \\tilde{\\mu}_t - \\mu_{\\theta} \\right\\rVert^2 $$ Slide 96: Parametrization of $\\mu_{\\theta}$ We know $\\tilde{\\mu}_t(z_t, x)$ can be written in terms of $z_t$ and the noise $\\epsilon$ used to generate it. $\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon)$ So, we should parameterize $\\mu_{\\theta}$ to predict $\\epsilon$! $$ \\mu_{\\theta} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(z_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(z_t, t)\\right) $$ Slide 97: Derivation Step 1 Recall the definition of $\\tilde{\\mu}_t$ (weighted average of $x_0$ and $z_t$). Slide 98: Derivation Step 2 Substitute $x_0$ with its expression derived from $z_t$ and $\\epsilon$. $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(z_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)$ Slide 99: Derivation Step 3 Simplify the algebra to get the expression for $\\tilde{\\mu}_t$ solely in terms of $z_t$ and $\\epsilon$. Slide 100: Matching Means $$ \\text{Since }\\tilde{\\mu}_t\\text{ depends on }\\epsilon\\text{, our model }\\mu_{\\theta}\\text{ should approximate it by using a neural network }\\epsilon_{\\theta}(z_t, t)\\text{ to predict that noise.} $$ Slide 101: Comparison $$ \\text{Comparison of the target mean }\\tilde{\\mu}_t\\text{ and the model mean }\\mu_{\\theta}\\text{.} $$ They are identical in form, except one uses true noise $\\epsilon$ and the other uses predicted noise $\\epsilon_{\\theta}$. Slide 102: Difference of Means $$ \\text{Calculating }\\tilde{\\mu}_t - \\mu_{\\theta}\\text{.} $$ Everything cancels out except the noise terms. Difference $\\propto (\\epsilon - \\epsilon_{\\theta})$. Slide 103: Simplified Loss Function The loss becomes weighted MSE between true noise and predicted noise. $$ L_{simple} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\left\\lVert \\epsilon - \\epsilon_{\\theta}\\big(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon, t\\big) \\right\\rVert^2 \\right] $$ $\\lambda_t$ is a weighting term derived from the variances. Slide 104: Final Simplification Ho et al. (DDPM) found that ignoring the weighting term $\\lambda_t$ (setting it to 1) works better in practice. It puts more weight on difficult aspects of the denoising task. Slide 105: The “Simple” Loss $$ L_{simple} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\left\\lVert \\epsilon - \\epsilon_{\\theta}\\big(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\\big) \\right\\rVert^2 \\right] $$ We simply train a network to predict the noise added to the image. Slide 106: Algorithms Algorithm 1 (Training): Sample $x_0$. Sample $t$. Sample noise $\\epsilon$. Gradient descent on $|| \\epsilon - \\epsilon_{\\theta}(\\text{noisy input}, t) ||^2$. Algorithm 2 (Sampling): Start from $x_T \\sim \\mathcal{N}(0, I)$. Iteratively denoise: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\dots \\epsilon_{\\theta}(x_t, t)) + \\sigma_t z$. Slide 107: Comparison of Training Methods (A) Full ELBO (Sohl-Dickstein): Requires $T$-step gradients, large memory, slow convergence. (B) Simplified Noise-Prediction (Ho et al.): One-step gradient (random $t$), no KL accumulation, efficient, stable training. Slide 108: Conclusion Thank You. ","wordCount":"2818","inLanguage":"en","datePublished":"2025-11-25T00:00:00Z","dateModified":"2025-11-25T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/diffusion/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Diffusion Models: From VAEs to DDPM Derivation</h1><div class=post-meta><span title='2025-11-25 00:00:00 +0000 UTC'>November 25, 2025</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2818 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p><img alt="Slide 1" loading=lazy src=/images/posts/diffusion/slide-01.jpg></p><h3 id=slide-1-preface>Slide 1: Preface<a hidden class=anchor aria-hidden=true href=#slide-1-preface>#</a></h3><ul><li><strong>Topic</strong>: Diffusion Models (In Process)</li><li><strong>Sources referenced</strong>:<ul><li>Deep Generative Modeling, Jacob M. Tomczak, 2022</li><li>Denoising Diffusion Probabilistic Models, Ho et al., 2020</li><li>Deep Unsupervised Learning using Nonequilibrium Thermodynamics, Sohl-Dickstein et al., 2015</li></ul></li></ul><hr><p><img alt="Slide 2" loading=lazy src=/images/posts/diffusion/slide-02.jpg></p><h3 id=slide-2-introduction---data-representation>Slide 2: Introduction - Data Representation<a hidden class=anchor aria-hidden=true href=#slide-2-introduction---data-representation>#</a></h3><ul><li>An example of how we perceive data versus how a machine might process features.</li><li><strong>Human view</strong>: &ldquo;How cute! This adorable little dog has a white soft coat and big eyes with excitement.&rdquo;</li><li><strong>Visual Data</strong>: An image of a white dog.</li></ul><hr><p><img alt="Slide 3" loading=lazy src=/images/posts/diffusion/slide-03.jpg></p><h3 id=slide-3-computer-representation>Slide 3: Computer Representation<a hidden class=anchor aria-hidden=true href=#slide-3-computer-representation>#</a></h3><ul><li><strong>Computer view</strong>: The image is represented as a large tensor of pixel values (e.g., RGB arrays like <code>[200, 189, 165]...</code>).</li></ul><hr><p><img alt="Slide 4" loading=lazy src=/images/posts/diffusion/slide-04.jpg></p><h3 id=slide-4-latent-variable-space-1>Slide 4: Latent Variable Space (1)<a hidden class=anchor aria-hidden=true href=#slide-4-latent-variable-space-1>#</a></h3><ul><li><strong>Concept</strong>: Introduction to <strong>Latent Variable Space</strong>.</li><li>We want to map the high-dimensional raw pixel data (Real Data, $x$) to a more meaningful, compressed representation.</li><li>The descriptive features (&ldquo;How cute&rdquo;, &ldquo;white soft coat&rdquo;) represent the abstract information we want to capture.</li></ul><hr><p><img alt="Slide 5" loading=lazy src=/images/posts/diffusion/slide-05.jpg></p><h3 id=slide-5-latent-variable-space-2>Slide 5: Latent Variable Space (2)<a hidden class=anchor aria-hidden=true href=#slide-5-latent-variable-space-2>#</a></h3><ul><li><strong>$x$ (Real Data)</strong>: High-dimensional pixel array.</li><li><strong>Latent Variable</strong>: A compressed vector (e.g., <code>[1.23, -0.85, 0.44, ..., 0.03]</code>) that encodes the semantic features of the data.</li></ul><hr><p><img alt="Slide 6" loading=lazy src=/images/posts/diffusion/slide-06.jpg></p><h3 id=slide-6-latent-variable-space-3>Slide 6: Latent Variable Space (3)<a hidden class=anchor aria-hidden=true href=#slide-6-latent-variable-space-3>#</a></h3><ul><li>Visualizing the relationship: The raw data array corresponds to the latent vector.</li></ul><hr><p><img alt="Slide 7" loading=lazy src=/images/posts/diffusion/slide-07.jpg></p><h3 id=slide-7-mapping-semantics>Slide 7: Mapping Semantics<a hidden class=anchor aria-hidden=true href=#slide-7-mapping-semantics>#</a></h3><ul><li><strong>Latent Variable</strong>: Represents features like &ldquo;Dog&rdquo;, &ldquo;Cute&rdquo;, &ldquo;White&rdquo;, &ldquo;soft&rdquo;, &ldquo;big eyes&rdquo;, &ldquo;excitement&rdquo;.</li><li>These abstract concepts are encoded into the numeric vector $z$.</li></ul><hr><p><img alt="Slide 8" loading=lazy src=/images/posts/diffusion/slide-08.jpg></p><h3 id=slide-8-the-generative-goal>Slide 8: The Generative Goal<a hidden class=anchor aria-hidden=true href=#slide-8-the-generative-goal>#</a></h3><ul><li><strong>Goal</strong>: To establish a two-way mapping.<ul><li>$x \to z$: Encoding the image into latent features.</li><li>$z \to x$: Generating the image from latent features.</li></ul></li></ul><hr><p><img alt="Slide 9" loading=lazy src=/images/posts/diffusion/slide-09.jpg></p><h3 id=slide-9-the-process>Slide 9: The Process<a hidden class=anchor aria-hidden=true href=#slide-9-the-process>#</a></h3><ol><li><strong>Make nice $z$</strong>: Generate a meaningful latent vector from semantic descriptions or distributions.</li><li><strong>Reconstruct image</strong>: Use $z$ to create $x$.</li></ol><hr><p><img alt="Slide 10" loading=lazy src=/images/posts/diffusion/slide-10.jpg></p><h3 id=slide-10-requirements-for-a-good-model>Slide 10: Requirements for a Good Model<a hidden class=anchor aria-hidden=true href=#slide-10-requirements-for-a-good-model>#</a></h3><ol><li><strong>Make nice $z$</strong>: The latent space should be well-structured (e.g., following a standard normal distribution).</li><li><strong>Reconstruct image well</strong>: The generated image should look like the real data.</li></ol><hr><p><img alt="Slide 11" loading=lazy src=/images/posts/diffusion/slide-11.jpg></p><h3 id=slide-11-example---hierarchical-vae>Slide 11: Example - Hierarchical VAE<a hidden class=anchor aria-hidden=true href=#slide-11-example---hierarchical-vae>#</a></h3><ul><li><strong>Hierarchical VAE</strong>: An extension of standard VAEs.</li><li>Uses multiple layers of latent variables ($z_1, z_2, &mldr;$) to capture features at different levels of abstraction.</li></ul><hr><p><img alt="Slide 12" loading=lazy src=/images/posts/diffusion/slide-12.jpg></p><h3 id=slide-12-hierarchical-vae-structure>Slide 12: Hierarchical VAE Structure<a hidden class=anchor aria-hidden=true href=#slide-12-hierarchical-vae-structure>#</a></h3><ul><li><strong>Structure</strong>:<ul><li><strong>A (Generative/Top-down)</strong>: $z_2 \to z_1 \to x$.</li><li><strong>B (Inference/Bottom-up)</strong>: $x \to z_1 \to z_2$.</li></ul></li><li><strong>Objective</strong>: Forces the model to use $z$ to create an <strong>Inductive Bias</strong>.</li></ul><hr><p><img alt="Slide 13" loading=lazy src=/images/posts/diffusion/slide-13.jpg></p><h3 id=slide-13-the-problem---posterior-collapse>Slide 13: The Problem - Posterior Collapse<a hidden class=anchor aria-hidden=true href=#slide-13-the-problem---posterior-collapse>#</a></h3><ul><li>Even with hierarchy, <strong>Posterior Collapse</strong> remains a problem.</li><li><strong>Issue</strong>: The variational posterior has a bottom-up path, but the generative part has a top-down path.</li><li>Sometimes the model ignores the latent code $z$ and relies solely on the powerful decoder (autoregressive part), making $z$ meaningless.</li></ul><hr><p><img alt="Slide 14" loading=lazy src=/images/posts/diffusion/slide-14.jpg></p><h3 id=slide-14-analyzing-posterior-collapse>Slide 14: Analyzing Posterior Collapse<a hidden class=anchor aria-hidden=true href=#slide-14-analyzing-posterior-collapse>#</a></h3><ul><li><strong>ELBO Equation</strong>:
$$ELBO(x) = E_{q(z_1:z_2|x)}[\ln p(x|z_1)] - KL[q(z_1|x)||p(z_1|z_2)] - KL[q(z_2|z_1)||p(z_2)]$$</li><li><strong>Collapse condition</strong>: If $q(z_2|z_1) \approx p(z_2) \sim \mathcal{N}(0,1)$, then $q$ doesn&rsquo;t encode any meaningful information about the data.</li></ul><hr><p><img alt="Slide 15" loading=lazy src=/images/posts/diffusion/slide-15.jpg></p><h3 id=slide-15-hierarchical-vae-top-down-inference>Slide 15: Hierarchical VAE (Top-down Inference)<a hidden class=anchor aria-hidden=true href=#slide-15-hierarchical-vae-top-down-inference>#</a></h3><ul><li><strong>Solution attempt</strong>: Make the variational posterior top-down as well.</li><li><strong>Mechanism</strong>: Use residual connections ($r_1, r_2$) and delta parameters ($\Delta\mu, \Delta\sigma^2$) to guide the generation.</li><li><strong>Benefit</strong>:<ul><li>Forces the model to use $z$.</li><li>Creates stronger inductive bias.</li><li>Relieves Posterior Collapse because both Variational Posterior & Generative parts have a top-down path.</li></ul></li><li><strong>Note</strong>: $q(z_2|z_1) \approx p(z_2)$ cannot happen easily here.</li></ul><hr><p><img alt="Slide 16" loading=lazy src=/images/posts/diffusion/slide-16.jpg></p><h3 id=slide-16-introduction-to-diffusion-based-deep-generative-models>Slide 16: Introduction to Diffusion Based Deep Generative Models<a hidden class=anchor aria-hidden=true href=#slide-16-introduction-to-diffusion-based-deep-generative-models>#</a></h3><ul><li>Comparing the structures.</li><li><strong>Diffusion Models</strong>:<ul><li><strong>B (Bottom-up)</strong>: Add Gaussian noise repeatedly.</li><li><strong>A (Top-down)</strong>: Denoise & generation.</li></ul></li></ul><hr><p><img alt="Slide 17" loading=lazy src=/images/posts/diffusion/slide-17.jpg></p><h3 id=slide-17-diffusion-process-visualization>Slide 17: Diffusion Process Visualization<a hidden class=anchor aria-hidden=true href=#slide-17-diffusion-process-visualization>#</a></h3><ul><li><strong>Forward (B)</strong>: Start with image $x$ -> add noise -> $z_1$ -> &mldr; -> $z_5$ (pure noise).</li><li><strong>Reverse (A)</strong>: Start with noise $z_5$ -> denoise -> &mldr; -> generate $x$.</li></ul><hr><p><img alt="Slide 18" loading=lazy src=/images/posts/diffusion/slide-18.jpg></p><h3 id=slide-18-vaes-vs-ddgms-structural-comparison>Slide 18: VAEs vs. DDGMs (Structural Comparison)<a hidden class=anchor aria-hidden=true href=#slide-18-vaes-vs-ddgms-structural-comparison>#</a></h3><ul><li><strong>VAEs</strong>:<ul><li>A (Generative): Top-down ($z_2 \to z_1 \to x$).</li><li>B (Inference): Bottom-up ($x \to z_1 \to z_2$).</li><li><em>Structure has opposite paths.</em></li></ul></li><li><strong>DDGMs (Diffusion)</strong>:<ul><li>A (Generative): Top-down ($z_2 \to z_1 \to x$).</li><li>B (Inference): Bottom-up ($x \to z_1 \to z_2$).</li><li><em>Same structure visually, but distinct operational paths.</em></li></ul></li></ul><hr><p><img alt="Slide 19" loading=lazy src=/images/posts/diffusion/slide-19.jpg></p><h3 id=slide-19-addressing-posterior-collapse-in-ddgms>Slide 19: Addressing Posterior Collapse in DDGMs<a hidden class=anchor aria-hidden=true href=#slide-19-addressing-posterior-collapse-in-ddgms>#</a></h3><ul><li><strong>Question</strong>: Does Posterior Collapse happen in DDGMs?</li><li><strong>VAE</strong>: Often suffers from $q(z_2|z_1) \approx p(z_2) \sim \mathcal{N}(0,1)$.</li><li><strong>DDGM</strong>: The structure is similar (opposite paths), so we need to check.</li></ul><hr><p><img alt="Slide 20" loading=lazy src=/images/posts/diffusion/slide-20.jpg></p><h3 id=slide-20-top-down-fix-in-vaes>Slide 20: Top-Down Fix in VAEs<a hidden class=anchor aria-hidden=true href=#slide-20-top-down-fix-in-vaes>#</a></h3><ul><li>Recall slide 15: VAEs fixed collapse by making <em>both</em> paths top-down using ResNet-like connections.</li><li>Does Diffusion do this?</li></ul><hr><p><img alt="Slide 21" loading=lazy src=/images/posts/diffusion/slide-21.jpg></p><h3 id=slide-21-diffusions-approach>Slide 21: Diffusion&rsquo;s Approach<a hidden class=anchor aria-hidden=true href=#slide-21-diffusions-approach>#</a></h3><ul><li>Diffusion models have the standard opposite path structure (Forward vs Reverse).</li><li><strong>Does Posterior Collapse remain?</strong><ul><li>$q(z_2|z_1) \approx p(z_2) \sim \mathcal{N}(0,1)$?</li><li><strong>Answer</strong>: No.</li></ul></li></ul><hr><p><img alt="Slide 22" loading=lazy src=/images/posts/diffusion/slide-22.jpg></p><h3 id=slide-22-why-no-posterior-collapse>Slide 22: Why No Posterior Collapse?<a hidden class=anchor aria-hidden=true href=#slide-22-why-no-posterior-collapse>#</a></h3><ul><li>In Diffusion, the forward process (Bottom-up B) is <strong>fixed</strong> to add noise. It forces the data to become Gaussian noise.</li><li>It is not a learned parameter that can &ldquo;collapse&rdquo; to zero information; it is a fixed physical process.</li></ul><hr><p><img alt="Slide 23" loading=lazy src=/images/posts/diffusion/slide-23.jpg></p><h3 id=slide-23-paper-introduction>Slide 23: Paper Introduction<a hidden class=anchor aria-hidden=true href=#slide-23-paper-introduction>#</a></h3><ul><li><strong>Paper</strong>: <em>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</em> (ICML 2015).</li><li><strong>Authors</strong>: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli.</li></ul><hr><p><img alt="Slide 24" loading=lazy src=/images/posts/diffusion/slide-24.jpg></p><h3 id=slide-24-motivation---nonequilibrium-thermodynamics>Slide 24: Motivation - Nonequilibrium Thermodynamics<a hidden class=anchor aria-hidden=true href=#slide-24-motivation---nonequilibrium-thermodynamics>#</a></h3><ul><li><strong>Physical Analogy</strong>: A drop of ink diffusing in water.</li><li><strong>Equation</strong>: $\frac{\partial p(x,t)}{\partial t} = D\nabla^2 p(x,t)$</li><li><strong>Forward Process</strong>: $dx_t = -\frac{1}{2}\beta x_t dt + \beta dW_t$</li><li>Organized structure (Ink drop) $\to$ Disorganized structure (Uniform distribution).</li></ul><hr><p><img alt="Slide 25" loading=lazy src=/images/posts/diffusion/slide-25.jpg></p><h3 id=slide-25-our-objective>Slide 25: Our Objective<a hidden class=anchor aria-hidden=true href=#slide-25-our-objective>#</a></h3><ul><li>Map the diffusion process to image generation.</li><li><strong>Sequence</strong>: $x \to z_1 \to z_2 \to z_3 \to z_4 \to z_5$ (Noise).</li><li>We want to reverse this.</li></ul><hr><p><img alt="Slide 26" loading=lazy src=/images/posts/diffusion/slide-26.jpg></p><h3 id=slide-26-defining-the-target-distribution>Slide 26: Defining the Target Distribution<a hidden class=anchor aria-hidden=true href=#slide-26-defining-the-target-distribution>#</a></h3><ul><li>We want to learn the distribution $p_\theta(x)$ that generates the cat image.</li><li><strong>Problem</strong>: How can we know the distribution of $x$ from $z$?</li></ul><hr><p><img alt="Slide 27" loading=lazy src=/images/posts/diffusion/slide-27.jpg></p><h3 id=slide-27-formulation-strategy>Slide 27: Formulation Strategy<a hidden class=anchor aria-hidden=true href=#slide-27-formulation-strategy>#</a></h3><ul><li>We need to write down $p_\theta(x)$ with respect to latent variables $z$.</li></ul><hr><p><img alt="Slide 28" loading=lazy src=/images/posts/diffusion/slide-28.jpg></p><h3 id=slide-28-formulation-top-down---marginalization>Slide 28: Formulation (Top-down) - Marginalization<a hidden class=anchor aria-hidden=true href=#slide-28-formulation-top-down---marginalization>#</a></h3><ul><li>Express $p_\theta(x)$ by marginalizing over all latent steps:
$$p_{\theta}(x) = \int p_{\theta}(x, z_{1:T}) dz_{1:T}$$</li></ul><hr><p><img alt="Slide 29" loading=lazy src=/images/posts/diffusion/slide-29.jpg></p><h3 id=slide-29-joint-distribution-expansion>Slide 29: Joint Distribution Expansion<a hidden class=anchor aria-hidden=true href=#slide-29-joint-distribution-expansion>#</a></h3><ul><li>Expand the joint probability using the chain rule (Markov property):
$$p_{\theta}(x, z_{1:T}) = p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T)$$</li></ul><hr><p><img alt="Slide 30" loading=lazy src=/images/posts/diffusion/slide-30.jpg></p><h3 id=slide-30-detailed-expansion>Slide 30: Detailed Expansion<a hidden class=anchor aria-hidden=true href=#slide-30-detailed-expansion>#</a></h3><ul><li>Since $p_{\theta}(x, z_{1:T}) = p_{\theta}(x|z_{1:T})p_{\theta}(z_{1:T})$</li><li>By Markov property (dependency only on previous step):
$$= p_{\theta}(x|z_1) p_{\theta}(z_1|z_2) p_{\theta}(z_2|z_3) \cdots p_{\theta}(z_{T-1}|z_T) p_{\theta}(z_T)$$
$$= p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T)$$</li></ul><hr><p><img alt="Slide 31" loading=lazy src=/images/posts/diffusion/slide-31.jpg></p><h3 id=slide-31-top-down-path-formula>Slide 31: Top-down Path Formula<a hidden class=anchor aria-hidden=true href=#slide-31-top-down-path-formula>#</a></h3><ul><li><strong>Top-down Integral</strong>:
$$p_{\theta}(x) = \int p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T) dz_{1:T}$$</li><li>This represents the path from Noise ($z_T$) back to Image ($x$).</li></ul><hr><p><img alt="Slide 32" loading=lazy src=/images/posts/diffusion/slide-32.jpg></p><h3 id=slide-32-formulation-bottom-up>Slide 32: Formulation (Bottom-up)<a hidden class=anchor aria-hidden=true href=#slide-32-formulation-bottom-up>#</a></h3><ul><li>Now, how do we make $z$ from $x$ (Forward process)?</li><li>Sequence: $x \to z_1 \to z_2 \dots \to z_5$.</li></ul><hr><p><img alt="Slide 33" loading=lazy src=/images/posts/diffusion/slide-33.jpg></p><h3 id=slide-33-forward-posterior>Slide 33: Forward Posterior<a hidden class=anchor aria-hidden=true href=#slide-33-forward-posterior>#</a></h3><ul><li>Define the approximate posterior $Q$:
$$Q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x) \prod_{i=2}^{T} q_{\phi}(z_i|z_{i-1})$$</li></ul><hr><p><img alt="Slide 34" loading=lazy src=/images/posts/diffusion/slide-34.jpg></p><h3 id=slide-34-forward-chain-expansion>Slide 34: Forward Chain Expansion<a hidden class=anchor aria-hidden=true href=#slide-34-forward-chain-expansion>#</a></h3><ul><li>Breaking down the joint posterior:
$$Q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x) q_{\phi}(z_2|z_1, x) \cdots q_{\phi}(z_T|z_{1:T-1}, x)$$</li><li>Utilizing Markov property:
$$= q_{\phi}(z_1|x) \prod_{i=2}^{T} q_{\phi}(z_i|z_{i-1})$$</li></ul><hr><p><img alt="Slide 35" loading=lazy src=/images/posts/diffusion/slide-35.jpg></p><h3 id=slide-35-defining-the-transition-kernel>Slide 35: Defining the Transition Kernel<a hidden class=anchor aria-hidden=true href=#slide-35-defining-the-transition-kernel>#</a></h3><ul><li>The forward step is defined as adding Gaussian noise:
$$q_{\phi}(z_i|z_{i-1}) = \mathcal{N}(z_i | \sqrt{1-\beta_i}z_{i-1}, \beta_i I)$$</li></ul><hr><p><img alt="Slide 36" loading=lazy src=/images/posts/diffusion/slide-36.jpg></p><h3 id=slide-36-reparameterization>Slide 36: Reparameterization<a hidden class=anchor aria-hidden=true href=#slide-36-reparameterization>#</a></h3><ul><li>Using the reparameterization trick:
$$z_i = \sqrt{1-\beta_i}z_{i-1} + \sqrt{\beta_i}\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$</li></ul><hr><p><img alt="Slide 37" loading=lazy src=/images/posts/diffusion/slide-37.jpg></p><h3 id=slide-37-learning-beta>Slide 37: Learning $\beta$?<a hidden class=anchor aria-hidden=true href=#slide-37-learning-beta>#</a></h3><ul><li><strong>Question</strong>: Do we have to learn the schedule of $\beta_i$?</li></ul><hr><p><img alt="Slide 38" loading=lazy src=/images/posts/diffusion/slide-38.jpg></p><h3 id=slide-38-fixed-variance>Slide 38: Fixed Variance<a hidden class=anchor aria-hidden=true href=#slide-38-fixed-variance>#</a></h3><ul><li><strong>Answer</strong>: It could be learned, but <strong>fixed $\beta$ also works well!</strong></li><li>References: Sohl-Dickstein et al., Ho et al.</li></ul><hr><p><img alt="Slide 39" loading=lazy src=/images/posts/diffusion/slide-39.jpg></p><h3 id=slide-39-overall-formulation-summary>Slide 39: Overall Formulation Summary<a hidden class=anchor aria-hidden=true href=#slide-39-overall-formulation-summary>#</a></h3><ul><li><strong>Bottom-up (Encoder/Forward)</strong>:
$$Q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x) \prod_{i=2}^{T} q_{\phi}(z_i|z_{i-1})$$
$$z_i = \sqrt{1-\beta_i}z_{i-1} + \sqrt{\beta_i}\epsilon$$</li><li><strong>Top-down (Decoder/Reverse)</strong>:
$$p_{\theta}(x) = \int p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T) dz_{1:T}$$</li></ul><hr><p><img alt="Slide 40" loading=lazy src=/images/posts/diffusion/slide-40.jpg></p><h3 id=slide-40-where-are-the-learnable-parameters>Slide 40: Where are the Learnable Parameters?<a hidden class=anchor aria-hidden=true href=#slide-40-where-are-the-learnable-parameters>#</a></h3><ul><li><strong>Bottom-up</strong>: Fixed (Gaussian noise).</li><li><strong>Top-down</strong>: Parameters $\theta$ exist here! We need to learn the reverse process.</li></ul><hr><p><img alt="Slide 41" loading=lazy src=/images/posts/diffusion/slide-41.jpg></p><h3 id=slide-41-learning-parameters>Slide 41: Learning Parameters<a hidden class=anchor aria-hidden=true href=#slide-41-learning-parameters>#</a></h3><ul><li>We learn parameters $\theta$ by <strong>maximizing likelihood</strong>.</li></ul><hr><p><img alt="Slide 42" loading=lazy src=/images/posts/diffusion/slide-42.jpg></p><h3 id=slide-42-intractability>Slide 42: Intractability<a hidden class=anchor aria-hidden=true href=#slide-42-intractability>#</a></h3><ul><li>Objective: Maximize $\ln p_{\theta}(x)$.</li><li>$$\ln p_{\theta}(x) = \ln \int p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T) dz_{1:T}$$</li><li><strong>Problem</strong>: Can we calculate this integral? No, it&rsquo;s <strong>too expensive</strong> (intractable).</li></ul><hr><p><img alt="Slide 43" loading=lazy src=/images/posts/diffusion/slide-43.jpg></p><h3 id=slide-43-the-trick>Slide 43: The Trick<a hidden class=anchor aria-hidden=true href=#slide-43-the-trick>#</a></h3><ul><li>However, the forward process $Q_{\phi}(z_{1:T}|x)$ is &ldquo;easy peasy&rdquo; (just adding noise).</li><li>We can use this to approximate the objective.</li></ul><hr><p><img alt="Slide 44" loading=lazy src=/images/posts/diffusion/slide-44.jpg></p><h3 id=slide-44-importance-sampling-setup>Slide 44: Importance Sampling Setup<a hidden class=anchor aria-hidden=true href=#slide-44-importance-sampling-setup>#</a></h3><ul><li>Multiply and divide by $Q$:
$$= \ln \int Q_{\phi}(z_{1:T}|x) \frac{p_{\theta}(x, z_{1:T})}{Q_{\phi}(z_{1:T}|x)} dz_{1:T}$$</li></ul><hr><p><img alt="Slide 45" loading=lazy src=/images/posts/diffusion/slide-45.jpg></p><h3 id=slide-45-expectation-form>Slide 45: Expectation Form<a hidden class=anchor aria-hidden=true href=#slide-45-expectation-form>#</a></h3><ul><li>Convert integral to expectation:
$$= \ln E_{Q_{\phi}(z_{1:T}|x)} \left[ \frac{p_{\theta}(x, z_{1:T})}{Q_{\phi}(z_{1:T}|x)} \right]$$</li><li>This is related to Importance Sampling.</li></ul><hr><p><img alt="Slide 46" loading=lazy src=/images/posts/diffusion/slide-46.jpg></p><h3 id=slide-46-jensens-inequality>Slide 46: Jensen&rsquo;s Inequality<a hidden class=anchor aria-hidden=true href=#slide-46-jensens-inequality>#</a></h3><ul><li>We use <strong>Jensen&rsquo;s Inequality</strong>: $f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]$ for convex $f$ (or $\ge$ for concave like $\ln$).</li><li>$$\ge E_{Q_{\phi}(z_{1:T}|x)} \left[ \ln \frac{p_{\theta}(x, z_{1:T})}{Q_{\phi}(z_{1:T}|x)} \right]$$</li></ul><hr><p><img alt="Slide 47" loading=lazy src=/images/posts/diffusion/slide-47.jpg></p><h3 id=slide-47-the-lower-bound>Slide 47: The Lower Bound<a hidden class=anchor aria-hidden=true href=#slide-47-the-lower-bound>#</a></h3><ul><li>We now have a lower bound on the log-likelihood:
$$\ln p_{\theta}(x) \ge E_{Q_{\phi}(z_{1:T}|x)} \left[ \ln \frac{p_{\theta}(x, z_{1:T})}{Q_{\phi}(z_{1:T}|x)} \right]$$</li></ul><hr><p><img alt="Slide 48" loading=lazy src=/images/posts/diffusion/slide-48.jpg></p><h3 id=slide-48-expanding-the-terms>Slide 48: Expanding the Terms<a hidden class=anchor aria-hidden=true href=#slide-48-expanding-the-terms>#</a></h3><ul><li>Substitute the definitions of $p_{\theta}$ (reverse) and $Q_{\phi}$ (forward):
$$p_{\theta}(x, z_{1:T}) = p_{\theta}(x|z_1) \prod_{i=1}^{T-1} p_{\theta}(z_i|z_{i+1}) p_{\theta}(z_T)$$
$$Q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x) \prod_{i=2}^{T} q_{\phi}(z_i|z_{i-1})$$</li></ul><hr><p><img alt="Slide 49" loading=lazy src=/images/posts/diffusion/slide-49.jpg></p><h3 id=slide-49-log-expansion-1>Slide 49: Log Expansion (1)<a hidden class=anchor aria-hidden=true href=#slide-49-log-expansion-1>#</a></h3><ul><li>Expanding the log term inside the expectation:
$$E_{Q_{\phi}} [\ln p_{\theta}(x|z_1) + \sum \ln p_{\theta} - \sum \ln q_{\phi} - \ln q_{\phi}(z_1|x)]$$</li></ul><hr><p><img alt="Slide 50" loading=lazy src=/images/posts/diffusion/slide-50.jpg></p><h3 id=slide-50-log-expansion-2>Slide 50: Log Expansion (2)<a hidden class=anchor aria-hidden=true href=#slide-50-log-expansion-2>#</a></h3><ul><li>Grouping terms:
$$= E_{Q_{\phi}} [\ln p_{\theta}(x|z_1) + \ln p_{\theta}(z_1|z_2) + \sum_{i=2}^{T-1} \ln p_{\theta}(z_i|z_{i+1}) + \ln p_{\theta}(z_T)$$
$$- \sum_{i=2}^{T-1} \ln q_{\phi}(z_i|z_{i-1}) - \ln q_{\phi}(z_T|z_{T-1}) - \ln q_{\phi}(z_1|x)]$$</li></ul><hr><p><img alt="Slide 51" loading=lazy src=/images/posts/diffusion/slide-51.jpg></p><h3 id=slide-51-regrouping-for-kl-divergence>Slide 51: Regrouping for KL Divergence<a hidden class=anchor aria-hidden=true href=#slide-51-regrouping-for-kl-divergence>#</a></h3><ul><li>We want to pair matching $p$ and $q$ terms to form KL Divergences.</li><li>Grouping $(p_{\theta}(z_i|z_{i+1})$ and $q_{\phi}(z_i|z_{i-1}))$.</li></ul><hr><p><img alt="Slide 52" loading=lazy src=/images/posts/diffusion/slide-52.jpg></p><h3 id=slide-52-kl-divergence-recap>Slide 52: KL Divergence Recap<a hidden class=anchor aria-hidden=true href=#slide-52-kl-divergence-recap>#</a></h3><ul><li>Definition: $KL(q||p) = \mathbb{E}_q [\ln q - \ln p]$</li><li>We will use this to simplify the expectation terms.</li></ul><hr><p><img alt="Slide 53" loading=lazy src=/images/posts/diffusion/slide-53.jpg></p><h3 id=slide-53-applying-kl-definition>Slide 53: Applying KL Definition<a hidden class=anchor aria-hidden=true href=#slide-53-applying-kl-definition>#</a></h3><ul><li>$E_{Q_{\phi}} [\ln p - \ln q] = - E_{Q_{\phi}} [\ln q - \ln p] = - KL(q||p)$</li></ul><hr><p><img alt="Slide 54" loading=lazy src=/images/posts/diffusion/slide-54.jpg></p><h3 id=slide-54-identifying-terms-1>Slide 54: Identifying Terms (1)<a hidden class=anchor aria-hidden=true href=#slide-54-identifying-terms-1>#</a></h3><ul><li>Looking at the expanded equation again to match terms.</li></ul><hr><p><img alt="Slide 55" loading=lazy src=/images/posts/diffusion/slide-55.jpg></p><h3 id=slide-55-identifying-terms-2>Slide 55: Identifying Terms (2)<a hidden class=anchor aria-hidden=true href=#slide-55-identifying-terms-2>#</a></h3><ul><li>The sum term becomes a sum of KL divergences:
$$\sum_{i=2}^{T-1} E [\ln p_{\theta}(z_i|z_{i+1}) - \ln q_{\phi}(z_i|z_{i-1})] = - \sum_{i=2}^{T-1} KL(q_{\phi}(z_i|z_{i-1}) || p_{\theta}(z_i|z_{i+1}))$$</li></ul><hr><p><img alt="Slide 56" loading=lazy src=/images/posts/diffusion/slide-56.jpg></p><h3 id=slide-56-identifying-terms-3>Slide 56: Identifying Terms (3)<a hidden class=anchor aria-hidden=true href=#slide-56-identifying-terms-3>#</a></h3><ul><li>Placing the summation term back into the equation.</li></ul><hr><p><img alt="Slide 57" loading=lazy src=/images/posts/diffusion/slide-57.jpg></p><h3 id=slide-57-identifying-terms-4>Slide 57: Identifying Terms (4)<a hidden class=anchor aria-hidden=true href=#slide-57-identifying-terms-4>#</a></h3><ul><li>Dealing with the $z_T$ term:
$$E [\ln p_{\theta}(z_T) - \ln q_{\phi}(z_T|z_{T-1})] = - KL(q_{\phi}(z_T|z_{T-1}) || p_{\theta}(z_T))$$</li></ul><hr><p><img alt="Slide 58" loading=lazy src=/images/posts/diffusion/slide-58.jpg></p><h3 id=slide-58-identifying-terms-5>Slide 58: Identifying Terms (5)<a hidden class=anchor aria-hidden=true href=#slide-58-identifying-terms-5>#</a></h3><ul><li>Only the $z_1$ term and reconstruction term remain.</li></ul><hr><p><img alt="Slide 59" loading=lazy src=/images/posts/diffusion/slide-59.jpg></p><h3 id=slide-59-identifying-terms-6>Slide 59: Identifying Terms (6)<a hidden class=anchor aria-hidden=true href=#slide-59-identifying-terms-6>#</a></h3><ul><li>The $z_1$ term:
$$E [\ln p_{\theta}(z_1|z_2) - \ln q_{\phi}(z_1|x)] = - KL(q_{\phi}(z_1|x) || p_{\theta}(z_1|z_2))$$</li></ul><hr><p><img alt="Slide 60" loading=lazy src=/images/posts/diffusion/slide-60.jpg></p><h3 id=slide-60-final-grouping>Slide 60: Final Grouping<a hidden class=anchor aria-hidden=true href=#slide-60-final-grouping>#</a></h3><ul><li>All terms are now converted to Expectations or KL divergences.</li></ul><hr><p><img alt="Slide 61" loading=lazy src=/images/posts/diffusion/slide-61.jpg></p><h3 id=slide-61-the-elbo-equation>Slide 61: The ELBO Equation<a hidden class=anchor aria-hidden=true href=#slide-61-the-elbo-equation>#</a></h3><ul><li><strong>Evidence Lower Bound (ELBO)</strong>:</li></ul><div class=tex2jax_process>$$
\begin{aligned}
L_{ELBO} &= \mathbb{E}_{Q_{\phi}} [\ln p_{\theta}(x|z_1)] \\
&\quad - \sum_{i=2}^{T-1} KL\big(q_{\phi}(z_i \mid z_{i-1}) \,\|\, p_{\theta}(z_i \mid z_{i+1})\big) \\
&\quad - KL\big(q_{\phi}(z_T \mid z_{T-1}) \,\|\, p_{\theta}(z_T)\big) \\
&\quad - KL\big(q_{\phi}(z_1 \mid x) \,\|\, p_{\theta}(z_1 \mid z_2)\big)
\end{aligned}
$$</div><hr><p><img alt="Slide 62" loading=lazy src=/images/posts/diffusion/slide-62.jpg></p><h3 id=slide-62-usefulness-of-elbo>Slide 62: Usefulness of ELBO<a hidden class=anchor aria-hidden=true href=#slide-62-usefulness-of-elbo>#</a></h3><ul><li>This gives us a <strong>computable lower bound</strong> on the true log-likelihood.</li><li>We maximize this ELBO to learn the parameters.</li></ul><hr><p><img alt="Slide 63" loading=lazy src=/images/posts/diffusion/slide-63.jpg></p><h3 id=slide-63-the-reconstruction-term>Slide 63: The Reconstruction Term<a hidden class=anchor aria-hidden=true href=#slide-63-the-reconstruction-term>#</a></h3><ul><li>Focus on the first term: $\mathbb{E}<em>{Q</em>{\phi}} [\ln p_{\theta}(x|z_1)]$</li><li>What distribution should we use for $p_{\theta}(x|z_1)$?</li></ul><hr><p><img alt="Slide 64" loading=lazy src=/images/posts/diffusion/slide-64.jpg></p><h3 id=slide-64-gaussian-reconstruction>Slide 64: Gaussian Reconstruction<a hidden class=anchor aria-hidden=true href=#slide-64-gaussian-reconstruction>#</a></h3><ul><li>Assume Gaussian distribution: $p(x|z_1) = \mathcal{N}(x | \tanh(NN(z_1)), I)$</li><li>The log-likelihood of a Gaussian is proportional to the negative Mean Squared Error (MSE).</li><li>$\ln p(x|z_1) = -MSE(x, \tanh(NN(z_1))) + const$</li></ul><hr><p><img alt="Slide 65" loading=lazy src=/images/posts/diffusion/slide-65.jpg></p><h3 id=slide-65-final-elbo-for-implementation>Slide 65: Final ELBO for Implementation<a hidden class=anchor aria-hidden=true href=#slide-65-final-elbo-for-implementation>#</a></h3><ul><li><strong>Result</strong>:
$$L_{ELBO} = E[-MSE] - \sum KL - KL - KL$$</li><li>Now we can implement this in code since it consists of MSE and KL divergence terms, which are differentiable.</li></ul><hr><p><img alt="Slide 66" loading=lazy src=/images/posts/diffusion/slide-66.jpg></p><h3 id=slide-66-revisiting-the-vae-comparison>Slide 66: Revisiting the VAE Comparison<a hidden class=anchor aria-hidden=true href=#slide-66-revisiting-the-vae-comparison>#</a></h3><ul><li>Why did we go through all this math? To check stability and Posterior Collapse again.</li></ul><hr><p><img alt="Slide 67" loading=lazy src=/images/posts/diffusion/slide-67.jpg></p><h3 id=slide-67-posterior-collapse-in-ddgms>Slide 67: Posterior Collapse in DDGMs?<a hidden class=anchor aria-hidden=true href=#slide-67-posterior-collapse-in-ddgms>#</a></h3><ul><li><strong>VAE</strong>: Collapsed if $q(z|x) \approx \mathcal{N}(0,1)$.</li><li><strong>DDGM</strong>: The goal <em>is</em> to make $z$ become Gaussian noise!</li><li>So &ldquo;collapse&rdquo; in the VAE sense is actually the <strong>objective</strong> of the forward diffusion process.</li></ul><hr><p><img alt="Slide 68" loading=lazy src=/images/posts/diffusion/slide-68.jpg></p><h3 id=slide-68-ddpm-paper>Slide 68: DDPM Paper<a hidden class=anchor aria-hidden=true href=#slide-68-ddpm-paper>#</a></h3><ul><li><strong>Paper</strong>: <em>Denoising Diffusion Probabilistic Models</em> (NeurIPS 2020).</li><li><strong>Authors</strong>: Jonathan Ho, Ajay Jain, Pieter Abbeel.</li><li>This paper simplified the training of diffusion models significantly.</li></ul><hr><p><img alt="Slide 69" loading=lazy src=/images/posts/diffusion/slide-69.jpg></p><h3 id=slide-69-original-model-recap>Slide 69: Original Model Recap<a hidden class=anchor aria-hidden=true href=#slide-69-original-model-recap>#</a></h3><ul><li>Bottom-up (Forward): Gaussian noise steps.</li><li>Top-down (Reverse): Learned denoising.</li></ul><hr><p><img alt="Slide 70" loading=lazy src=/images/posts/diffusion/slide-70.jpg></p><h3 id=slide-70-efficiency-problem>Slide 70: Efficiency Problem<a hidden class=anchor aria-hidden=true href=#slide-70-efficiency-problem>#</a></h3><ul><li>Do we have to sample iteratively $i=1&mldr;T$ just to get the forward noise $z_T$?</li><li>$z_i = \sqrt{1-\beta_i}z_{i-1} + \sqrt{\beta_i}\epsilon$</li></ul><hr><p><img alt="Slide 71" loading=lazy src=/images/posts/diffusion/slide-71.jpg></p><h3 id=slide-71-cumulation-trick-1>Slide 71: Cumulation Trick (1)<a hidden class=anchor aria-hidden=true href=#slide-71-cumulation-trick-1>#</a></h3><ul><li>We can skip steps.</li><li>$z_2 = \sqrt{\alpha_2}z_1 + \sqrt{1-\alpha_2}\epsilon_2$</li><li>Substitute $z_1$:
$z_2 = \sqrt{\alpha_2}(\sqrt{\alpha_1}x + \sqrt{1-\alpha_1}\epsilon_1) + \sqrt{1-\alpha_2}\epsilon_2$
(Where $\alpha_i = 1 - \beta_i$)</li></ul><hr><p><img alt="Slide 72" loading=lazy src=/images/posts/diffusion/slide-72.jpg></p><h3 id=slide-72-cumulation-trick-2>Slide 72: Cumulation Trick (2)<a hidden class=anchor aria-hidden=true href=#slide-72-cumulation-trick-2>#</a></h3><ul><li>Combining Gaussians:
$z_t = \sqrt{\alpha_t}z_{t-1} + \sqrt{1-\alpha_t}\epsilon_t$</li><li>By induction, we can express $z_t$ directly from $x$.</li></ul><hr><p><img alt="Slide 73" loading=lazy src=/images/posts/diffusion/slide-73.jpg></p><h3 id=slide-73-cumulation-trick-3>Slide 73: Cumulation Trick (3)<a hidden class=anchor aria-hidden=true href=#slide-73-cumulation-trick-3>#</a></h3><ul><li>Result:
$$z_t = \sqrt{\bar{\alpha}_t}x + \sqrt{1-\bar{\alpha}_t}\epsilon$$</li><li>Where $\bar{\alpha}<em>t = \prod</em>{s=1}^t \alpha_s$.</li><li>Mean: $\sqrt{\bar{\alpha}_t}x$</li><li>Variance: $(1-\bar{\alpha}_t)I$</li></ul><hr><p><img alt="Slide 74" loading=lazy src=/images/posts/diffusion/slide-74.jpg></p><h3 id=slide-74-closed-form-forward-step>Slide 74: Closed Form Forward Step<a hidden class=anchor aria-hidden=true href=#slide-74-closed-form-forward-step>#</a></h3><ul><li>We can sample $z_t$ at any timestep $t$ in one go:
$$q(z_t|x) = \mathcal{N}(z_t | \sqrt{\bar{\alpha}_t}x, (1-\bar{\alpha}_t)I)$$</li></ul><hr><p><img alt="Slide 75" loading=lazy src=/images/posts/diffusion/slide-75.jpg></p><h3 id=slide-75-new-model-formulation>Slide 75: New Model Formulation<a hidden class=anchor aria-hidden=true href=#slide-75-new-model-formulation>#</a></h3><ul><li><strong>Forward</strong>: Directly sample $z_t$ using the cumulation trick.</li><li><strong>Reverse</strong>: Same learned process.</li></ul><hr><p><img alt="Slide 76" loading=lazy src=/images/posts/diffusion/slide-76.jpg></p><h3 id=slide-76-ddpm-learning-parameters>Slide 76: DDPM Learning Parameters<a hidden class=anchor aria-hidden=true href=#slide-76-ddpm-learning-parameters>#</a></h3><ul><li>We revisit the loss function derivation with this new trick in mind.</li><li>Objective: Maximize $\ln p_{\theta}(x)$.</li></ul><hr><p><img alt="Slide 77" loading=lazy src=/images/posts/diffusion/slide-77.jpg></p><h3 id=slide-77-deriving-ddpm-loss-1>Slide 77: Deriving DDPM Loss (1)<a hidden class=anchor aria-hidden=true href=#slide-77-deriving-ddpm-loss-1>#</a></h3><ul><li>Using the same variational bound logic:
$$= E_Q [\ln \frac{p(z_T) \prod p(z_{t-1}|z_t) p(x|z_1)}{Q(z_T|x) \prod Q(z_{t-1}|z_t, x)}]$$</li></ul><hr><p><img alt="Slide 78" loading=lazy src=/images/posts/diffusion/slide-78.jpg></p><h3 id=slide-78-deriving-ddpm-loss-2>Slide 78: Deriving DDPM Loss (2)<a hidden class=anchor aria-hidden=true href=#slide-78-deriving-ddpm-loss-2>#</a></h3><ul><li>Grouping terms into KL divergences again.</li><li>Terminology:<ul><li>$L_T$: KL between $Q(z_T|x)$ and $p(z_T)$ (Prior matching).</li><li>$L_{t-1}$: KL between $Q(z_{t-1}|z_t, x)$ and $p_{\theta}(z_{t-1}|z_t)$ (Denoising matching).</li><li>$L_0$: Reconstruction log likelihood.</li></ul></li></ul><hr><p><img alt="Slide 79" loading=lazy src=/images/posts/diffusion/slide-79.jpg></p><h3 id=slide-79-loss-terms-definition>Slide 79: Loss Terms Definition<a hidden class=anchor aria-hidden=true href=#slide-79-loss-terms-definition>#</a></h3><ul><li>$L_T = D_{KL}(Q_{\phi}(z_T|x) || p(z_T))$</li><li>$L_{t-1} = D_{KL}(Q_{\phi}(z_{t-1}|z_t, x) || p_{\theta}(z_{t-1}|z_t))$</li><li>$L_0 = -E_{Q}[ \ln p_{\theta}(x|z_1) ]$</li><li><strong>Total Loss</strong>: Minimize $\sum L$ terms.</li></ul><hr><p><img alt="Slide 80" loading=lazy src=/images/posts/diffusion/slide-80.jpg></p><h3 id=slide-80-the-posterior-qz_t-1z_t-x_0>Slide 80: The Posterior $q(z_{t-1}|z_t, x_0)$<a hidden class=anchor aria-hidden=true href=#slide-80-the-posterior-qz_t-1z_t-x_0>#</a></h3><ul><li>This term $Q_{\phi}(z_{t-1}|z_t, x)$ is tractable!</li><li>It is a Gaussian distribution $\mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t I)$.</li><li><strong>Mean $\tilde{\mu}_t$</strong>: A weighted combination of $x_0$ and $z_t$.</li><li><strong>Variance $\tilde{\beta}_t$</strong>: Function of $\beta$ and $\bar{\alpha}$.</li></ul><hr><p><img alt="Slide 81" loading=lazy src=/images/posts/diffusion/slide-81.jpg></p><h3 id=slide-81-posterior-derivation>Slide 81: Posterior Derivation<a hidden class=anchor aria-hidden=true href=#slide-81-posterior-derivation>#</a></h3><ul><li>Proof involves Gaussian conditioning formulas.</li><li>Combining the forward equations allows us to solve for the distribution of $z_{t-1}$ given $z_t$ and $x_0$.</li></ul><hr><p><img alt="Slide 82" loading=lazy src=/images/posts/diffusion/slide-82.jpg></p><h3 id=slide-82-loss-summation>Slide 82: Loss Summation<a hidden class=anchor aria-hidden=true href=#slide-82-loss-summation>#</a></h3><ul><li>We sum the expectations of the loss terms.</li></ul><hr><p><img alt="Slide 83" loading=lazy src=/images/posts/diffusion/slide-83.jpg></p><h3 id=slide-83-expectation-simplification>Slide 83: Expectation Simplification<a hidden class=anchor aria-hidden=true href=#slide-83-expectation-simplification>#</a></h3><ul><li>$L_T$ relies on $z_T$.</li><li>$L_{t-1}$ relies on $z_t, z_{t-1}$.</li><li>By Markov chain property, we calculate expectations appropriately.</li></ul><hr><p><img alt="Slide 84" loading=lazy src=/images/posts/diffusion/slide-84.jpg></p><h3 id=slide-84-minimization-objective>Slide 84: Minimization Objective<a hidden class=anchor aria-hidden=true href=#slide-84-minimization-objective>#</a></h3><ul><li>Minimize $\sum_{t=1}^T \mathbb{E}_{q}[L_t]$.</li></ul><hr><p><img alt="Slide 85" loading=lazy src=/images/posts/diffusion/slide-85.jpg></p><h3 id=slide-85-randomized-time-sampling>Slide 85: Randomized Time Sampling<a hidden class=anchor aria-hidden=true href=#slide-85-randomized-time-sampling>#</a></h3><ul><li>Instead of summing all $T$ terms every step, we can sample $t \sim Uniform(1, \dots, T)$.</li><li>Minimize $E_{t} [E_{q}[L_t]]$.</li></ul><hr><p><img alt="Slide 86" loading=lazy src=/images/posts/diffusion/slide-86.jpg></p><h3 id=slide-86-gradient-descent>Slide 86: Gradient Descent<a hidden class=anchor aria-hidden=true href=#slide-86-gradient-descent>#</a></h3><ul><li>We can take the gradient of this expected loss.</li></ul><hr><p><img alt="Slide 87" loading=lazy src=/images/posts/diffusion/slide-87.jpg></p><h3 id=slide-87-efficient-training>Slide 87: Efficient Training<a hidden class=anchor aria-hidden=true href=#slide-87-efficient-training>#</a></h3><ul><li><strong>OOM</strong>: Calculating all $T$ steps is memory intensive.</li><li><strong>Solution</strong>: Only calculate one random $t$ per optimization step.</li></ul><hr><p><img alt="Slide 88" loading=lazy src=/images/posts/diffusion/slide-88.jpg></p><h3 id=slide-88-analyzing-the-loss>Slide 88: Analyzing the Loss<a hidden class=anchor aria-hidden=true href=#slide-88-analyzing-the-loss>#</a></h3><ul><li>Let&rsquo;s look at the full ELBO equation again.</li></ul><hr><p><img alt="Slide 89" loading=lazy src=/images/posts/diffusion/slide-89.jpg></p><h3 id=slide-89-positivity-of-kl>Slide 89: Positivity of KL<a hidden class=anchor aria-hidden=true href=#slide-89-positivity-of-kl>#</a></h3><ul><li>KL divergences are non-negative.</li><li>Since our models are not perfect approximations, these KL terms will be strictly positive.</li></ul><hr><p><img alt="Slide 90" loading=lazy src=/images/posts/diffusion/slide-90.jpg></p><h3 id=slide-90-accumulation-of-error>Slide 90: Accumulation of Error<a hidden class=anchor aria-hidden=true href=#slide-90-accumulation-of-error>#</a></h3><ul><li>As $T$ grows (many timesteps), there are many KL terms.</li><li>This makes the ELBO value very small (large negative).</li></ul><hr><p><img alt="Slide 91" loading=lazy src=/images/posts/diffusion/slide-91.jpg></p><h3 id=slide-91-unstable-learning>Slide 91: Unstable Learning<a hidden class=anchor aria-hidden=true href=#slide-91-unstable-learning>#</a></h3><ul><li>Summing many KL terms can lead to variance issues and unstable learning.</li></ul><hr><p><img alt="Slide 92" loading=lazy src=/images/posts/diffusion/slide-92.jpg></p><h3 id=slide-92-changing-the-goal>Slide 92: Changing the Goal<a hidden class=anchor aria-hidden=true href=#slide-92-changing-the-goal>#</a></h3><ul><li>We focus on the specific term: $L_{t-1} = D_{KL}(Q(z_{t-1}|z_t, x) || p_{\theta}(z_{t-1}|z_t))$.</li></ul><hr><p><img alt="Slide 93" loading=lazy src=/images/posts/diffusion/slide-93.jpg></p><h3 id=slide-93-matching-distributions>Slide 93: Matching Distributions<a hidden class=anchor aria-hidden=true href=#slide-93-matching-distributions>#</a></h3><ul><li>Our goal is to make the learned reverse process $p_{\theta}(z_{t-1}|z_t)$ approximate the true posterior $q_{\phi}(z_{t-1}|z_t, x)$.</li></ul><hr><p><img alt="Slide 94" loading=lazy src=/images/posts/diffusion/slide-94.jpg></p><h3 id=slide-94-gaussian-matching>Slide 94: Gaussian Matching<a hidden class=anchor aria-hidden=true href=#slide-94-gaussian-matching>#</a></h3><ul><li>Both distributions are Gaussian.</li><li>$q \sim \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t I)$</li><li>$p_{\theta} \sim \mathcal{N}(\mu_{\theta}, \sigma_t^2 I)$ (Usually $\sigma_t^2$ is fixed to $\beta_t$ or $\tilde{\beta}_t$).</li></ul><hr><p><img alt="Slide 95" loading=lazy src=/images/posts/diffusion/slide-95.jpg></p><h3 id=slide-95-kl-between-gaussians>Slide 95: KL between Gaussians<a hidden class=anchor aria-hidden=true href=#slide-95-kl-between-gaussians>#</a></h3><ul><li>The KL divergence between two Gaussians with diagonal covariance is proportional to the <strong>MSE of their means</strong>.</li><li>Minimize:</li></ul><div class=tex2jax_process>$$
\frac{1}{2\sigma_t^2} \left\lVert \tilde{\mu}_t - \mu_{\theta} \right\rVert^2
$$</div><hr><p><img alt="Slide 96" loading=lazy src=/images/posts/diffusion/slide-96.jpg></p><h3 id=slide-96-parametrization-of-mu_theta>Slide 96: Parametrization of $\mu_{\theta}$<a hidden class=anchor aria-hidden=true href=#slide-96-parametrization-of-mu_theta>#</a></h3><ul><li>We know $\tilde{\mu}_t(z_t, x)$ can be written in terms of $z_t$ and the noise $\epsilon$ used to generate it.</li><li>$\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}} (z_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon)$</li><li>So, we should parameterize $\mu_{\theta}$ to predict $\epsilon$!</li></ul><div class=tex2jax_process>$$
\mu_{\theta} = \frac{1}{\sqrt{\alpha_t}} \left(z_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(z_t, t)\right)
$$</div><hr><p><img alt="Slide 97" loading=lazy src=/images/posts/diffusion/slide-97.jpg></p><h3 id=slide-97-derivation-step-1>Slide 97: Derivation Step 1<a hidden class=anchor aria-hidden=true href=#slide-97-derivation-step-1>#</a></h3><ul><li>Recall the definition of $\tilde{\mu}_t$ (weighted average of $x_0$ and $z_t$).</li></ul><hr><p><img alt="Slide 98" loading=lazy src=/images/posts/diffusion/slide-98.jpg></p><h3 id=slide-98-derivation-step-2>Slide 98: Derivation Step 2<a hidden class=anchor aria-hidden=true href=#slide-98-derivation-step-2>#</a></h3><ul><li>Substitute $x_0$ with its expression derived from $z_t$ and $\epsilon$.</li><li>$x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(z_t - \sqrt{1-\bar{\alpha}_t}\epsilon)$</li></ul><hr><p><img alt="Slide 99" loading=lazy src=/images/posts/diffusion/slide-99.jpg></p><h3 id=slide-99-derivation-step-3>Slide 99: Derivation Step 3<a hidden class=anchor aria-hidden=true href=#slide-99-derivation-step-3>#</a></h3><ul><li>Simplify the algebra to get the expression for $\tilde{\mu}_t$ solely in terms of $z_t$ and $\epsilon$.</li></ul><hr><p><img alt="Slide 100" loading=lazy src=/images/posts/diffusion/slide-100.jpg></p><h3 id=slide-100-matching-means>Slide 100: Matching Means<a hidden class=anchor aria-hidden=true href=#slide-100-matching-means>#</a></h3><div class=tex2jax_process>$$
\text{Since }\tilde{\mu}_t\text{ depends on }\epsilon\text{, our model }\mu_{\theta}\text{ should approximate it by using a neural network }\epsilon_{\theta}(z_t, t)\text{ to predict that noise.}
$$</div><hr><p><img alt="Slide 101" loading=lazy src=/images/posts/diffusion/slide-101.jpg></p><h3 id=slide-101-comparison>Slide 101: Comparison<a hidden class=anchor aria-hidden=true href=#slide-101-comparison>#</a></h3><div class=tex2jax_process>$$
\text{Comparison of the target mean }\tilde{\mu}_t\text{ and the model mean }\mu_{\theta}\text{.}
$$</div><ul><li>They are identical in form, except one uses true noise $\epsilon$ and the other uses predicted noise $\epsilon_{\theta}$.</li></ul><hr><p><img alt="Slide 102" loading=lazy src=/images/posts/diffusion/slide-102.jpg></p><h3 id=slide-102-difference-of-means>Slide 102: Difference of Means<a hidden class=anchor aria-hidden=true href=#slide-102-difference-of-means>#</a></h3><div class=tex2jax_process>$$
\text{Calculating }\tilde{\mu}_t - \mu_{\theta}\text{.}
$$</div><ul><li>Everything cancels out except the noise terms.</li><li>Difference $\propto (\epsilon - \epsilon_{\theta})$.</li></ul><hr><p><img alt="Slide 103" loading=lazy src=/images/posts/diffusion/slide-103.jpg></p><h3 id=slide-103-simplified-loss-function>Slide 103: Simplified Loss Function<a hidden class=anchor aria-hidden=true href=#slide-103-simplified-loss-function>#</a></h3><ul><li>The loss becomes weighted MSE between true noise and predicted noise.</li></ul><div class=tex2jax_process>$$
L_{simple} = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\lVert \epsilon - \epsilon_{\theta}\big(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon, t\big) \right\rVert^2 \right]
$$</div><ul><li>$\lambda_t$ is a weighting term derived from the variances.</li></ul><hr><p><img alt="Slide 104" loading=lazy src=/images/posts/diffusion/slide-104.jpg></p><h3 id=slide-104-final-simplification>Slide 104: Final Simplification<a hidden class=anchor aria-hidden=true href=#slide-104-final-simplification>#</a></h3><ul><li><strong>Ho et al. (DDPM)</strong> found that ignoring the weighting term $\lambda_t$ (setting it to 1) works better in practice.</li><li>It puts more weight on difficult aspects of the denoising task.</li></ul><hr><p><img alt="Slide 105" loading=lazy src=/images/posts/diffusion/slide-105.jpg></p><h3 id=slide-105-the-simple-loss>Slide 105: The &ldquo;Simple&rdquo; Loss<a hidden class=anchor aria-hidden=true href=#slide-105-the-simple-loss>#</a></h3><div class=tex2jax_process>$$
L_{simple} = \mathbb{E}_{t, x_0, \epsilon} \left[ \left\lVert \epsilon - \epsilon_{\theta}\big(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t\big) \right\rVert^2 \right]
$$</div><ul><li>We simply train a network to predict the noise added to the image.</li></ul><hr><p><img alt="Slide 106" loading=lazy src=/images/posts/diffusion/slide-106.jpg></p><h3 id=slide-106-algorithms>Slide 106: Algorithms<a hidden class=anchor aria-hidden=true href=#slide-106-algorithms>#</a></h3><ul><li><strong>Algorithm 1 (Training)</strong>:<ol><li>Sample $x_0$.</li><li>Sample $t$.</li><li>Sample noise $\epsilon$.</li><li>Gradient descent on $|| \epsilon - \epsilon_{\theta}(\text{noisy input}, t) ||^2$.</li></ol></li><li><strong>Algorithm 2 (Sampling)</strong>:<ol><li>Start from $x_T \sim \mathcal{N}(0, I)$.</li><li>Iteratively denoise: $x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \dots \epsilon_{\theta}(x_t, t)) + \sigma_t z$.</li></ol></li></ul><hr><p><img alt="Slide 107" loading=lazy src=/images/posts/diffusion/slide-107.jpg></p><h3 id=slide-107-comparison-of-training-methods>Slide 107: Comparison of Training Methods<a hidden class=anchor aria-hidden=true href=#slide-107-comparison-of-training-methods>#</a></h3><ul><li><strong>(A) Full ELBO (Sohl-Dickstein)</strong>: Requires $T$-step gradients, large memory, slow convergence.</li><li><strong>(B) Simplified Noise-Prediction (Ho et al.)</strong>: One-step gradient (random $t$), no KL accumulation, efficient, stable training.</li></ul><hr><p><img alt="Slide 108" loading=lazy src=/images/posts/diffusion/slide-108.jpg></p><h3 id=slide-108-conclusion>Slide 108: Conclusion<a hidden class=anchor aria-hidden=true href=#slide-108-conclusion>#</a></h3><ul><li><strong>Thank You</strong>.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/generative-models/>Generative Models</a></li><li><a href=https://mookjsi.github.io/tags/diffusion-models/>Diffusion Models</a></li><li><a href=https://mookjsi.github.io/tags/vae/>VAE</a></li><li><a href=https://mookjsi.github.io/tags/ddpm/>DDPM</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/math/>Math</a></li></ul><nav class=paginav><a class=next href=https://mookjsi.github.io/posts/paper-review-flamingo/><span class=title>Next »</span><br><span>Flamingo: a Visual Language Model for Few-Shot Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on x" href="https://x.com/intent/tweet/?text=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f&amp;hashtags=GenerativeModels%2cDiffusionModels%2cVAE%2cDDPM%2cDeepLearning%2cMath"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f&amp;title=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation&amp;summary=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f&title=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on whatsapp" href="https://api.whatsapp.com/send?text=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on telegram" href="https://telegram.me/share/url?text=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Diffusion Models: From VAEs to DDPM Derivation on ycombinator" href="https://news.ycombinator.com/submitlink?t=Diffusion%20Models%3a%20From%20VAEs%20to%20DDPM%20Derivation&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fdiffusion%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>