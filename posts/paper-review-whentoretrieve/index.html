<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>When to Retrieve? | MookStudy</title><meta name=keywords content="Paper Review,Retrieval-Augmented Generation,Adaptive Retrieval,ACL 2024"><meta name=description content="This post reviews &lsquo;When to Retrieve?&rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community&rsquo;s feedback, and the reasons it was not accepted."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-whentoretrieve/><link crossorigin=anonymous href=/assets/css/stylesheet.11ee013dd5a386759d3b4c965ae95ae1ca0f4ee553d0b1703ffeb46d15507aee.css integrity="sha256-Ee4BPdWjhnWdO0yWWula4coPTuVT0LFwP/60bRVQeu4=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-whentoretrieve/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-whentoretrieve/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="When to Retrieve?"><meta property="og:description" content="This post reviews ‘When to Retrieve?’, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community’s feedback, and the reasons it was not accepted."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-28T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Retrieval-Augmented Generation"><meta property="article:tag" content="Adaptive Retrieval"><meta property="article:tag" content="ACL 2024"><meta name=twitter:card content="summary"><meta name=twitter:title content="When to Retrieve?"><meta name=twitter:description content="This post reviews &lsquo;When to Retrieve?&rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community&rsquo;s feedback, and the reasons it was not accepted."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"When to Retrieve?","item":"https://mookjsi.github.io/posts/paper-review-whentoretrieve/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"When to Retrieve?","name":"When to Retrieve?","description":"This post reviews \u0026lsquo;When to Retrieve?\u0026rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community\u0026rsquo;s feedback, and the reasons it was not accepted.","keywords":["Paper Review","Retrieval-Augmented Generation","Adaptive Retrieval","ACL 2024"],"articleBody":"This is a detailed slide-by-slide review of the paper When to Retrieve?, which was submitted to ACL 2024 but was ultimately rejected.\nThe paper questions the efficiency of the standard Retrieval-Augmented Generation (RAG) framework, which naively performs retrieval for every query. The authors propose an adaptive retrieval model (RET indicator) that dynamically decides, for each query, whether external knowledge retrieval is necessary. If the LLM’s parametric memory is sufficient, retrieval is skipped; otherwise, external information is fetched.\nThis is the title slide for my presentation on the paper “When to Retrieve?”, which I delivered at the Yonsei University Machine Learning Lab on February 28, 2025.\nTo set the stage, I’m introducing a real-world application I developed, “momugo,” a restaurant recommendation app. This slide shows the user input screen, where a user is asking for a recommendation for a quiet place to talk with a friend while enjoying soju and hot fish cake soup.\nThis slide demonstrates the detailed output of the “momugo” app. When a user clicks on a recommended restaurant, it displays more information, including relevant review snippets that match the user’s query, providing a justification for the recommendation.\nHere, I’m breaking down the initial data filtering process in my application. The system first performs a primary filtering based on structured data like category and business hours. Then, it moves to a more detailed secondary filtering based on the specifics of the user’s natural language query.\nThis slide illustrates the core retrieval and generation pipeline. After the initial filtering, the user’s query is used to find the top 10 most similar review embeddings from a vector database. The top 3 restaurants are then selected, and an LLM generates a descriptive recommendation reason for each.\nI’m detailing the “Detailed Filtering” step. An LLM is prompted with a system message to analyze the user’s query and extract specific requirements, such as “corkage available” or “pet-friendly,” into a structured JSON format.\nThis slide provides a high-level overview, separating the entire process into two main phases: Retrieval and Generation. The retrieval part finds the most relevant restaurants, and the generation part creates the user-facing explanation.\nI’m explaining the embedding process for the retrieval phase. The descriptions and latest reviews for each restaurant are converted into a 1536-dimensional vector using OpenAI’s text-embedding-3-small model and stored in a Pinecone vector database.\nThis slide details the retrieval mechanism. We embed all reviews and the user’s query. Using LangChain, we retrieve the top 10 review vectors based on cosine similarity. Finally, we determine the top 3 restaurants by calculating the average vector similarity for each.\nThis slide outlines the generation phase. The top 3 restaurants identified during retrieval, along with their metadata and the original user query, are used to create three separate prompts for the LLM.\nI’m showcasing the prompt engineering aspect for the generation phase. A structured prompt, including a system message and a human message with the query and restaurant context, is fed to the LLM to generate a tailored recommendation reason for each of the top 3 restaurants.\nThis slide concludes the overview of my application’s architecture. It shows how the generated recommendation reasons (from the LLM) and the filtered restaurant data are combined to produce the final, detailed output card shown to the user.\nShifting from my personal project to the main topic, this slide introduces the standard Retrieval-Augmented Generation (RAG) process, breaking it down into the ‘Retrieval’ and ‘Generation’ stages.\nHere, I pose a critical question about the standard RAG framework: Is the naive approach of always retrieving information for every query the most effective or efficient method?\nThis slide frames the core problem investigated in the paper. I’m questioning the fundamental assumption of RAG: “Should we always retrieve, regardless of the query?” This sets the stage for a more nuanced approach.\nI’m introducing the central concept of the paper: an adaptive retrieval model. The key idea is to use an indicator, which the authors call “RET,” to decide whether to retrieve external information or to answer directly from the LLM’s parametric memory.\nNow that we’ve established the need for a decision mechanism, the key research question becomes: “Where to place a RET?” In other words, how does the model learn when it’s appropriate to trigger the retrieval step?\nThis slide presents the core hypothesis of the paper. The authors believe that retrieval is unnecessary for “popular entities” (information likely stored in the LLM’s parameters) but crucial for “non-popular entities.”\nBuilding on the hypothesis, I’m highlighting the practical challenge: How can a model quantitatively identify if an entity is “popular”? The paper proposes a model that can learn this distinction to enhance the efficiency and accuracy of the RAG process.\nThis slide formally introduces the paper I’m reviewing: “When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively” by Labruna, Campos, and Azkune. They name their proposed model “ADAPT-LLM.”\nI’m providing context with related works. This slide contrasts “Closed-book QA,” which relies solely on an LLM’s internal knowledge, with “Open-book QA,” which always uses an external information retrieval system.\nThis slide summarizes key issues in the field and their corresponding solutions. It covers the high cost of retraining models, the lexical gap in keyword-based retrieval, and the latency costs associated with retrieval, positioning an adaptive approach as a solution.\nContinuing with the related works, this slide discusses the problem of tool overuse in models like Toolformer and the unrealistic nature of previous popularity-based retrieval methods. It positions ADAPT-LLM as a solution that provides a robust baseline for comparison.\nHere, I’m outlining the four-step process of the ADAPT-LLM framework: 1) A query is sent to the model. 2) The model decides whether to retrieve or not. 3) If needed, it retrieves information. 4) It generates the final answer.\nThis slide focuses on the critical decision-making step. The central question is how the model determines whether to retrieve information. The paper’s answer is to fine-tune the LLM on a specially curated dataset to learn this behavior.\nI’m illustrating the data preparation process for fine-tuning ADAPT-LLM. The process starts with QA data, which is fed to a base LLM. The model’s answers are classified as correct or incorrect, which then informs the creation of different types of training prompts.\nThis slide provides a concrete example of the data preparation pipeline. Using a QA pair about the capital of South Korea, I show how the query is extracted from the source data.\nContinuing the example, this slide shows the base LLM generating both a correct answer (“Seoul”) and an incorrect answer (“Busan”) to the query, which is a crucial step for creating the fine-tuning dataset.\nThis slide details how the fine-tuning dataset, named DS_adapt, is constructed. Based on the LLM’s correctness, three types of prompts are generated: a direct answer for correct parametric knowledge, a token for incorrect answers, and a context-based prompt for incorrect answers.\nThis slide visualizes the final step of the model creation process. The DS_adapt dataset, with its varied prompt structures, is used to fine-tune a base LLM, resulting in the final ADAPT-LLM.\nI’m presenting the headline results from the paper. The table shows that ADAPT-LLM, trained on both NQ and SQuAD datasets, outperforms both the NEVER RETRIEVE (NR-LLM) and ALWAYS RETRIEVE (AR-LLM) baselines in terms of accuracy on the PopQA test set.\nThis slide outlines the structure of the experiments section of my review. I will cover three key areas: comparison to baselines, the model’s ability to determine when context is needed, and a comparison with the state-of-the-art approach for the PopQA dataset.\nI’m beginning the detailed experimental setup. This slide specifies the training datasets used for the fine-tuning process: Natural Questions (NQ) and Stanford Question Answering Dataset (SQuAD).\nThis slide specifies the base Large Language Model used in the experiments. The authors chose the Llama-2-7B model as the foundation for their fine-tuning.\nHere, I specify the dataset used for evaluating the models at test time. All configurations are evaluated on the PopQA dataset, which is designed to test knowledge of popular entities.\nI’m explaining how the baseline models are created for comparison. The NR-LLM (NEVER RETRIEVE) is fine-tuned only on prompts that require direct, parametric answers.\nSimilarly, this slide explains the creation of the AR-LLM (ALWAYS RETRIEVE) baseline. This model is fine-tuned exclusively on prompts that include retrieved context, forcing it to always rely on external information.\nThis slide recaps the training process for the main model, ADAPT-LLM. It is trained on the comprehensive DS_adapt dataset, which includes a mix of parametric, retrieval-triggering, and context-aware prompts.\nI’m presenting the main results table again, this time to emphasize the direct performance comparison. ADAPT-LLM consistently achieves the highest accuracy, demonstrating the effectiveness of its selective retrieval strategy.\nTo provide more context on the datasets, this slide presents a table comparing the statistics of NQ, SQuAD, and PopQA, including the number of questions and the average length of questions and answers.\nNow, I’m moving to the second part of the experimental analysis: evaluating ADAPT-LLM’s ability to correctly decide when to retrieve. This slide sets up the analysis of the model’s decision-making accuracy.\nThis slide presents a detailed breakdown of ADAPT-LLM’s performance. It analyzes the accuracy of the model in four scenarios: when it correctly decides to retrieve (and is given context), when it incorrectly decides not to retrieve (and isn’t given context), and so on.\nI’m highlighting a key observation from the results table. The accuracy for questions where the model chose to retrieve (Acc. w/ context for (RET)) seems quite low, around 33%. This prompts a deeper investigation.\nThis slide provides an explanation for the previously noted low accuracy. The authors point out that the performance of the underlying Information Retrieval (IR) system itself was a limiting factor.\nTo support the claim about poor IR performance, this slide presents results from Table 4 of the paper. It shows a massive accuracy gap when using the gold standard passages versus passages retrieved by the Contriever system, confirming the IR bottleneck.\nDespite the issues with the IR component, this slide shows that the model’s decision-making process is sound. The histograms show a clear inverse correlation between entity popularity and the usage of the token, confirming the model learned the core hypothesis.\nI’m now moving to the final experimental comparison: ADAPT-LLM versus the previous state-of-the-art method for the PopQA dataset. This slide visually contrasts the two approaches, highlighting differences in training data and thresholding methods.\nThis slide presents the results of the state-of-the-art comparison. While the accuracy is comparable, I’m highlighting the authors’ claims that ADAPT-LLM is a more generalizable and efficient approach due to its lower reliance on the IR system and its independence from dataset-specific features like popularity scores.\nTo begin the conclusion, I’m bringing back the diagram illustrating the creation of the DS_adapt fine-tuning dataset. This serves as a reminder of the core technical contribution of the paper.\nThis slide recaps the fine-tuning process itself, showing how the diverse set of prompts from DS_adapt is used to train the base LLM into the final, adaptive ADAPT-LLM.\nIn my final slide, I’m summarizing the key takeaways from the paper. The experiments demonstrate that ADAPT-LLM is a robust model that successfully learns when to retrieve information, outperforming standard baselines and showing strong potential as a general, efficient approach to adaptive RAG.\n","wordCount":"1880","inLanguage":"en","datePublished":"2025-02-28T00:00:00Z","dateModified":"2025-02-28T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-whentoretrieve/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">When to Retrieve?</h1><div class=post-meta><span title='2025-02-28 00:00:00 +0000 UTC'>February 28, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1880 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>This is a detailed slide-by-slide review of the paper <strong>When to Retrieve?</strong>, which was submitted to ACL 2024 but was ultimately rejected.</p><p>The paper questions the efficiency of the standard Retrieval-Augmented Generation (RAG) framework, which naively performs retrieval for every query. The authors propose an adaptive retrieval model (RET indicator) that dynamically decides, for each query, whether external knowledge retrieval is necessary. If the LLM&rsquo;s parametric memory is sufficient, retrieval is skipped; otherwise, external information is fetched.</p><hr><p><img alt="Slide 1" loading=lazy src=/images/posts/whentoretrieve/slide-01.jpg></p><blockquote><p>This is the title slide for my presentation on the paper &ldquo;When to Retrieve?&rdquo;, which I delivered at the Yonsei University Machine Learning Lab on February 28, 2025.</p></blockquote><p><img alt="Slide 2" loading=lazy src=/images/posts/whentoretrieve/slide-02.jpg></p><blockquote><p>To set the stage, I&rsquo;m introducing a real-world application I developed, &ldquo;momugo,&rdquo; a restaurant recommendation app. This slide shows the user input screen, where a user is asking for a recommendation for a quiet place to talk with a friend while enjoying soju and hot fish cake soup.</p></blockquote><p><img alt="Slide 3" loading=lazy src=/images/posts/whentoretrieve/slide-03.jpg></p><blockquote><p>This slide demonstrates the detailed output of the &ldquo;momugo&rdquo; app. When a user clicks on a recommended restaurant, it displays more information, including relevant review snippets that match the user&rsquo;s query, providing a justification for the recommendation.</p></blockquote><p><img alt="Slide 4" loading=lazy src=/images/posts/whentoretrieve/slide-04.jpg></p><blockquote><p>Here, I&rsquo;m breaking down the initial data filtering process in my application. The system first performs a primary filtering based on structured data like category and business hours. Then, it moves to a more detailed secondary filtering based on the specifics of the user&rsquo;s natural language query.</p></blockquote><p><img alt="Slide 5" loading=lazy src=/images/posts/whentoretrieve/slide-05.jpg></p><blockquote><p>This slide illustrates the core retrieval and generation pipeline. After the initial filtering, the user&rsquo;s query is used to find the top 10 most similar review embeddings from a vector database. The top 3 restaurants are then selected, and an LLM generates a descriptive recommendation reason for each.</p></blockquote><p><img alt="Slide 6" loading=lazy src=/images/posts/whentoretrieve/slide-06.jpg></p><blockquote><p>I&rsquo;m detailing the &ldquo;Detailed Filtering&rdquo; step. An LLM is prompted with a system message to analyze the user&rsquo;s query and extract specific requirements, such as &ldquo;corkage available&rdquo; or &ldquo;pet-friendly,&rdquo; into a structured JSON format.</p></blockquote><p><img alt="Slide 7" loading=lazy src=/images/posts/whentoretrieve/slide-07.jpg></p><blockquote><p>This slide provides a high-level overview, separating the entire process into two main phases: Retrieval and Generation. The retrieval part finds the most relevant restaurants, and the generation part creates the user-facing explanation.</p></blockquote><p><img alt="Slide 8" loading=lazy src=/images/posts/whentoretrieve/slide-08.jpg></p><blockquote><p>I&rsquo;m explaining the embedding process for the retrieval phase. The descriptions and latest reviews for each restaurant are converted into a 1536-dimensional vector using OpenAI&rsquo;s <code>text-embedding-3-small</code> model and stored in a Pinecone vector database.</p></blockquote><p><img alt="Slide 9" loading=lazy src=/images/posts/whentoretrieve/slide-09.jpg></p><blockquote><p>This slide details the retrieval mechanism. We embed all reviews and the user&rsquo;s query. Using LangChain, we retrieve the top 10 review vectors based on cosine similarity. Finally, we determine the top 3 restaurants by calculating the average vector similarity for each.</p></blockquote><p><img alt="Slide 10" loading=lazy src=/images/posts/whentoretrieve/slide-10.jpg></p><blockquote><p>This slide outlines the generation phase. The top 3 restaurants identified during retrieval, along with their metadata and the original user query, are used to create three separate prompts for the LLM.</p></blockquote><p><img alt="Slide 11" loading=lazy src=/images/posts/whentoretrieve/slide-11.jpg></p><blockquote><p>I&rsquo;m showcasing the prompt engineering aspect for the generation phase. A structured prompt, including a system message and a human message with the query and restaurant context, is fed to the LLM to generate a tailored recommendation reason for each of the top 3 restaurants.</p></blockquote><p><img alt="Slide 12" loading=lazy src=/images/posts/whentoretrieve/slide-12.jpg></p><blockquote><p>This slide concludes the overview of my application&rsquo;s architecture. It shows how the generated recommendation reasons (from the LLM) and the filtered restaurant data are combined to produce the final, detailed output card shown to the user.</p></blockquote><p><img alt="Slide 13" loading=lazy src=/images/posts/whentoretrieve/slide-13.jpg></p><blockquote><p>Shifting from my personal project to the main topic, this slide introduces the standard Retrieval-Augmented Generation (RAG) process, breaking it down into the &lsquo;Retrieval&rsquo; and &lsquo;Generation&rsquo; stages.</p></blockquote><p><img alt="Slide 14" loading=lazy src=/images/posts/whentoretrieve/slide-14.jpg></p><blockquote><p>Here, I pose a critical question about the standard RAG framework: Is the naive approach of always retrieving information for every query the most effective or efficient method?</p></blockquote><p><img alt="Slide 15" loading=lazy src=/images/posts/whentoretrieve/slide-15.jpg></p><blockquote><p>This slide frames the core problem investigated in the paper. I&rsquo;m questioning the fundamental assumption of RAG: &ldquo;Should we always retrieve, regardless of the query?&rdquo; This sets the stage for a more nuanced approach.</p></blockquote><p><img alt="Slide 16" loading=lazy src=/images/posts/whentoretrieve/slide-16.jpg></p><blockquote><p>I&rsquo;m introducing the central concept of the paper: an adaptive retrieval model. The key idea is to use an indicator, which the authors call &ldquo;RET,&rdquo; to decide whether to retrieve external information or to answer directly from the LLM&rsquo;s parametric memory.</p></blockquote><p><img alt="Slide 17" loading=lazy src=/images/posts/whentoretrieve/slide-17.jpg></p><blockquote><p>Now that we&rsquo;ve established the need for a decision mechanism, the key research question becomes: &ldquo;Where to place a RET?&rdquo; In other words, how does the model learn when it&rsquo;s appropriate to trigger the retrieval step?</p></blockquote><p><img alt="Slide 18" loading=lazy src=/images/posts/whentoretrieve/slide-18.jpg></p><blockquote><p>This slide presents the core hypothesis of the paper. The authors believe that retrieval is unnecessary for &ldquo;popular entities&rdquo; (information likely stored in the LLM&rsquo;s parameters) but crucial for &ldquo;non-popular entities.&rdquo;</p></blockquote><p><img alt="Slide 19" loading=lazy src=/images/posts/whentoretrieve/slide-19.jpg></p><blockquote><p>Building on the hypothesis, I&rsquo;m highlighting the practical challenge: How can a model quantitatively identify if an entity is &ldquo;popular&rdquo;? The paper proposes a model that can learn this distinction to enhance the efficiency and accuracy of the RAG process.</p></blockquote><p><img alt="Slide 20" loading=lazy src=/images/posts/whentoretrieve/slide-20.jpg></p><blockquote><p>This slide formally introduces the paper I&rsquo;m reviewing: &ldquo;When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively&rdquo; by Labruna, Campos, and Azkune. They name their proposed model &ldquo;ADAPT-LLM.&rdquo;</p></blockquote><p><img alt="Slide 21" loading=lazy src=/images/posts/whentoretrieve/slide-21.jpg></p><blockquote><p>I&rsquo;m providing context with related works. This slide contrasts &ldquo;Closed-book QA,&rdquo; which relies solely on an LLM&rsquo;s internal knowledge, with &ldquo;Open-book QA,&rdquo; which always uses an external information retrieval system.</p></blockquote><p><img alt="Slide 22" loading=lazy src=/images/posts/whentoretrieve/slide-22.jpg></p><blockquote><p>This slide summarizes key issues in the field and their corresponding solutions. It covers the high cost of retraining models, the lexical gap in keyword-based retrieval, and the latency costs associated with retrieval, positioning an adaptive approach as a solution.</p></blockquote><p><img alt="Slide 23" loading=lazy src=/images/posts/whentoretrieve/slide-23.jpg></p><blockquote><p>Continuing with the related works, this slide discusses the problem of tool overuse in models like Toolformer and the unrealistic nature of previous popularity-based retrieval methods. It positions ADAPT-LLM as a solution that provides a robust baseline for comparison.</p></blockquote><p><img alt="Slide 24" loading=lazy src=/images/posts/whentoretrieve/slide-24.jpg></p><blockquote><p>Here, I&rsquo;m outlining the four-step process of the ADAPT-LLM framework: 1) A query is sent to the model. 2) The model decides whether to retrieve or not. 3) If needed, it retrieves information. 4) It generates the final answer.</p></blockquote><p><img alt="Slide 25" loading=lazy src=/images/posts/whentoretrieve/slide-25.jpg></p><blockquote><p>This slide focuses on the critical decision-making step. The central question is how the model determines whether to retrieve information. The paper&rsquo;s answer is to fine-tune the LLM on a specially curated dataset to learn this behavior.</p></blockquote><p><img alt="Slide 26" loading=lazy src=/images/posts/whentoretrieve/slide-26.jpg></p><blockquote><p>I&rsquo;m illustrating the data preparation process for fine-tuning ADAPT-LLM. The process starts with QA data, which is fed to a base LLM. The model&rsquo;s answers are classified as correct or incorrect, which then informs the creation of different types of training prompts.</p></blockquote><p><img alt="Slide 27" loading=lazy src=/images/posts/whentoretrieve/slide-27.jpg></p><blockquote><p>This slide provides a concrete example of the data preparation pipeline. Using a QA pair about the capital of South Korea, I show how the query is extracted from the source data.</p></blockquote><p><img alt="Slide 28" loading=lazy src=/images/posts/whentoretrieve/slide-28.jpg></p><blockquote><p>Continuing the example, this slide shows the base LLM generating both a correct answer (&ldquo;Seoul&rdquo;) and an incorrect answer (&ldquo;Busan&rdquo;) to the query, which is a crucial step for creating the fine-tuning dataset.</p></blockquote><p><img alt="Slide 29" loading=lazy src=/images/posts/whentoretrieve/slide-29.jpg></p><blockquote><p>This slide details how the fine-tuning dataset, named <code>DS_adapt</code>, is constructed. Based on the LLM&rsquo;s correctness, three types of prompts are generated: a direct answer for correct parametric knowledge, a <code>&lt;RET></code> token for incorrect answers, and a context-based prompt for incorrect answers.</p></blockquote><p><img alt="Slide 30" loading=lazy src=/images/posts/whentoretrieve/slide-30.jpg></p><blockquote><p>This slide visualizes the final step of the model creation process. The <code>DS_adapt</code> dataset, with its varied prompt structures, is used to fine-tune a base LLM, resulting in the final ADAPT-LLM.</p></blockquote><p><img alt="Slide 31" loading=lazy src=/images/posts/whentoretrieve/slide-31.jpg></p><blockquote><p>I&rsquo;m presenting the headline results from the paper. The table shows that ADAPT-LLM, trained on both NQ and SQuAD datasets, outperforms both the NEVER RETRIEVE (NR-LLM) and ALWAYS RETRIEVE (AR-LLM) baselines in terms of accuracy on the PopQA test set.</p></blockquote><p><img alt="Slide 32" loading=lazy src=/images/posts/whentoretrieve/slide-32.jpg></p><blockquote><p>This slide outlines the structure of the experiments section of my review. I will cover three key areas: comparison to baselines, the model&rsquo;s ability to determine when context is needed, and a comparison with the state-of-the-art approach for the PopQA dataset.</p></blockquote><p><img alt="Slide 33" loading=lazy src=/images/posts/whentoretrieve/slide-33.jpg></p><blockquote><p>I&rsquo;m beginning the detailed experimental setup. This slide specifies the training datasets used for the fine-tuning process: Natural Questions (NQ) and Stanford Question Answering Dataset (SQuAD).</p></blockquote><p><img alt="Slide 34" loading=lazy src=/images/posts/whentoretrieve/slide-34.jpg></p><blockquote><p>This slide specifies the base Large Language Model used in the experiments. The authors chose the Llama-2-7B model as the foundation for their fine-tuning.</p></blockquote><p><img alt="Slide 35" loading=lazy src=/images/posts/whentoretrieve/slide-35.jpg></p><blockquote><p>Here, I specify the dataset used for evaluating the models at test time. All configurations are evaluated on the PopQA dataset, which is designed to test knowledge of popular entities.</p></blockquote><p><img alt="Slide 36" loading=lazy src=/images/posts/whentoretrieve/slide-36.jpg></p><blockquote><p>I&rsquo;m explaining how the baseline models are created for comparison. The NR-LLM (NEVER RETRIEVE) is fine-tuned only on prompts that require direct, parametric answers.</p></blockquote><p><img alt="Slide 37" loading=lazy src=/images/posts/whentoretrieve/slide-37.jpg></p><blockquote><p>Similarly, this slide explains the creation of the AR-LLM (ALWAYS RETRIEVE) baseline. This model is fine-tuned exclusively on prompts that include retrieved context, forcing it to always rely on external information.</p></blockquote><p><img alt="Slide 38" loading=lazy src=/images/posts/whentoretrieve/slide-38.jpg></p><blockquote><p>This slide recaps the training process for the main model, ADAPT-LLM. It is trained on the comprehensive <code>DS_adapt</code> dataset, which includes a mix of parametric, retrieval-triggering, and context-aware prompts.</p></blockquote><p><img alt="Slide 39" loading=lazy src=/images/posts/whentoretrieve/slide-39.jpg></p><blockquote><p>I&rsquo;m presenting the main results table again, this time to emphasize the direct performance comparison. ADAPT-LLM consistently achieves the highest accuracy, demonstrating the effectiveness of its selective retrieval strategy.</p></blockquote><p><img alt="Slide 40" loading=lazy src=/images/posts/whentoretrieve/slide-40.jpg></p><blockquote><p>To provide more context on the datasets, this slide presents a table comparing the statistics of NQ, SQuAD, and PopQA, including the number of questions and the average length of questions and answers.</p></blockquote><p><img alt="Slide 41" loading=lazy src=/images/posts/whentoretrieve/slide-41.jpg></p><blockquote><p>Now, I&rsquo;m moving to the second part of the experimental analysis: evaluating ADAPT-LLM&rsquo;s ability to correctly decide when to retrieve. This slide sets up the analysis of the model&rsquo;s decision-making accuracy.</p></blockquote><p><img alt="Slide 42" loading=lazy src=/images/posts/whentoretrieve/slide-42.jpg></p><blockquote><p>This slide presents a detailed breakdown of ADAPT-LLM&rsquo;s performance. It analyzes the accuracy of the model in four scenarios: when it correctly decides to retrieve (and is given context), when it incorrectly decides not to retrieve (and isn&rsquo;t given context), and so on.</p></blockquote><p><img alt="Slide 43" loading=lazy src=/images/posts/whentoretrieve/slide-43.jpg></p><blockquote><p>I&rsquo;m highlighting a key observation from the results table. The accuracy for questions where the model chose to retrieve (<code>Acc. w/ context</code> for <code>(RET)</code>) seems quite low, around 33%. This prompts a deeper investigation.</p></blockquote><p><img alt="Slide 44" loading=lazy src=/images/posts/whentoretrieve/slide-44.jpg></p><blockquote><p>This slide provides an explanation for the previously noted low accuracy. The authors point out that the performance of the underlying Information Retrieval (IR) system itself was a limiting factor.</p></blockquote><p><img alt="Slide 45" loading=lazy src=/images/posts/whentoretrieve/slide-45.jpg></p><blockquote><p>To support the claim about poor IR performance, this slide presents results from Table 4 of the paper. It shows a massive accuracy gap when using the gold standard passages versus passages retrieved by the Contriever system, confirming the IR bottleneck.</p></blockquote><p><img alt="Slide 46" loading=lazy src=/images/posts/whentoretrieve/slide-46.jpg></p><blockquote><p>Despite the issues with the IR component, this slide shows that the model&rsquo;s <em>decision-making process</em> is sound. The histograms show a clear inverse correlation between entity popularity and the usage of the <code>&lt;RET></code> token, confirming the model learned the core hypothesis.</p></blockquote><p><img alt="Slide 47" loading=lazy src=/images/posts/whentoretrieve/slide-47.jpg></p><blockquote><p>I&rsquo;m now moving to the final experimental comparison: ADAPT-LLM versus the previous state-of-the-art method for the PopQA dataset. This slide visually contrasts the two approaches, highlighting differences in training data and thresholding methods.</p></blockquote><p><img alt="Slide 48" loading=lazy src=/images/posts/whentoretrieve/slide-48.jpg></p><blockquote><p>This slide presents the results of the state-of-the-art comparison. While the accuracy is comparable, I&rsquo;m highlighting the authors&rsquo; claims that ADAPT-LLM is a more generalizable and efficient approach due to its lower reliance on the IR system and its independence from dataset-specific features like popularity scores.</p></blockquote><p><img alt="Slide 49" loading=lazy src=/images/posts/whentoretrieve/slide-49.jpg></p><blockquote><p>To begin the conclusion, I&rsquo;m bringing back the diagram illustrating the creation of the <code>DS_adapt</code> fine-tuning dataset. This serves as a reminder of the core technical contribution of the paper.</p></blockquote><p><img alt="Slide 50" loading=lazy src=/images/posts/whentoretrieve/slide-50.jpg></p><blockquote><p>This slide recaps the fine-tuning process itself, showing how the diverse set of prompts from <code>DS_adapt</code> is used to train the base LLM into the final, adaptive ADAPT-LLM.</p></blockquote><p><img alt="Slide 51" loading=lazy src=/images/posts/whentoretrieve/slide-51.jpg></p><blockquote><p>In my final slide, I&rsquo;m summarizing the key takeaways from the paper. The experiments demonstrate that ADAPT-LLM is a robust model that successfully learns when to retrieve information, outperforming standard baselines and showing strong potential as a general, efficient approach to adaptive RAG.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/retrieval-augmented-generation/>Retrieval-Augmented Generation</a></li><li><a href=https://mookjsi.github.io/tags/adaptive-retrieval/>Adaptive Retrieval</a></li><li><a href=https://mookjsi.github.io/tags/acl-2024/>ACL 2024</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/paper-review-stochastic/><span class=title>« Prev</span><br><span>Stochastic Approximation to Contrastive Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on x" href="https://x.com/intent/tweet/?text=When%20to%20Retrieve%3f&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f&amp;hashtags=PaperReview%2cRetrieval-AugmentedGeneration%2cAdaptiveRetrieval%2cACL2024"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f&amp;title=When%20to%20Retrieve%3f&amp;summary=When%20to%20Retrieve%3f&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f&title=When%20to%20Retrieve%3f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on whatsapp" href="https://api.whatsapp.com/send?text=When%20to%20Retrieve%3f%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on telegram" href="https://telegram.me/share/url?text=When%20to%20Retrieve%3f&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share When to Retrieve? on ycombinator" href="https://news.ycombinator.com/submitlink?t=When%20to%20Retrieve%3f&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-whentoretrieve%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>