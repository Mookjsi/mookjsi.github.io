<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Flamingo: a Visual Language Model for Few-Shot Learning | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Multimodal,Few-Shot Learning,Flamingo,Vision-Language Models,DeepMind"><meta name=description content="DeepMind의 Flamingo 논문 리뷰."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-flamingo/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-flamingo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-flamingo/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Flamingo: a Visual Language Model for Few-Shot Learning"><meta property="og:description" content="DeepMind의 Flamingo 논문 리뷰."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-25T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Multimodal"><meta property="article:tag" content="Few-Shot Learning"><meta property="article:tag" content="Flamingo"><meta property="article:tag" content="Vision-Language Models"><meta name=twitter:card content="summary"><meta name=twitter:title content="Flamingo: a Visual Language Model for Few-Shot Learning"><meta name=twitter:description content="DeepMind의 Flamingo 논문 리뷰."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Flamingo: a Visual Language Model for Few-Shot Learning","item":"https://mookjsi.github.io/posts/paper-review-flamingo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Flamingo: a Visual Language Model for Few-Shot Learning","name":"Flamingo: a Visual Language Model for Few-Shot Learning","description":"DeepMind의 Flamingo 논문 리뷰.","keywords":["Paper Review","Deep Learning","Multimodal","Few-Shot Learning","Flamingo","Vision-Language Models","DeepMind"],"articleBody":"개요 엄청난 독서광이라 세상의 모든 책을 읽은 친구(Large Language Model)가 있다고 상상해 보세요. 이 친구는 글은 기가 막히게 잘 쓰지만, 태어나서 한 번도 그림이나 사진을 본 적이 없습니다. 반대로, 그림만 하루 종일 봐서 시각적인 정보는 완벽하게 알지만 말주변이 없는 친구(Vision Encoder)도 있습니다.\nFlamingo는 이 두 친구를 이어주는 ‘마법의 통역사’ 같은 모델입니다.\n그림 잘 보는 친구가 사진을 보고 “이건 털이 복슬복슬한 고양이야\"라고 정보를 압축해서 넘겨주면, Flamingo의 특별한 장치(Perceiver Resampler 등)가 이것을 글 잘 쓰는 친구가 이해할 수 있는 언어로 번역해 줍니다.\n가장 대단한 점은 **‘눈치(Few-Shot Learning)’**가 엄청나게 빠르다는 것입니다. 예전에는 인공지능에게 “이건 고양이고, 이건 강아지야\"라고 수만 번 가르쳐야(Fine-tuning) 겨우 알아들었습니다. 하지만 Flamingo는 마치 똑똑한 고등학생처럼, 시험 문제집에서 예시 문제 한두 개만 쓱 보여주면(Few-Shot), 바로 원리를 깨닫고 처음 보는 문제도 척척 풀어냅니다. 사진과 글이 섞여 있는(Interleaved) 복잡한 블로그 게시물 같은 것도 순서대로 이해하고 대화할 수 있는 능력을 갖췄습니다.\n1. 서론 (Introduction) 이 논문은 비전(Vision)과 언어(Language)를 결합한 멀티모달 모델인 Flamingo를 소개합니다. 기존 모델들은 새로운 작업을 수행하기 위해 수천 개의 데이터로 모델을 미세 조정(Fine-tuning)해야 했습니다. 하지만 Flamingo는 사전 학습된 강력한 Vision Encoder와 Large Language Model(LLM)을 결합하여, 단 몇 개의 예시(Few-Shot)만으로도 이미지 및 비디오 이해 작업에서 최고 성능(SOTA)을 달성합니다.\n이 그림은 Flamingo가 이미지와 텍스트가 섞인 입력을 받아, 동물 분류, 도시 이름 맞추기, 수학 문제 풀이 등 다양한 작업을 수행하는 것을 보여줍니다. 특히 몇 개의 예시만 주어지면(Few-shot prompting) 새로운 태스크에 빠르게 적응하는 것을 확인할 수 있습니다.\n왼쪽은 Flamingo가 별도의 Fine-tuning 없이도, 수천 배의 데이터를 사용해 학습한 기존 모델들(SOTA)보다 16개 중 6개 태스크에서 더 뛰어난 성능을 보임을 나타냅니다. 오른쪽은 모델의 크기(Parameters)와 샷(Shot) 수가 늘어날수록 성능이 향상됨을 보여줍니다.\n2. 기술적 설명 (Technical Description) Flamingo의 핵심은 이미 잘 학습된 Vision Encoder와 LLM을 ‘얼리지(Frozen)’ 않은 상태로 두고, 둘 사이를 연결하는 새로운 레이어만 학습시키는 것입니다. 이를 통해 기존 모델들이 가진 방대한 지식을 잊어버리지 않으면서(Catastrophic Forgetting 방지) 시각 정보를 통합합니다.\n2.1 아키텍처 (Architecture) 입력된 시각 데이터(이미지/비디오)는 Vision Encoder를 거쳐 Perceiver Resampler로 들어갑니다. 여기서 고정된 개수의 Visual Token으로 변환된 후, LLM 층 사이에 삽입된 GATED XATTN-DENSE 레이어를 통해 언어 모델에 시각 정보가 주입됩니다. 파란색 부분은 학습되지 않는 Frozen 상태, 보라색 부분만 학습되는 파라미터입니다.\n2.2 Visual Processing \u0026 Perceiver Resampler Vision Encoder(NFNet)는 시각적 특징을 추출합니다. 하지만 이미지나 비디오의 해상도나 길이에 따라 특징맵의 크기가 달라질 수 있습니다. 이를 해결하기 위해 Perceiver Resampler를 도입했습니다.\n이 모듈은 가변적인 크기의 시각적 특징(Visual features)을 입력받아, 고정된 개수(예: 64개)의 Visual Token으로 변환합니다. 이는 Transformer 기반 구조로, 학습 가능한 Latent Query를 사용하여 시각 정보에서 중요한 부분만 추출합니다. 이를 통해 계산 복잡도를 줄이고 LLM이 시각 정보를 효율적으로 처리하게 합니다.\n2.3 GATED XATTN-DENSE \u0026 Conditioning 추출된 시각 정보는 LLM이 다음 토큰을 예측할 때 활용됩니다. 이를 위해 기존 LLM 레이어 사이에 새로운 Cross-Attention 레이어를 삽입합니다.\n기존 LLM 블록 앞에 Cross-Attention(시각 정보 참조)과 Feed-Forward 레이어를 추가했습니다. 중요한 점은 Tanh Gating 기법을 사용했다는 것입니다.\n초기에 Tanh 게이트 값은 0으로 설정됩니다. 이는 학습 초기에는 시각 정보를 무시하고 오로지 언어 모델의 원래 성능 그대로 작동하게 하여 학습 안정성을 높입니다. 학습이 진행될수록 게이트 값이 커지며 시각 정보를 받아들입니다.\n수식 설명 (Likelihood Modeling):\n이 모델은 인터리브 된(섞인) 이미지와 비디오 $x$가 주어졌을 때 텍스트 $y$의 확률을 다음과 같이 모델링합니다.\n$$ p(y|x) = \\prod_{l=1}^{L} p(y_l \\mid y_{","wordCount":"847","inLanguage":"en","datePublished":"2025-11-25T00:00:00Z","dateModified":"2025-11-25T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-flamingo/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Flamingo: a Visual Language Model for Few-Shot Learning</h1><div class=post-meta><span title='2025-11-25 00:00:00 +0000 UTC'>November 25, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;847 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><h3 id=개요>개요<a hidden class=anchor aria-hidden=true href=#개요>#</a></h3><p>엄청난 독서광이라 세상의 모든 책을 읽은 친구(Large Language Model)가 있다고 상상해 보세요. 이 친구는 글은 기가 막히게 잘 쓰지만, 태어나서 한 번도 그림이나 사진을 본 적이 없습니다. 반대로, 그림만 하루 종일 봐서 시각적인 정보는 완벽하게 알지만 말주변이 없는 친구(Vision Encoder)도 있습니다.</p><p>Flamingo는 이 두 친구를 이어주는 &lsquo;마법의 통역사&rsquo; 같은 모델입니다.</p><p>그림 잘 보는 친구가 사진을 보고 &ldquo;이건 털이 복슬복슬한 고양이야"라고 정보를 압축해서 넘겨주면, Flamingo의 특별한 장치(Perceiver Resampler 등)가 이것을 글 잘 쓰는 친구가 이해할 수 있는 언어로 번역해 줍니다.</p><p>가장 대단한 점은 **&lsquo;눈치(Few-Shot Learning)&rsquo;**가 엄청나게 빠르다는 것입니다.
예전에는 인공지능에게 &ldquo;이건 고양이고, 이건 강아지야"라고 수만 번 가르쳐야(Fine-tuning) 겨우 알아들었습니다. 하지만 Flamingo는 마치 똑똑한 고등학생처럼, 시험 문제집에서 예시 문제 한두 개만 쓱 보여주면(Few-Shot), 바로 원리를 깨닫고 처음 보는 문제도 척척 풀어냅니다. 사진과 글이 섞여 있는(Interleaved) 복잡한 블로그 게시물 같은 것도 순서대로 이해하고 대화할 수 있는 능력을 갖췄습니다.</p><hr><h3 id=1-서론-introduction>1. 서론 (Introduction)<a hidden class=anchor aria-hidden=true href=#1-서론-introduction>#</a></h3><p>이 논문은 비전(Vision)과 언어(Language)를 결합한 멀티모달 모델인 Flamingo를 소개합니다. 기존 모델들은 새로운 작업을 수행하기 위해 수천 개의 데이터로 모델을 미세 조정(Fine-tuning)해야 했습니다. 하지만 Flamingo는 사전 학습된 강력한 Vision Encoder와 Large Language Model(LLM)을 결합하여, 단 몇 개의 예시(Few-Shot)만으로도 이미지 및 비디오 이해 작업에서 최고 성능(SOTA)을 달성합니다.</p><p><img alt="Figure 1: Flamingo-80B 모델의 입력과 출력 예시를 보여줍니다." loading=lazy src=/images/posts/flamingo/figure1.png></p><p>이 그림은 Flamingo가 이미지와 텍스트가 섞인 입력을 받아, 동물 분류, 도시 이름 맞추기, 수학 문제 풀이 등 다양한 작업을 수행하는 것을 보여줍니다. 특히 몇 개의 예시만 주어지면(Few-shot prompting) 새로운 태스크에 빠르게 적응하는 것을 확인할 수 있습니다.</p><p><img alt="Figure 2: Flamingo의 성능 개요 그래프입니다." loading=lazy src=/images/posts/flamingo/figure2.png></p><p>왼쪽은 Flamingo가 별도의 Fine-tuning 없이도, 수천 배의 데이터를 사용해 학습한 기존 모델들(SOTA)보다 16개 중 6개 태스크에서 더 뛰어난 성능을 보임을 나타냅니다. 오른쪽은 모델의 크기(Parameters)와 샷(Shot) 수가 늘어날수록 성능이 향상됨을 보여줍니다.</p><hr><h3 id=2-기술적-설명-technical-description>2. 기술적 설명 (Technical Description)<a hidden class=anchor aria-hidden=true href=#2-기술적-설명-technical-description>#</a></h3><p>Flamingo의 핵심은 이미 잘 학습된 Vision Encoder와 LLM을 &lsquo;얼리지(Frozen)&rsquo; 않은 상태로 두고, 둘 사이를 연결하는 새로운 레이어만 학습시키는 것입니다. 이를 통해 기존 모델들이 가진 방대한 지식을 잊어버리지 않으면서(Catastrophic Forgetting 방지) 시각 정보를 통합합니다.</p><h4 id=21-아키텍처-architecture>2.1 아키텍처 (Architecture)<a hidden class=anchor aria-hidden=true href=#21-아키텍처-architecture>#</a></h4><p><img alt="Figure 3: Flamingo의 전체 아키텍처 개요입니다." loading=lazy src=/images/posts/flamingo/figure3.png></p><p>입력된 시각 데이터(이미지/비디오)는 Vision Encoder를 거쳐 Perceiver Resampler로 들어갑니다. 여기서 고정된 개수의 Visual Token으로 변환된 후, LLM 층 사이에 삽입된 GATED XATTN-DENSE 레이어를 통해 언어 모델에 시각 정보가 주입됩니다. 파란색 부분은 학습되지 않는 Frozen 상태, 보라색 부분만 학습되는 파라미터입니다.</p><h4 id=22-visual-processing--perceiver-resampler>2.2 Visual Processing & Perceiver Resampler<a hidden class=anchor aria-hidden=true href=#22-visual-processing--perceiver-resampler>#</a></h4><p>Vision Encoder(NFNet)는 시각적 특징을 추출합니다. 하지만 이미지나 비디오의 해상도나 길이에 따라 특징맵의 크기가 달라질 수 있습니다. 이를 해결하기 위해 Perceiver Resampler를 도입했습니다.</p><p><img alt="Figure 5: Perceiver Resampler의 구조와 의사 코드(Pseudo-code)입니다." loading=lazy src=/images/posts/flamingo/figure5.png></p><p>이 모듈은 가변적인 크기의 시각적 특징(Visual features)을 입력받아, 고정된 개수(예: 64개)의 Visual Token으로 변환합니다. 이는 Transformer 기반 구조로, 학습 가능한 Latent Query를 사용하여 시각 정보에서 중요한 부분만 추출합니다. 이를 통해 계산 복잡도를 줄이고 LLM이 시각 정보를 효율적으로 처리하게 합니다.</p><h4 id=23-gated-xattn-dense--conditioning>2.3 GATED XATTN-DENSE & Conditioning<a hidden class=anchor aria-hidden=true href=#23-gated-xattn-dense--conditioning>#</a></h4><p>추출된 시각 정보는 LLM이 다음 토큰을 예측할 때 활용됩니다. 이를 위해 기존 LLM 레이어 사이에 새로운 Cross-Attention 레이어를 삽입합니다.</p><p><img alt="Figure 4: GATED XATTN-DENSE 레이어의 구조입니다." loading=lazy src=/images/posts/flamingo/figure4.png></p><p>기존 LLM 블록 앞에 Cross-Attention(시각 정보 참조)과 Feed-Forward 레이어를 추가했습니다. 중요한 점은 Tanh Gating 기법을 사용했다는 것입니다.</p><p><img alt="Figure 6: 학습 진행에 따른 Tanh Gating 값의 변화를 보여줍니다." loading=lazy src=/images/posts/flamingo/figure6.png></p><p>초기에 Tanh 게이트 값은 0으로 설정됩니다. 이는 학습 초기에는 시각 정보를 무시하고 오로지 언어 모델의 원래 성능 그대로 작동하게 하여 학습 안정성을 높입니다. 학습이 진행될수록 게이트 값이 커지며 시각 정보를 받아들입니다.</p><p><strong>수식 설명 (Likelihood Modeling):</strong></p><p>이 모델은 인터리브 된(섞인) 이미지와 비디오 $x$가 주어졌을 때 텍스트 $y$의 확률을 다음과 같이 모델링합니다.</p><p>$$
p(y|x) = \prod_{l=1}^{L} p(y_l \mid y_{&lt;l}, x_{\leq l})
$$</p><p>이 수식은 조건부 확률의 연쇄 법칙(Chain Rule)을 나타냅니다.</p><ul><li>$y_l$: 현재 예측하려는 $l$번째 언어 토큰입니다.</li><li>$y_{&lt;l}$: 이전에 생성된 모든 텍스트 토큰들입니다.</li><li>$x_{\le l}$: 현재 텍스트 토큰 $y_l$보다 시퀀스상 이전에 등장한 이미지/비디오들의 집합입니다.</li></ul><p>의미: Flamingo는 텍스트를 생성할 때, 텍스트의 문맥($y_{&lt;l}$)뿐만 아니라, 현재 시점 이전에 나왔던 모든 시각 정보($x_{\le l}$)를 조건(Condition)으로 하여 다음 단어를 예측합니다. 이는 자기회귀(Autoregressive) 생성 방식입니다.</p><h4 id=24-multi-visual-input--data>2.4 Multi-visual Input & Data<a hidden class=anchor aria-hidden=true href=#24-multi-visual-input--data>#</a></h4><p>Flamingo는 텍스트와 이미지가 섞인 데이터를 처리하기 위해 특별한 마스킹 기법을 사용합니다.</p><p><img alt="Figure 7: 인터리브 된 데이터 처리와 마스킹(Masking) 방식을 보여줍니다." loading=lazy src=/images/posts/flamingo/figure7.png></p><p>모델은 텍스트 토큰을 예측할 때, 해당 텍스트 직전에 나온 이미지에만 Cross-Attention을 수행하도록 마스킹 처리(진한 파란색) 됩니다. 하지만 Self-Attention(텍스트끼리의 관계)을 통해 전체 문맥은 유지됩니다.</p><p><img alt="Figure 8: Few-shot 프롬프트 생성 과정입니다." loading=lazy src=/images/posts/flamingo/figure8.png></p><p>지원 예제(Support examples) 이미지와 텍스트를 나열하고, 마지막에 쿼리(Query) 이미지를 붙여 모델이 답을 생성하게 유도하는 구조를 보여줍니다.</p><p><img alt="Figure 9: 학습 데이터셋의 구성을 보여줍니다." loading=lazy src=/images/posts/flamingo/figure9.png></p><ul><li><strong>M3W:</strong> 웹페이지에서 수집한 이미지-텍스트가 섞인 데이터 (Few-shot 능력의 핵심).</li><li><strong>Image-Text Pairs (ALIGN, LTIP):</strong> 이미지와 캡션 쌍.</li><li><strong>Video-Text Pairs (VTP):</strong> 비디오와 캡션 쌍.</li></ul><hr><h3 id=3-실험-experiments>3. 실험 (Experiments)<a hidden class=anchor aria-hidden=true href=#3-실험-experiments>#</a></h3><p>실험은 Flamingo가 얼마나 적은 데이터로 새로운 태스크에 적응하는지, 그리고 SOTA 모델들과 비교해 어떤 성능을 내는지를 중점적으로 다룹니다.</p><h4 id=31-few-shot-learning-성능>3.1 Few-Shot Learning 성능<a hidden class=anchor aria-hidden=true href=#31-few-shot-learning-성능>#</a></h4><p><img alt="Table 1: 16개 멀티모달 벤치마크에 대한 SOTA 비교 표입니다." loading=lazy src=/images/posts/flamingo/table1.png></p><p>Flamingo는 단 4개의 Shot만으로도 이전의 Zero/Few-shot SOTA를 압도합니다. 특히 6개의 태스크에서는 수천~수십만 개의 데이터로 Fine-tuning 된 모델들보다 32-shot Flamingo가 더 높은 성능을 기록했습니다. 이는 데이터 효율성 측면에서 혁신적인 결과입니다.</p><p><img alt="Table 6: 평가에 사용된 16개 벤치마크(COCO, VQAv2 등)에 대한 요약입니다." loading=lazy src=/images/posts/flamingo/table6.png></p><h4 id=32-fine-tuning--classification>3.2 Fine-tuning & Classification<a hidden class=anchor aria-hidden=true href=#32-fine-tuning--classification>#</a></h4><p><img alt="Table 2: Flamingo를 전체 데이터셋으로 Fine-tuning 했을 때의 결과입니다." loading=lazy src=/images/posts/flamingo/table2.png></p><p>데이터를 충분히 주고 학습시키면 VQAv2, VATEX 등 5개 태스크에서 새로운 SOTA를 달성함을 보여줍니다.</p><p><img alt="Table 7: ImageNet 등 분류(Classification) 태스크에서의 Few-shot 성능입니다." loading=lazy src=/images/posts/flamingo/table7.png></p><p>Contrastive 모델보다는 다소 성능이 떨어지지만, RICES(유사한 예시를 검색해서 프롬프트에 넣는 방법)를 사용하면 성능이 크게 향상됨을 보여줍니다.</p><h4 id=33-ablation-studies-소거-연구>3.3 Ablation Studies (소거 연구)<a hidden class=anchor aria-hidden=true href=#33-ablation-studies-소거-연구>#</a></h4><p><img alt="Table 3: 주요 구성 요소에 대한 Ablation study입니다." loading=lazy src=/images/posts/flamingo/table3.png></p><ul><li>M3W(인터리브 데이터)가 없으면 성능이 17% 하락합니다.</li><li>Tanh gating이 없으면 학습이 불안정하고 성능이 떨어집니다.</li><li>Perceiver Resampler가 MLP나 바닐라 Transformer보다 효율적이고 성능이 좋습니다.</li></ul><p><img alt="Table 10: 추가적인 Ablation study 결과입니다." loading=lazy src=/images/posts/flamingo/table10.png></p><p>Resampler의 크기, Multi-image attention 방식, LM 사전 학습 데이터(C4 vs MassiveText) 등의 영향을 분석합니다.</p><p><img alt="Table 11: Contrastive Pretraining 시 데이터셋 조합 효과입니다." loading=lazy src=/images/posts/flamingo/table11.png></p><p>데이터의 양보다 질(LTIP 데이터셋)이 중요하며, 여러 데이터셋을 합칠 때 단순 병합보다 Gradient Accumulation 방식이 더 효과적임을 보여줍니다.</p><p><img alt="Table 9: Vision Encoder의 Zero-shot 검색 성능 비교입니다." loading=lazy src=/images/posts/flamingo/table9.png></p><p><img alt="Table 12: COCO 캡셔닝에서의 성별 및 피부색 편향(Bias) 평가입니다." loading=lazy src=/images/posts/flamingo/table12.png></p><p>성별이나 피부색에 따른 성능 차이가 통계적으로 유의미하지 않음(p-value > 0.05)을 보여줍니다.</p><hr><h3 id=4-결론-conclusion>4. 결론 (Conclusion)<a hidden class=anchor aria-hidden=true href=#4-결론-conclusion>#</a></h3><p>Flamingo는 거대 언어 모델(LLM)과 시각 모델을 효과적으로 연결하여, 최소한의 학습 데이터(Few-shot)만으로 다양한 멀티모달 태스크를 수행할 수 있는 범용 모델입니다.</p><ul><li><strong>핵심 기여:</strong> Perceiver Resampler와 GATED XATTN-DENSE를 도입하여 시각-언어 간의 간극을 줄였고, M3W 데이터셋을 통해 인터리브 데이터 처리 능력을 확보했습니다.</li><li><strong>의의:</strong> 별도의 파인 튜닝 없이도 프롬프트만으로 시각적 질문 응답, 캡션 생성, 대화 등이 가능함을 입증하여, 범용 인공지능(AGI)으로 가는 중요한 발판을 마련했습니다.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/multimodal/>Multimodal</a></li><li><a href=https://mookjsi.github.io/tags/few-shot-learning/>Few-Shot Learning</a></li><li><a href=https://mookjsi.github.io/tags/flamingo/>Flamingo</a></li><li><a href=https://mookjsi.github.io/tags/vision-language-models/>Vision-Language Models</a></li><li><a href=https://mookjsi.github.io/tags/deepmind/>DeepMind</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/diffusion/><span class=title>« Prev</span><br><span>Diffusion Models: From VAEs to DDPM Derivation</span>
</a><a class=next href=https://mookjsi.github.io/posts/paper-review-gan/><span class=title>Next »</span><br><span>Generative Adversarial Nets (GAN) 논문 리뷰</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on x" href="https://x.com/intent/tweet/?text=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f&amp;hashtags=PaperReview%2cDeepLearning%2cMultimodal%2cFew-ShotLearning%2cFlamingo%2cVision-LanguageModels%2cDeepMind"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f&amp;title=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning&amp;summary=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f&title=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on whatsapp" href="https://api.whatsapp.com/send?text=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on telegram" href="https://telegram.me/share/url?text=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Flamingo: a Visual Language Model for Few-Shot Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Flamingo%3a%20a%20Visual%20Language%20Model%20for%20Few-Shot%20Learning&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-flamingo%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>