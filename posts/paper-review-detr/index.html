<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DETR: End-to-End Object Detection with Transformers | MookStudy</title><meta name=keywords content="Paper Review,Deep Learning,Computer Vision,Object Detection,Transformer,DETR,End-to-End,ECCV 2020"><meta name=description content="&lsquo;End-to-End Object Detection with Transformers&rsquo; 논문 심층 리뷰"><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-detr/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-detr/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-detr/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="DETR: End-to-End Object Detection with Transformers"><meta property="og:description" content="‘End-to-End Object Detection with Transformers’ 논문 심층 리뷰"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-23T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-23T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Object Detection"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="DETR"><meta name=twitter:card content="summary"><meta name=twitter:title content="DETR: End-to-End Object Detection with Transformers"><meta name=twitter:description content="&lsquo;End-to-End Object Detection with Transformers&rsquo; 논문 심층 리뷰"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DETR: End-to-End Object Detection with Transformers","item":"https://mookjsi.github.io/posts/paper-review-detr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DETR: End-to-End Object Detection with Transformers","name":"DETR: End-to-End Object Detection with Transformers","description":"\u0026lsquo;End-to-End Object Detection with Transformers\u0026rsquo; 논문 심층 리뷰","keywords":["Paper Review","Deep Learning","Computer Vision","Object Detection","Transformer","DETR","End-to-End","ECCV 2020"],"articleBody":"1. 서론 객체 탐지란 이미지 속에서 우리가 관심 있는 물체(객체)가 무엇인지(분류, classification) 그리고 어디에 있는지(위치 파악, localization)를 알아내는 기술입니다. 예를 들어, 자율주행 자동차가 도로 위의 사람, 다른 자동차, 신호등을 정확히 인식하는 데 이 기술이 사용됩니다.\n기존의 객체 탐지 모델들은 이 문제를 약간 간접적인 방식으로 풀어왔습니다. 이미지 위에 수많은 가상의 사각형(앵커 박스, anchor box)이나 **후보 영역(proposal)**을 미리 뿌려놓고, 이 중에서 어떤 사각형이 실제 물체를 잘 감싸고 있는지, 그리고 그 물체는 무엇인지를 맞추는 방식이었습니다. 이 방법은 꽤 성공적이었지만 몇 가지 번거로운 과정이 필요했습니다.\n수작업 설계 (Hand-designed components): 앵커 박스의 크기나 비율을 미리 정해야 하는 등 사람이 직접 설정해야 하는 부분이 많았습니다. 후처리 (Post-processing): 모델이 하나의 물체에 대해 여러 개의 겹치는 박스를 예측하는 경우가 많아서, 이 중 가장 정확한 박스 하나만 남기는 **‘비최대 억제(Non-Maximum Suppression, NMS)’**라는 복잡한 후처리 과정이 필수적이었습니다. 이 논문에서 제안하는 DETR (DEtection TRansformer) 은 이러한 복잡한 과정들을 모두 없애고, 객체 탐지를 “정답 집합을 한 번에 예측하는 문제\"로 재정의했습니다. 마치 사람이 이미지를 한 번 쓱 보고 “여기엔 고양이 한 마리, 저기엔 강아지 두 마리가 있네\"라고 말하는 것처럼, 모델이 직접 최종 예측 결과의 **‘집합(set)’**을 출력하도록 만든 것입니다.\n이러한 혁신은 두 가지 핵심 요소 덕분에 가능했습니다.\n이분 매칭 손실 (Bipartite Matching Loss): 모델의 예측 결과와 실제 정답을 일대일로 짝지어주는 손실 함수입니다. 이를 통해 중복된 예측을 자연스럽게 방지합니다. 트랜스포머 (Transformer) 구조: 원래 번역 분야에서 뛰어난 성능을 보인 아키텍처로, 이미지 전체의 맥락을 종합적으로 이해하는 데 탁월한 능력을 보입니다. 위 그림은 DETR의 전체적인 흐름을 보여줍니다.\n먼저, 일반적인 CNN을 사용해 이미지의 특징(feature)을 추출합니다. 이는 이미지의 중요한 시각적 정보를 압축하는 과정입니다. 이 특징 정보를 트랜스포머 인코더-디코더에 입력합니다. 트랜스포머는 이미지 전체의 관계를 파악하여 고정된 개수의 예측 상자(box prediction) 집합을 출력합니다. 학습 과정에서는 **이분 매칭 손실(bipartite matching loss)**을 사용해 모델이 예측한 상자들과 실제 정답 상자들을 가장 효율적으로 일대일 매칭시킵니다. 매칭되지 못한 예측 상자는 **“물체 없음(no object)”**으로 학습됩니다. 2. 기술 설명 (The DETR model) 위 그림은 DETR의 상세한 구조를 보여줍니다.\n백본 (Backbone) ResNet과 같은 표준 CNN 모델을 사용하여 이미지에서 저해상도의 특징 맵(feature map)을 추출합니다. 예를 들어, 2048개의 채널을 가진 특징 맵을 만들어냅니다.\n트랜스포머 인코더 (Transformer Encoder) CNN이 만든 특징 맵은 2D 형태인데, 트랜스포머에 넣기 위해 1D 시퀀스(순서가 있는 데이터) 형태로 길게 펼칩니다. 각 특징의 위치 정보를 알려주기 위해 **공간적 위치 인코딩(spatial positional encoding)**을 더해줍니다. 트랜스포머는 본래 순서 개념이 없기 때문에, “이 특징은 이미지의 왼쪽 위에 있다\"와 같은 위치 정보를 인위적으로 주입해야 합니다. 인코더는 셀프 어텐션(self-attention) 메커니즘을 통해 이미지의 모든 픽셀 영역들이 서로 어떻게 연관되어 있는지를 학습합니다. 예를 들어, 이미지에 얼룩말 두 마리가 있다면, 인코더는 얼룩말 무늬를 가진 픽셀들이 서로 강하게 연관되어 있음을 파악하고, 각 얼룩말을 별개의 개체로 분리해내는 역할을 합니다. 트랜스포머 디코더 (Transformer Decoder) 객체 쿼리란? - 100명의 전문 탐정단\nDETR의 **객체 쿼리(object queries)**를 가장 쉽게 이해하는 방법은, 이미지를 ‘사건 현장’이라고 보고 100명의 전문 탐정단이 각자 빈 수첩(=객체 쿼리)을 들고 파견된다고 상상하는 것입니다.\n이 탐정들은 처음에는 아무 정보도 없는 상태(빈 수첩)로 시작하지만, 학습을 거치며 각자 자신만의 전문 분야(특정 위치, 크기, 형태 등)를 갖게 됩니다. 각 탐정(객체 쿼리)은 이미지 전체(인코더의 출력)를 샅샅이 살피며, 자신이 맡을 단서(객체)를 찾으려고 합니다. 탐정들끼리도 계속 정보를 공유(셀프 어텐션)하여, 한 명이 하나의 단서만 맡도록 역할을 분담합니다. 이 덕분에 하나의 물체에 여러 박스가 중복 예측되는 문제가 자연스럽게 해결됩니다(NMS 불필요). 디코더에서의 객체 쿼리 역할:\n디코더는 이미지 특징을 직접 받는 대신, **객체 쿼리(object queries)**라는 고정된 개수(N=100)의 학습 가능한 임베딩을 입력으로 받습니다. 이 객체 쿼리는 “이미지에서 찾을 N개의 물체 슬롯\"이자, ‘사건 현장에 파견된 100명의 탐정’에 해당합니다. 처음에는 빈 슬롯(빈 수첩)이지만, 학습을 통해 각각 특정 위치나 크기의 물체를 찾는 데 특화됩니다. 디코더는 이 객체 쿼리들과 인코더의 출력(이미지 전체의 문맥 정보)을 함께 사용하여 “물체가 어디에 있는지\"를 추론합니다. 디코더의 셀프 어텐션은 N개의 예측 간의 관계를 모델링하여, 예를 들어 하나의 물체에 대해 여러 슬롯이 중복으로 예측하는 것을 억제하는 역할을 합니다(탐정들끼리 역할 분담). 하나의 중요한 특징은 이 N개의 객체를 병렬적으로(in parallel), 즉 한 번에 디코딩한다는 점입니다. 기존 트랜스포머가 단어를 하나씩 순서대로 생성하는 것과 대조적입니다. 예측 헤드 (Prediction Heads, FFNs) 디코더에서 나온 N개의 출력 임베딩은 각각 동일한 구조를 공유하는 작은 신경망(FFN)으로 전달됩니다. 이 FFN은 최종적으로 각 슬롯에 대해 물체의 **클래스(class)**와 바운딩 박스(bounding box) 좌표를 예측합니다. 만약 해당 슬롯이 찾은 물체가 없다면, **“물체 없음(no object)”**이라는 특별한 클래스를 예측하게 됩니다. 손실 함수 (Loss Function): 객체 탐지를 위한 집합 예측 DETR의 핵심 아이디어는 예측과 정답을 ‘집합 대 집합’으로 보고, 둘 사이의 최적의 짝을 찾아 손실을 계산하는 것입니다.\n이분 매칭 (Bipartite Matching) 모델이 100개의 예측(집합 $\\hat{y}$)을 내놓고, 실제 이미지에는 5개의 물체(정답 집합 y)가 있다고 가정해 봅시다. 100개의 예측 중 어떤 것이 5개의 정답 각각에 해당하는지를 결정해야 합니다. 이때 **헝가리안 알고리즘(Hungarian Algorithm)**을 사용하여 전체 매칭 비용이 최소가 되는 최적의 일대일 짝(optimal assignment)을 찾습니다.\n이 매칭 비용 $\\mathcal{L}_{match}$는 두 가지를 고려합니다: (1) 예측한 클래스가 정답 클래스와 일치할 확률, (2) 예측한 박스가 정답 박스와 얼마나 비슷한지.\n수식 1: 최적 할당 (Optimal Assignment) $$ \\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_N}{\\arg\\min} \\sum_{i}^{N} \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) $$ 최적의 순열(permutation) $\\hat{\\sigma}$를 찾는 과정을 나타냅니다. 여기서 $\\mathfrak{S}_{N}$은 N개 원소의 모든 가능한 순열 집합을 의미합니다. $y_{i}$는 i번째 실제 정답 객체(클래스 $c_i$, 박스 $b_i$)이고, $\\hat{y}_{\\sigma(i)}$는 순열 $\\sigma$에 의해 재배열된 예측 중 i번째 위치에 온 예측입니다. 이 수식의 목표는 모든 N개의 쌍에 대한 매칭 비용($\\mathcal{L}_{\\text{match}}$)의 총합을 최소화하는 순열 $\\hat{\\sigma}$를 찾는 것입니다. 이는 전형적인 할당 문제(assignment problem)이며, 헝가리안 알고리즘을 통해 다항 시간 내에 효율적으로 해결할 수 있습니다. 이 과정을 통해 각 정답 객체에 대해 가장 적절한 예측이 단 하나만 할당되도록 보장하여, 중복 탐지를 원천적으로 방지합니다.\n수식 2: 헝가리안 손실 (Hungarian Loss) $$ \\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^{N} \\left[ -\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbf{1}_{c_i \\ne \\emptyset} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)}) \\right] $$ 위에서 찾은 최적의 짝 $\\hat{\\sigma}$을 바탕으로 최종 손실을 계산합니다. 이 손실은 모든 N개의 매칭된 쌍에 대한 손실의 합입니다.\n클래스 예측 손실: $-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_{i})$는 표준적인 교차 엔트로피(cross-entropy) 손실입니다. 즉, 최적의 짝으로 매칭된 예측이 정답 클래스 $c_i$를 얼마나 정확하게 예측했는지를 측정합니다. 박스 손실: $\\mathbf{1}_{{c_{i}\\ne\\emptyset}}\\mathcal{L}_{\\text{box}}(...)$는 바운딩 박스에 대한 손실입니다. $\\mathbf{1}_{{c_{i}\\ne\\emptyset}}$는 정답이 ‘물체 없음’이 아닐 경우에만 박스 손실을 계산하라는 의미의 지시 함수(indicator function)입니다. $\\mathcal{L}_{\\text{box}}$는 아래에서 설명할 L1 손실과 GIoU 손실의 조합으로 이루어집니다. 바운딩 박스 손실 ($\\mathcal{L}_{box}$) 기존의 L1 손실(좌표 차이의 절댓값 합)은 박스가 클 때 손실 값도 커지는 문제가 있어, 박스 크기에 따라 손실의 스케일이 달라집니다. 이를 해결하기 위해 DETR은 두 가지 손실을 조합합니다. $$ \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{L1}||b_i - \\hat{b}_{\\sigma(i)}||_1 $$ L1 손실: 예측 박스와 정답 박스의 중심점 좌표, 너비, 높이 간의 차이를 계산합니다. 일반화된 IoU (GIoU) 손실: 두 박스가 얼마나 겹치는지를 나타내는 IoU(Intersection over Union)를 일반화한 것으로, 박스 크기에 무관하게(scale-invariant) 손실을 측정할 수 있어 작은 물체와 큰 물체를 공평하게 학습할 수 있습니다. 수식 예시로 이해하기: “고양이와 강아지 찾기\"로 풀어보는 DETR의 집합 예측 지금까지 소개한 수식들을 실제 객체 탐지 상황에 대입해, 처음부터 끝까지 하나의 흐름으로 예시를 들어 설명해보겠습니다.\n상황 예시: 이미지 속 고양이와 강아지 입력 이미지: 왼쪽에는 고양이 한 마리, 오른쪽에는 강아지 한 마리가 있는 사진 DETR 모델: 최대 4개($N=4$)의 객체를 예측하도록 설정 1. 정답(Ground Truth)과 모델의 예측 정답 집합 (y):\nGT1: {클래스: ‘고양이’, 박스: b_cat} GT2: {클래스: ‘강아지’, 박스: b_dog} GT3: {클래스: ‘없음 ∅’} GT4: {클래스: ‘없음 ∅’} 모델의 예측 집합 (ŷ):\nP1: {‘고양이’ 확률 0.9, 박스: b_p1} (고양이와 매우 비슷하게 예측) P2: {‘강아지’ 확률 0.8, 박스: b_p2} (강아지와 매우 비슷하게 예측) P3: {‘고양이’ 확률 0.7, 박스: b_p3} (P1과 중복된, 덜 정확한 고양이 예측) P4: {‘없음 ∅’ 확률 0.85, 박스: b_p4} (스스로 ‘없음’이라고 예측) 2. 수식 1: 최적의 짝 찾기 (Optimal Assignment) $\\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_N}{\\arg\\min} \\sum_{i}^{N} \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})$ 이 단계에서는 4개의 정답과 4개의 예측 사이에서 가장 합리적인 일대일 짝을 찾습니다.\n비용($\\mathcal{L}_{match}$)은 (1) 클래스 확률, (2) 박스 유사도를 모두 고려합니다.\n컴퓨터는 아래와 같은 비용 행렬을 만듭니다(실제 값은 예시):\n예측 P1 (고양이 0.9) 예측 P2 (강아지 0.8) 예측 P3 (고양이 0.7) 예측 P4 (없음 0.85) GT1 (고양이) 비용 낮음 비용 높음 비용 중간 비용 높음 GT2 (강아지) 비용 높음 비용 낮음 비용 높음 비용 높음 GT3 (없음 ∅) 비용 중간 비용 중간 비용 중간 비용 중간 GT4 (없음 ∅) 비용 중간 비용 중간 비용 중간 비용 중간 헝가리안 알고리즘이 이 표를 분석해 전체 비용이 최소가 되는 조합을 찾습니다.\n결과적으로 다음과 같은 짝($\\hat{\\sigma}$)이 만들어집니다.\nGT1 (‘고양이’) ↔️ P1 (가장 정확한 고양이 예측) GT2 (‘강아지’) ↔️ P2 (가장 정확한 강아지 예측) GT3 (‘없음 ∅’) ↔️ P3 (중복된 고양이 예측은 ‘없음’ 처리) GT4 (‘없음 ∅’) ↔️ P4 (쓸모없는 예측은 ‘없음’ 처리) 3. 수식 2: 최종 손실 계산 (Hungarian Loss) $\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^{N} \\left[ -\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbf{1}_{c_i \\ne \\emptyset} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)}) \\right]$ 위에서 찾은 짝을 바탕으로, 각 쌍에 대해 손실을 계산합니다.\n[GT1↔P1] 클래스 손실: 고양이 확률 0.9 → $-\\log(0.9)$ (낮은 손실) 박스 손실: 실제 고양이 박스와 예측 박스의 차이($\\mathcal{L}_{box}$) [GT2↔P2] 클래스 손실: 강아지 확률 0.8 → $-\\log(0.8)$ (낮은 손실) 박스 손실: 실제 강아지 박스와 예측 박스의 차이($\\mathcal{L}_{box}$) [GT3↔P3] 클래스 손실: 정답은 ‘없음’, 예측이 ‘없음’일 확률이 10%라면 $-\\log(0.1)$ (매우 큰 손실) 박스 손실: 정답이 ‘없음’이므로 계산하지 않음(0점) [GT4↔P4] 클래스 손실: 정답은 ‘없음’, 예측이 ‘없음’일 확률이 85%라면 $-\\log(0.85)$ (낮은 손실) 박스 손실: 정답이 ‘없음’이므로 계산하지 않음(0점) 이렇게 네 쌍의 손실을 모두 더한 값이 최종 손실이 되어, 모델이 더 똑똑해지도록 학습에 사용됩니다.\n4. 바운딩 박스 손실($\\mathcal{L}_{box}$)의 실제 의미 $\\mathcal{L}_{\\text{box}} = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}} + \\lambda_{L1}||\\cdot||_1$ L1 손실: 예측 박스와 실제 박스의 좌표(x, y, w, h) 차이의 절댓값 합\n예) 고양이 박스와 예측 박스가 각각 (10,10,50,50), (12,11,48,52)라면, L1 손실은 $|10-12|+|10-11|+|50-48|+|50-52|=2+1+2+2=7$ 단점: 큰 물체와 작은 물체에 동일한 픽셀 오차가 들어가면 불공평함 GIoU 손실: 두 박스가 얼마나 겹치는지(Intersection over Union, IoU)를 일반화한 값\n예) 고양이 박스와 예측 박스가 거의 겹치면 GIoU 손실이 매우 작음(정확) 장점: 박스 크기에 상관없이 공평하게 평가 최종 조합: GIoU 손실로 전체적인 위치와 크기 일치도를, L1 손실로 미세한 위치 조정을 동시에 학습\n이처럼 DETR의 집합 예측 손실은\n중복 예측을 방지하고, 박스 크기에 상관없이 공평하게 평가하며, 후처리(NMS) 없이도 정확한 객체 탐지가 가능하도록 설계되어 있습니다. 3. 실험 (Experiments) DETR의 성능을 검증하기 위해 가장 널리 사용되는 COCO 데이터셋으로 다양한 실험을 진행했습니다.\n이 표는 DETR과 기존의 강력한 모델인 Faster R-CNN의 성능을 비교합니다.\n결론적으로, DETR은 매우 잘 최적화된 Faster R-CNN과 **비교할 만한 성능(AP 42.0 vs 42.0)**을 달성했습니다. 특히 주목할 점은, DETR이 **큰 물체에 대해서는 훨씬 뛰어난 성능($AP_L$ 61.1 vs 53.4)**을 보인다는 것입니다. 이는 트랜스포머 인코더가 이미지 전체의 넓은 문맥을 보기 때문에 가능한 것으로 분석됩니다. 반면, 작은 물체에 대해서는 성능이 다소 낮게($AP_S$ 20.5 vs 26.6) 나타났습니다.\n주요 구성 요소 분석 (Ablation Studies) DETR의 어떤 부분이 성능에 얼마나 기여하는지 알아보기 위해 여러 실험을 진행했습니다.\n인코더의 중요성 (Table 2, Figure 3): 인코더 층을 제거하자 전체 성능(AP)이 약 3.9점 하락했으며, 특히 큰 물체에 대한 성능이 6.0점이나 떨어졌습니다. 이는 이미지 전체의 맥락을 이해하는 인코더가 객체들을 서로 분리하는 데 중요한 역할을 함을 시사합니다.\nFigure 3은 인코더의 셀프 어텐션이 어떻게 동작하는지 시각화한 것입니다. 그림 중앙의 소 이미지 위에 찍힌 빨간 점이 특정 위치를 의미하고, 주변의 작은 이미지들은 해당 위치가 이미지의 다른 어떤 부분에 주목(attention)하는지를 보여줍니다. 각기 다른 소들이 서로 다른 영역으로 분리되어 주목받는 것을 볼 수 있는데, 이는 인코더가 이미 개별 인스턴스들을 분리하고 있음을 보여줍니다.\n디코더의 중요성 (Figure 4): 이 그래프는 디코더 층이 깊어질수록 성능(AP)이 꾸준히 향상되는 것을 보여줍니다. 첫 번째 디코더 층의 결과와 마지막 층의 결과를 비교하면 AP가 8점 이상 크게 향상되었습니다.\n흥미로운 점은, 첫 번째 디코더 층에서는 NMS 후처리를 적용하면 성능이 오르지만, 층이 깊어질수록 NMS의 효과가 줄어들고 마지막에는 오히려 성능을 약간 해친다는 것입니다. 이는 디코더의 셀프 어텐션이 스스로 중복 예측을 억제하는 법을 학습하기 때문에, DETR은 설계적으로 NMS가 필요 없다는 것을 실험적으로 증명합니다.\n그 외 요소들 (Table 3, 4): 위치 인코딩 (Table 3): 공간적 위치 인코딩을 제거하자 성능이 7.8 AP나 하락하여, 이 요소가 매우 중요함을 확인했습니다. 손실 함수 (Table 4): L1 손실과 GIoU 손실 중 GIoU 손실이 성능에 거의 절대적인 기여를 하며, L1 손실은 보조적인 역할만 한다는 것을 발견했습니다. Figure 6은 디코더가 각 물체를 예측할 때 이미지의 어느 부분을 주목하는지 보여줍니다. 코끼리나 얼룩말을 탐지할 때, 주로 머리, 다리 등 물체의 **윤곽을 결정하는 극단적인 부분(extremities)**에 주목하는 경향을 보입니다. 이는 인코더가 이미 “여기에 얼룩말이 있다\"고 알려주면, 디코더는 그 경계만 정확히 그리는 데 집중한다는 가설을 뒷받침합니다.\nFigure 7은 100개의 객체 쿼리(슬롯)가 각각 어떤 종류의 박스를 예측하도록 학습되는지 보여줍니다. 각 슬롯은 특정 **위치(area)와 크기(box size)**를 전담하도록 특화되는 경향을 보입니다. 예를 들어 어떤 슬롯은 이미지 중앙의 큰 물체를, 다른 슬롯은 왼쪽 하단의 작은 물체를 주로 찾도록 학습됩니다.\n학습 데이터에는 기린이 13마리 이상 있는 이미지가 없었습니다. 이 모델이 학습 데이터에 없는 상황에서도 잘 동작하는지 알아보기 위해, 인공적으로 기린 24마리가 있는 이미지를 만들어 테스트했습니다.\nFigure 5에서 볼 수 있듯이, DETR은 놀랍게도 24마리의 기린을 모두 정확하게 찾아냈습니다. 이는 100개의 객체 쿼리가 특정 클래스(예: ‘기린 전용 쿼리’)에 과적합되지 않고, 일반적인 물체를 찾는 능력을 학습했음을 보여줍니다.\nPanoptic Segmentation으로의 확장 Panoptic Segmentation은 이미지의 모든 픽셀을 “어떤 물체에 속하는지(things)” 또는 “어떤 배경에 속하는지(stuff)“로 구분하는 더 복잡한 작업입니다.\nFigure 8은 DETR에 간단한 **마스크 헤드(mask head)**를 추가하여 이 작업을 수행하는 방법을 보여줍니다. 디코더의 출력 각각에 대해 해당 물체의 마스크를 예측하고, 이를 합쳐 최종 결과를 만듭니다.\nTable 5는 DETR이 PanopticFPN과 같은 기존의 강력한 모델들을 능가하는 성능을 보였음을 나타냅니다. 특히 배경(stuff) 클래스에서 강점을 보이는데, 이는 이미지 전체를 보는 인코더의 힘 덕분일 가능성이 높습니다.\nFigure 9는 DETR이 생성한 Panoptic Segmentation의 예시 이미지로, 물체와 배경 모두에 대해 깔끔한 결과를 보여줍니다.\n4. 결론 (Conclusion) 이 논문은 트랜스포머와 이분 매칭 손실을 기반으로 객체 탐지를 **‘직접적인 집합 예측 문제’**로 풀어내는 새로운 패러다임인 DETR을 제시했습니다. DETR은 기존의 복잡한 파이프라인(앵커, NMS 등)을 제거하면서도, 고도로 최적화된 Faster R-CNN과 대등한 성능을 달성했습니다.\n또한, DETR의 구조는 매우 간단하고 유연하여 Panoptic Segmentation과 같은 더 복잡한 문제로도 쉽게 확장될 수 있음을 보여주었습니다. 특히 이미지의 전역적인 정보를 활용하는 능력 덕분에 큰 객체 탐지에서 뛰어난 성능을 보였습니다.\n","wordCount":"2098","inLanguage":"en","datePublished":"2025-09-23T00:00:00Z","dateModified":"2025-09-23T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-detr/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">DETR: End-to-End Object Detection with Transformers</h1><div class=post-meta><span title='2025-09-23 00:00:00 +0000 UTC'>September 23, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2098 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><h3 id=1-서론>1. 서론<a hidden class=anchor aria-hidden=true href=#1-서론>#</a></h3><p>객체 탐지란 이미지 속에서 우리가 관심 있는 물체(객체)가 무엇인지(분류, classification) 그리고 어디에 있는지(위치 파악, localization)를 알아내는 기술입니다. 예를 들어, 자율주행 자동차가 도로 위의 사람, 다른 자동차, 신호등을 정확히 인식하는 데 이 기술이 사용됩니다.</p><p>기존의 객체 탐지 모델들은 이 문제를 약간 간접적인 방식으로 풀어왔습니다. 이미지 위에 수많은 가상의 사각형(<strong>앵커 박스, anchor box</strong>)이나 **후보 영역(proposal)**을 미리 뿌려놓고, 이 중에서 어떤 사각형이 실제 물체를 잘 감싸고 있는지, 그리고 그 물체는 무엇인지를 맞추는 방식이었습니다. 이 방법은 꽤 성공적이었지만 몇 가지 번거로운 과정이 필요했습니다.</p><p><img alt="Figure: 기존의 Detection 메커니즘, 출처: https://herbwood.tistory.com/26" loading=lazy src=/images/posts/detr/figure_a.png></p><ul><li><strong>수작업 설계 (Hand-designed components)</strong>: 앵커 박스의 크기나 비율을 미리 정해야 하는 등 사람이 직접 설정해야 하는 부분이 많았습니다.</li><li><strong>후처리 (Post-processing)</strong>: 모델이 하나의 물체에 대해 여러 개의 겹치는 박스를 예측하는 경우가 많아서, 이 중 가장 정확한 박스 하나만 남기는 **&lsquo;비최대 억제(Non-Maximum Suppression, NMS)&rsquo;**라는 복잡한 후처리 과정이 필수적이었습니다.</li></ul><p>이 논문에서 제안하는 <strong>DETR (DEtection TRansformer)</strong> 은 이러한 복잡한 과정들을 모두 없애고, 객체 탐지를 &ldquo;정답 집합을 한 번에 예측하는 문제"로 재정의했습니다. 마치 사람이 이미지를 한 번 쓱 보고 &ldquo;여기엔 고양이 한 마리, 저기엔 강아지 두 마리가 있네"라고 말하는 것처럼, 모델이 직접 최종 예측 결과의 **&lsquo;집합(set)&rsquo;**을 출력하도록 만든 것입니다.</p><p>이러한 혁신은 두 가지 핵심 요소 덕분에 가능했습니다.</p><ol><li><strong>이분 매칭 손실 (Bipartite Matching Loss)</strong>: 모델의 예측 결과와 실제 정답을 일대일로 짝지어주는 손실 함수입니다. 이를 통해 중복된 예측을 자연스럽게 방지합니다.</li><li><strong>트랜스포머 (Transformer) 구조</strong>: 원래 번역 분야에서 뛰어난 성능을 보인 아키텍처로, 이미지 전체의 맥락을 종합적으로 이해하는 데 탁월한 능력을 보입니다.</li></ol><p><img alt="Figure 1: DETR 모델 개요" loading=lazy src=/images/posts/detr/figure1.png></p><p>위 그림은 DETR의 전체적인 흐름을 보여줍니다.</p><ul><li>먼저, 일반적인 CNN을 사용해 이미지의 특징(feature)을 추출합니다. 이는 이미지의 중요한 시각적 정보를 압축하는 과정입니다.</li><li>이 특징 정보를 트랜스포머 인코더-디코더에 입력합니다. 트랜스포머는 이미지 전체의 관계를 파악하여 고정된 개수의 예측 상자(box prediction) 집합을 출력합니다.</li><li>학습 과정에서는 **이분 매칭 손실(bipartite matching loss)**을 사용해 모델이 예측한 상자들과 실제 정답 상자들을 가장 효율적으로 일대일 매칭시킵니다. 매칭되지 못한 예측 상자는 **&ldquo;물체 없음(no object)&rdquo;**으로 학습됩니다.</li></ul><hr><h3 id=2-기술-설명-the-detr-model>2. 기술 설명 (The DETR model)<a hidden class=anchor aria-hidden=true href=#2-기술-설명-the-detr-model>#</a></h3><p><img alt="Figure 2: DETR 상세 아키텍처" loading=lazy src=/images/posts/detr/figure2.png></p><p><img alt="Figure 10: DETR 상세 아키텍처" loading=lazy src=/images/posts/detr/figure10.png></p><p>위 그림은 DETR의 상세한 구조를 보여줍니다.</p><h4 id=백본-backbone>백본 (Backbone)<a hidden class=anchor aria-hidden=true href=#백본-backbone>#</a></h4><p>ResNet과 같은 표준 CNN 모델을 사용하여 이미지에서 저해상도의 특징 맵(feature map)을 추출합니다. 예를 들어, 2048개의 채널을 가진 특징 맵을 만들어냅니다.</p><h4 id=트랜스포머-인코더-transformer-encoder>트랜스포머 인코더 (Transformer Encoder)<a hidden class=anchor aria-hidden=true href=#트랜스포머-인코더-transformer-encoder>#</a></h4><ol><li>CNN이 만든 특징 맵은 2D 형태인데, 트랜스포머에 넣기 위해 1D 시퀀스(순서가 있는 데이터) 형태로 길게 펼칩니다.</li><li>각 특징의 위치 정보를 알려주기 위해 **공간적 위치 인코딩(spatial positional encoding)**을 더해줍니다. 트랜스포머는 본래 순서 개념이 없기 때문에, &ldquo;이 특징은 이미지의 왼쪽 위에 있다"와 같은 위치 정보를 인위적으로 주입해야 합니다.</li><li>인코더는 <strong>셀프 어텐션(self-attention)</strong> 메커니즘을 통해 이미지의 모든 픽셀 영역들이 서로 어떻게 연관되어 있는지를 학습합니다. 예를 들어, 이미지에 얼룩말 두 마리가 있다면, 인코더는 얼룩말 무늬를 가진 픽셀들이 서로 강하게 연관되어 있음을 파악하고, 각 얼룩말을 별개의 개체로 분리해내는 역할을 합니다.</li></ol><h4 id=트랜스포머-디코더-transformer-decoder>트랜스포머 디코더 (Transformer Decoder)<a hidden class=anchor aria-hidden=true href=#트랜스포머-디코더-transformer-decoder>#</a></h4><p>객체 쿼리란? - 100명의 전문 탐정단<br>DETR의 **객체 쿼리(object queries)**를 가장 쉽게 이해하는 방법은, 이미지를 &lsquo;사건 현장&rsquo;이라고 보고 <strong>100명의 전문 탐정단</strong>이 각자 빈 수첩(=객체 쿼리)을 들고 파견된다고 상상하는 것입니다.</p><ul><li>이 탐정들은 처음에는 아무 정보도 없는 상태(빈 수첩)로 시작하지만, 학습을 거치며 각자 자신만의 전문 분야(특정 위치, 크기, 형태 등)를 갖게 됩니다.</li><li>각 탐정(객체 쿼리)은 이미지 전체(인코더의 출력)를 샅샅이 살피며, 자신이 맡을 단서(객체)를 찾으려고 합니다.</li><li>탐정들끼리도 계속 정보를 공유(셀프 어텐션)하여, 한 명이 하나의 단서만 맡도록 역할을 분담합니다. 이 덕분에 하나의 물체에 여러 박스가 중복 예측되는 문제가 자연스럽게 해결됩니다(NMS 불필요).</li></ul><p>디코더에서의 객체 쿼리 역할:</p><ol><li>디코더는 이미지 특징을 직접 받는 대신, **객체 쿼리(object queries)**라는 고정된 개수(N=100)의 학습 가능한 임베딩을 입력으로 받습니다. 이 객체 쿼리는 &ldquo;이미지에서 찾을 N개의 물체 슬롯"이자, &lsquo;사건 현장에 파견된 100명의 탐정&rsquo;에 해당합니다. 처음에는 빈 슬롯(빈 수첩)이지만, 학습을 통해 각각 특정 위치나 크기의 물체를 찾는 데 특화됩니다.</li><li>디코더는 이 객체 쿼리들과 인코더의 출력(이미지 전체의 문맥 정보)을 함께 사용하여 &ldquo;물체가 어디에 있는지"를 추론합니다. 디코더의 셀프 어텐션은 N개의 예측 간의 관계를 모델링하여, 예를 들어 하나의 물체에 대해 여러 슬롯이 중복으로 예측하는 것을 억제하는 역할을 합니다(탐정들끼리 역할 분담).</li><li>하나의 중요한 특징은 이 N개의 객체를 <strong>병렬적으로(in parallel)</strong>, 즉 한 번에 디코딩한다는 점입니다. 기존 트랜스포머가 단어를 하나씩 순서대로 생성하는 것과 대조적입니다.</li></ol><h4 id=예측-헤드-prediction-heads-ffns>예측 헤드 (Prediction Heads, FFNs)<a hidden class=anchor aria-hidden=true href=#예측-헤드-prediction-heads-ffns>#</a></h4><ul><li>디코더에서 나온 N개의 출력 임베딩은 각각 동일한 구조를 공유하는 작은 신경망(FFN)으로 전달됩니다.</li><li>이 FFN은 최종적으로 각 슬롯에 대해 물체의 **클래스(class)**와 <strong>바운딩 박스(bounding box)</strong> 좌표를 예측합니다. 만약 해당 슬롯이 찾은 물체가 없다면, **&ldquo;물체 없음(no object)&rdquo;**이라는 특별한 클래스를 예측하게 됩니다.</li></ul><h4 id=손실-함수-loss-function-객체-탐지를-위한-집합-예측>손실 함수 (Loss Function): 객체 탐지를 위한 집합 예측<a hidden class=anchor aria-hidden=true href=#손실-함수-loss-function-객체-탐지를-위한-집합-예측>#</a></h4><p>DETR의 핵심 아이디어는 예측과 정답을 &lsquo;집합 대 집합&rsquo;으로 보고, 둘 사이의 최적의 짝을 찾아 손실을 계산하는 것입니다.</p><h5 id=이분-매칭-bipartite-matching>이분 매칭 (Bipartite Matching)<a hidden class=anchor aria-hidden=true href=#이분-매칭-bipartite-matching>#</a></h5><p>모델이 100개의 예측(집합 $\hat{y}$)을 내놓고, 실제 이미지에는 5개의 물체(정답 집합 y)가 있다고 가정해 봅시다. 100개의 예측 중 어떤 것이 5개의 정답 각각에 해당하는지를 결정해야 합니다. 이때 **헝가리안 알고리즘(Hungarian Algorithm)**을 사용하여 전체 매칭 비용이 최소가 되는 최적의 일대일 짝(optimal assignment)을 찾습니다.</p><p>이 매칭 비용 $\mathcal{L}_{match}$는 두 가지를 고려합니다: (1) 예측한 클래스가 정답 클래스와 일치할 확률, (2) 예측한 박스가 정답 박스와 얼마나 비슷한지.</p><p><strong>수식 1: 최적 할당 (Optimal Assignment)</strong>
$$
\hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\arg\min} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})
$$</p><p>최적의 순열(permutation) $\hat{\sigma}$를 찾는 과정을 나타냅니다. 여기서 $\mathfrak{S}_{N}$은 N개 원소의 모든 가능한 순열 집합을 의미합니다. $y_{i}$는 i번째 실제 정답 객체(클래스 $c_i$, 박스 $b_i$)이고, $\hat{y}_{\sigma(i)}$는 순열 $\sigma$에 의해 재배열된 예측 중 i번째 위치에 온 예측입니다. 이 수식의 목표는 모든 N개의 쌍에 대한 매칭 비용($\mathcal{L}_{\text{match}}$)의 총합을 최소화하는 순열 $\hat{\sigma}$를 찾는 것입니다. 이는 전형적인 할당 문제(assignment problem)이며, 헝가리안 알고리즘을 통해 다항 시간 내에 효율적으로 해결할 수 있습니다. 이 과정을 통해 각 정답 객체에 대해 가장 적절한 예측이 단 하나만 할당되도록 보장하여, 중복 탐지를 원천적으로 방지합니다.</p><p><strong>수식 2: 헝가리안 손실 (Hungarian Loss)</strong>
$$
\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log\hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbf{1}_{c_i \ne \emptyset} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]
$$
위에서 찾은 최적의 짝 $\hat{\sigma}$을 바탕으로 최종 손실을 계산합니다. 이 손실은 모든 N개의 매칭된 쌍에 대한 손실의 합입니다.</p><ul><li><strong>클래스 예측 손실</strong>: $-\log\hat{p}_{\hat{\sigma}(i)}(c_{i})$는 표준적인 교차 엔트로피(cross-entropy) 손실입니다. 즉, 최적의 짝으로 매칭된 예측이 정답 클래스 $c_i$를 얼마나 정확하게 예측했는지를 측정합니다.</li><li><strong>박스 손실</strong>: $\mathbf{1}_{{c_{i}\ne\emptyset}}\mathcal{L}_{\text{box}}(...)$는 바운딩 박스에 대한 손실입니다. $\mathbf{1}_{{c_{i}\ne\emptyset}}$는 정답이 &lsquo;물체 없음&rsquo;이 아닐 경우에만 박스 손실을 계산하라는 의미의 지시 함수(indicator function)입니다. $\mathcal{L}_{\text{box}}$는 아래에서 설명할 L1 손실과 GIoU 손실의 조합으로 이루어집니다.</li></ul><h5 id=바운딩-박스-손실-mathcall_box>바운딩 박스 손실 ($\mathcal{L}_{box}$)<a hidden class=anchor aria-hidden=true href=#바운딩-박스-손실-mathcall_box>#</a></h5><p>기존의 L1 손실(좌표 차이의 절댓값 합)은 박스가 클 때 손실 값도 커지는 문제가 있어, 박스 크기에 따라 손실의 스케일이 달라집니다. 이를 해결하기 위해 DETR은 두 가지 손실을 조합합니다.
$$
\mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{\text{iou}}\mathcal{L}_{\text{iou}}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1}||b_i - \hat{b}_{\sigma(i)}||_1
$$</p><ul><li><strong>L1 손실</strong>: 예측 박스와 정답 박스의 중심점 좌표, 너비, 높이 간의 차이를 계산합니다.</li><li><strong>일반화된 IoU (GIoU) 손실</strong>: 두 박스가 얼마나 겹치는지를 나타내는 IoU(Intersection over Union)를 일반화한 것으로, 박스 크기에 무관하게(scale-invariant) 손실을 측정할 수 있어 작은 물체와 큰 물체를 공평하게 학습할 수 있습니다.</li></ul><hr><h3 id=수식-예시로-이해하기-고양이와-강아지-찾기로-풀어보는-detr의-집합-예측>수식 예시로 이해하기: &ldquo;고양이와 강아지 찾기"로 풀어보는 DETR의 집합 예측<a hidden class=anchor aria-hidden=true href=#수식-예시로-이해하기-고양이와-강아지-찾기로-풀어보는-detr의-집합-예측>#</a></h3><p>지금까지 소개한 수식들을 실제 객체 탐지 상황에 대입해, 처음부터 끝까지 하나의 흐름으로 예시를 들어 설명해보겠습니다.</p><hr><h4 id=상황-예시-이미지-속-고양이와-강아지><strong>상황 예시: 이미지 속 고양이와 강아지</strong><a hidden class=anchor aria-hidden=true href=#상황-예시-이미지-속-고양이와-강아지>#</a></h4><ul><li><strong>입력 이미지:</strong> 왼쪽에는 고양이 한 마리, 오른쪽에는 강아지 한 마리가 있는 사진</li><li><strong>DETR 모델:</strong> 최대 4개($N=4$)의 객체를 예측하도록 설정</li></ul><h5 id=1-정답ground-truth과-모델의-예측><strong>1. 정답(Ground Truth)과 모델의 예측</strong><a hidden class=anchor aria-hidden=true href=#1-정답ground-truth과-모델의-예측>#</a></h5><ul><li><p><strong>정답 집합 (y):</strong></p><ul><li>GT1: {클래스: &lsquo;고양이&rsquo;, 박스: <code>b_cat</code>}</li><li>GT2: {클래스: &lsquo;강아지&rsquo;, 박스: <code>b_dog</code>}</li><li>GT3: {클래스: &lsquo;없음 ∅&rsquo;}</li><li>GT4: {클래스: &lsquo;없음 ∅&rsquo;}</li></ul></li><li><p><strong>모델의 예측 집합 (ŷ):</strong></p><ul><li>P1: {&lsquo;고양이&rsquo; 확률 0.9, 박스: <code>b_p1</code>} <em>(고양이와 매우 비슷하게 예측)</em></li><li>P2: {&lsquo;강아지&rsquo; 확률 0.8, 박스: <code>b_p2</code>} <em>(강아지와 매우 비슷하게 예측)</em></li><li>P3: {&lsquo;고양이&rsquo; 확률 0.7, 박스: <code>b_p3</code>} <em>(P1과 중복된, 덜 정확한 고양이 예측)</em></li><li>P4: {&lsquo;없음 ∅&rsquo; 확률 0.85, 박스: <code>b_p4</code>} <em>(스스로 &lsquo;없음&rsquo;이라고 예측)</em></li></ul></li></ul><hr><h5 id=2-수식-1-최적의-짝-찾기-optimal-assignment><strong>2. 수식 1: 최적의 짝 찾기 (Optimal Assignment)</strong><a hidden class=anchor aria-hidden=true href=#2-수식-1-최적의-짝-찾기-optimal-assignment>#</a></h5><blockquote>$\hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\arg\min} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})$</blockquote><p>이 단계에서는 4개의 정답과 4개의 예측 사이에서 <strong>가장 합리적인 일대일 짝</strong>을 찾습니다.<br>비용($\mathcal{L}_{match}$)은 (1) 클래스 확률, (2) 박스 유사도를 모두 고려합니다.</p><p>컴퓨터는 아래와 같은 <strong>비용 행렬</strong>을 만듭니다(실제 값은 예시):</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>예측 P1 (고양이 0.9)</th><th style=text-align:left>예측 P2 (강아지 0.8)</th><th style=text-align:left>예측 P3 (고양이 0.7)</th><th style=text-align:left>예측 P4 (없음 0.85)</th></tr></thead><tbody><tr><td style=text-align:left><strong>GT1 (고양이)</strong></td><td style=text-align:left><strong>비용 낮음</strong></td><td style=text-align:left>비용 높음</td><td style=text-align:left>비용 중간</td><td style=text-align:left>비용 높음</td></tr><tr><td style=text-align:left><strong>GT2 (강아지)</strong></td><td style=text-align:left>비용 높음</td><td style=text-align:left><strong>비용 낮음</strong></td><td style=text-align:left>비용 높음</td><td style=text-align:left>비용 높음</td></tr><tr><td style=text-align:left><strong>GT3 (없음 ∅)</strong></td><td style=text-align:left>비용 중간</td><td style=text-align:left>비용 중간</td><td style=text-align:left><strong>비용 중간</strong></td><td style=text-align:left><strong>비용 중간</strong></td></tr><tr><td style=text-align:left><strong>GT4 (없음 ∅)</strong></td><td style=text-align:left>비용 중간</td><td style=text-align:left>비용 중간</td><td style=text-align:left><strong>비용 중간</strong></td><td style=text-align:left><strong>비용 중간</strong></td></tr></tbody></table><p><strong>헝가리안 알고리즘</strong>이 이 표를 분석해 전체 비용이 최소가 되는 조합을 찾습니다.<br>결과적으로 다음과 같은 짝($\hat{\sigma}$)이 만들어집니다.</p><ul><li>GT1 (&lsquo;고양이&rsquo;) ↔️ P1 (가장 정확한 고양이 예측)</li><li>GT2 (&lsquo;강아지&rsquo;) ↔️ P2 (가장 정확한 강아지 예측)</li><li>GT3 (&lsquo;없음 ∅&rsquo;) ↔️ P3 (중복된 고양이 예측은 &lsquo;없음&rsquo; 처리)</li><li>GT4 (&lsquo;없음 ∅&rsquo;) ↔️ P4 (쓸모없는 예측은 &lsquo;없음&rsquo; 처리)</li></ul><hr><h5 id=3-수식-2-최종-손실-계산-hungarian-loss><strong>3. 수식 2: 최종 손실 계산 (Hungarian Loss)</strong><a hidden class=anchor aria-hidden=true href=#3-수식-2-최종-손실-계산-hungarian-loss>#</a></h5><blockquote>$\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log\hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbf{1}_{c_i \ne \emptyset} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]$</blockquote><p>위에서 찾은 짝을 바탕으로, 각 쌍에 대해 손실을 계산합니다.</p><ul><li><strong>[GT1↔P1]</strong><ul><li>클래스 손실: 고양이 확률 0.9 → $-\log(0.9)$ (낮은 손실)</li><li>박스 손실: 실제 고양이 박스와 예측 박스의 차이($\mathcal{L}_{box}$)</li></ul></li><li><strong>[GT2↔P2]</strong><ul><li>클래스 손실: 강아지 확률 0.8 → $-\log(0.8)$ (낮은 손실)</li><li>박스 손실: 실제 강아지 박스와 예측 박스의 차이($\mathcal{L}_{box}$)</li></ul></li><li><strong>[GT3↔P3]</strong><ul><li>클래스 손실: 정답은 &lsquo;없음&rsquo;, 예측이 &lsquo;없음&rsquo;일 확률이 10%라면 $-\log(0.1)$ (매우 큰 손실)</li><li>박스 손실: 정답이 &lsquo;없음&rsquo;이므로 계산하지 않음(0점)</li></ul></li><li><strong>[GT4↔P4]</strong><ul><li>클래스 손실: 정답은 &lsquo;없음&rsquo;, 예측이 &lsquo;없음&rsquo;일 확률이 85%라면 $-\log(0.85)$ (낮은 손실)</li><li>박스 손실: 정답이 &lsquo;없음&rsquo;이므로 계산하지 않음(0점)</li></ul></li></ul><p>이렇게 네 쌍의 손실을 모두 더한 값이 최종 손실이 되어, 모델이 더 똑똑해지도록 학습에 사용됩니다.</p><hr><h5 id=4-바운딩-박스-손실hahahugoshortcode4s28hbhb의-실제-의미><strong>4. 바운딩 박스 손실($\mathcal{L}_{box}$)의 실제 의미</strong><a hidden class=anchor aria-hidden=true href=#4-바운딩-박스-손실hahahugoshortcode4s28hbhb의-실제-의미>#</a></h5><blockquote>$\mathcal{L}_{\text{box}} = \lambda_{\text{iou}}\mathcal{L}_{\text{iou}} + \lambda_{L1}||\cdot||_1$</blockquote><ul><li><p><strong>L1 손실:</strong> 예측 박스와 실제 박스의 좌표(x, y, w, h) 차이의 절댓값 합</p><ul><li>예) 고양이 박스와 예측 박스가 각각 (10,10,50,50), (12,11,48,52)라면, L1 손실은 $|10-12|+|10-11|+|50-48|+|50-52|=2+1+2+2=7$</li><li>단점: 큰 물체와 작은 물체에 동일한 픽셀 오차가 들어가면 불공평함</li></ul></li><li><p><strong>GIoU 손실:</strong> 두 박스가 얼마나 겹치는지(Intersection over Union, IoU)를 일반화한 값</p><ul><li>예) 고양이 박스와 예측 박스가 거의 겹치면 GIoU 손실이 매우 작음(정확)</li><li>장점: 박스 크기에 상관없이 공평하게 평가</li></ul></li><li><p><strong>최종 조합:</strong> GIoU 손실로 전체적인 위치와 크기 일치도를, L1 손실로 미세한 위치 조정을 동시에 학습</p></li></ul><hr><p>이처럼 DETR의 집합 예측 손실은</p><ul><li><strong>중복 예측을 방지</strong>하고,</li><li><strong>박스 크기에 상관없이 공평하게 평가</strong>하며,</li><li><strong>후처리(NMS) 없이도</strong> 정확한 객체 탐지가 가능하도록 설계되어 있습니다.</li></ul><hr><h3 id=3-실험-experiments>3. 실험 (Experiments)<a hidden class=anchor aria-hidden=true href=#3-실험-experiments>#</a></h3><p>DETR의 성능을 검증하기 위해 가장 널리 사용되는 COCO 데이터셋으로 다양한 실험을 진행했습니다.</p><p><img alt="Table 1: Faster R-CNN과의 성능 비교" loading=lazy src=/images/posts/detr/table1.png></p><p>이 표는 DETR과 기존의 강력한 모델인 Faster R-CNN의 성능을 비교합니다.</p><p>결론적으로, DETR은 매우 잘 최적화된 Faster R-CNN과 **비교할 만한 성능(AP 42.0 vs 42.0)**을 달성했습니다.
특히 주목할 점은, DETR이 **큰 물체에 대해서는 훨씬 뛰어난 성능($AP_L$ 61.1 vs 53.4)**을 보인다는 것입니다. 이는 트랜스포머 인코더가 이미지 전체의 넓은 문맥을 보기 때문에 가능한 것으로 분석됩니다. 반면, <strong>작은 물체에 대해서는 성능이 다소 낮게($AP_S$ 20.5 vs 26.6)</strong> 나타났습니다.</p><h4 id=주요-구성-요소-분석-ablation-studies>주요 구성 요소 분석 (Ablation Studies)<a hidden class=anchor aria-hidden=true href=#주요-구성-요소-분석-ablation-studies>#</a></h4><p>DETR의 어떤 부분이 성능에 얼마나 기여하는지 알아보기 위해 여러 실험을 진행했습니다.</p><p><strong>인코더의 중요성 (Table 2, Figure 3):</strong>
<img alt="Table 2: Ablation study on encoder layers" loading=lazy src=/images/posts/detr/table2.png>
인코더 층을 제거하자 전체 성능(AP)이 약 3.9점 하락했으며, 특히 큰 물체에 대한 성능이 6.0점이나 떨어졌습니다. 이는 이미지 전체의 맥락을 이해하는 인코더가 객체들을 서로 분리하는 데 중요한 역할을 함을 시사합니다.</p><p><img alt="Figure 3: 인코더 셀프 어텐션 시각화" loading=lazy src=/images/posts/detr/figure3.png>
Figure 3은 인코더의 셀프 어텐션이 어떻게 동작하는지 시각화한 것입니다. 그림 중앙의 소 이미지 위에 찍힌 빨간 점이 특정 위치를 의미하고, 주변의 작은 이미지들은 해당 위치가 이미지의 다른 어떤 부분에 주목(attention)하는지를 보여줍니다. 각기 다른 소들이 서로 다른 영역으로 분리되어 주목받는 것을 볼 수 있는데, 이는 인코더가 이미 개별 인스턴스들을 분리하고 있음을 보여줍니다.</p><p><strong>디코더의 중요성 (Figure 4):</strong>
<img alt="Figure 4: 디코더 층 수에 따른 성능 변화" loading=lazy src=/images/posts/detr/figure4.png>
이 그래프는 디코더 층이 깊어질수록 성능(AP)이 꾸준히 향상되는 것을 보여줍니다. 첫 번째 디코더 층의 결과와 마지막 층의 결과를 비교하면 AP가 8점 이상 크게 향상되었습니다.</p><p>흥미로운 점은, 첫 번째 디코더 층에서는 NMS 후처리를 적용하면 성능이 오르지만, 층이 깊어질수록 NMS의 효과가 줄어들고 마지막에는 오히려 성능을 약간 해친다는 것입니다. 이는 디코더의 셀프 어텐션이 스스로 중복 예측을 억제하는 법을 학습하기 때문에, DETR은 설계적으로 NMS가 필요 없다는 것을 실험적으로 증명합니다.</p><p><strong>그 외 요소들 (Table 3, 4):</strong>
<img alt="Table 3 & 4: 위치 인코딩 및 손실 함수 분석" loading=lazy src=/images/posts/detr/table3_4.png></p><ul><li><strong>위치 인코딩 (Table 3)</strong>: 공간적 위치 인코딩을 제거하자 성능이 7.8 AP나 하락하여, 이 요소가 매우 중요함을 확인했습니다.</li><li><strong>손실 함수 (Table 4)</strong>: L1 손실과 GIoU 손실 중 GIoU 손실이 성능에 거의 절대적인 기여를 하며, L1 손실은 보조적인 역할만 한다는 것을 발견했습니다.</li></ul><p><img alt="Figure 6: 디코더 동작 분석" loading=lazy src=/images/posts/detr/figure6.png>
<strong>Figure 6</strong>은 디코더가 각 물체를 예측할 때 이미지의 어느 부분을 주목하는지 보여줍니다. 코끼리나 얼룩말을 탐지할 때, 주로 머리, 다리 등 물체의 **윤곽을 결정하는 극단적인 부분(extremities)**에 주목하는 경향을 보입니다. 이는 인코더가 이미 &ldquo;여기에 얼룩말이 있다"고 알려주면, 디코더는 그 경계만 정확히 그리는 데 집중한다는 가설을 뒷받침합니다.</p><p><img alt="Figure 7: 디코더 동작 분석" loading=lazy src=/images/posts/detr/figure7.png>
<strong>Figure 7</strong>은 100개의 객체 쿼리(슬롯)가 각각 어떤 종류의 박스를 예측하도록 학습되는지 보여줍니다. 각 슬롯은 특정 **위치(area)와 크기(box size)**를 전담하도록 특화되는 경향을 보입니다. 예를 들어 어떤 슬롯은 이미지 중앙의 큰 물체를, 다른 슬롯은 왼쪽 하단의 작은 물체를 주로 찾도록 학습됩니다.</p><p><img alt="Figure 5: 일반화 능력 검증" loading=lazy src=/images/posts/detr/figure5.png>
학습 데이터에는 기린이 13마리 이상 있는 이미지가 없었습니다. 이 모델이 학습 데이터에 없는 상황에서도 잘 동작하는지 알아보기 위해, 인공적으로 기린 24마리가 있는 이미지를 만들어 테스트했습니다.</p><p>Figure 5에서 볼 수 있듯이, DETR은 놀랍게도 24마리의 기린을 모두 정확하게 찾아냈습니다. 이는 100개의 객체 쿼리가 특정 클래스(예: &lsquo;기린 전용 쿼리&rsquo;)에 과적합되지 않고, 일반적인 물체를 찾는 능력을 학습했음을 보여줍니다.</p><h4 id=panoptic-segmentation으로의-확장>Panoptic Segmentation으로의 확장<a hidden class=anchor aria-hidden=true href=#panoptic-segmentation으로의-확장>#</a></h4><p>Panoptic Segmentation은 이미지의 모든 픽셀을 &ldquo;어떤 물체에 속하는지(things)&rdquo; 또는 &ldquo;어떤 배경에 속하는지(stuff)&ldquo;로 구분하는 더 복잡한 작업입니다.</p><p><img alt="Figure 8: Panoptic Segmentation을 위한 DETR 확장" loading=lazy src=/images/posts/detr/figure8.png>
Figure 8은 DETR에 간단한 **마스크 헤드(mask head)**를 추가하여 이 작업을 수행하는 방법을 보여줍니다. 디코더의 출력 각각에 대해 해당 물체의 마스크를 예측하고, 이를 합쳐 최종 결과를 만듭니다.</p><p><img alt="Table 5: Panoptic Segmentation 성능 비교" loading=lazy src=/images/posts/detr/table5.png>
Table 5는 DETR이 PanopticFPN과 같은 기존의 강력한 모델들을 능가하는 성능을 보였음을 나타냅니다. 특히 배경(stuff) 클래스에서 강점을 보이는데, 이는 이미지 전체를 보는 인코더의 힘 덕분일 가능성이 높습니다.</p><p><img alt="Figure 9: Panoptic Segmentation 결과 예시" loading=lazy src=/images/posts/detr/figure9.png>
Figure 9는 DETR이 생성한 Panoptic Segmentation의 예시 이미지로, 물체와 배경 모두에 대해 깔끔한 결과를 보여줍니다.</p><hr><h3 id=4-결론-conclusion>4. 결론 (Conclusion)<a hidden class=anchor aria-hidden=true href=#4-결론-conclusion>#</a></h3><p>이 논문은 트랜스포머와 이분 매칭 손실을 기반으로 객체 탐지를 **&lsquo;직접적인 집합 예측 문제&rsquo;**로 풀어내는 새로운 패러다임인 <strong>DETR</strong>을 제시했습니다. DETR은 기존의 복잡한 파이프라인(앵커, NMS 등)을 제거하면서도, 고도로 최적화된 Faster R-CNN과 대등한 성능을 달성했습니다.</p><p>또한, DETR의 구조는 매우 간단하고 유연하여 Panoptic Segmentation과 같은 더 복잡한 문제로도 쉽게 확장될 수 있음을 보여주었습니다. 특히 이미지의 전역적인 정보를 활용하는 능력 덕분에 큰 객체 탐지에서 뛰어난 성능을 보였습니다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mookjsi.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://mookjsi.github.io/tags/object-detection/>Object Detection</a></li><li><a href=https://mookjsi.github.io/tags/transformer/>Transformer</a></li><li><a href=https://mookjsi.github.io/tags/detr/>DETR</a></li><li><a href=https://mookjsi.github.io/tags/end-to-end/>End-to-End</a></li><li><a href=https://mookjsi.github.io/tags/eccv-2020/>ECCV 2020</a></li></ul><nav class=paginav><a class=next href=https://mookjsi.github.io/posts/paper-review-resnet/><span class=title>Next »</span><br><span>ResNet - 더 깊은 신경망을 위한 잔차 학습</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on x" href="https://x.com/intent/tweet/?text=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f&amp;hashtags=PaperReview%2cDeepLearning%2cComputerVision%2cObjectDetection%2cTransformer%2cDETR%2cEnd-to-End%2cECCV2020"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f&amp;title=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers&amp;summary=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f&title=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on whatsapp" href="https://api.whatsapp.com/send?text=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on telegram" href="https://telegram.me/share/url?text=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DETR: End-to-End Object Detection with Transformers on ycombinator" href="https://news.ycombinator.com/submitlink?t=DETR%3a%20End-to-End%20Object%20Detection%20with%20Transformers&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-detr%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>