<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>My Paper Review: Promptriever - Instruction-Trained Retrievers | MookStudy</title><meta name=keywords content="Paper Review,LLM,RAG,Retrieval,ICLR 2025"><meta name=description content="Here&rsquo;s my full 35-slide review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that&rsquo;s trained to follow natural language instructions."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-promptriever/><link crossorigin=anonymous href=/assets/css/stylesheet.11ee013dd5a386759d3b4c965ae95ae1ca0f4ee553d0b1703ffeb46d15507aee.css integrity="sha256-Ee4BPdWjhnWdO0yWWula4coPTuVT0LFwP/60bRVQeu4=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-promptriever/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-promptriever/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="My Paper Review: Promptriever - Instruction-Trained Retrievers"><meta property="og:description" content="Here’s my full 35-slide review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that’s trained to follow natural language instructions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-12T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RAG"><meta property="article:tag" content="Retrieval"><meta property="article:tag" content="ICLR 2025"><meta name=twitter:card content="summary"><meta name=twitter:title content="My Paper Review: Promptriever - Instruction-Trained Retrievers"><meta name=twitter:description content="Here&rsquo;s my full 35-slide review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that&rsquo;s trained to follow natural language instructions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"My Paper Review: Promptriever - Instruction-Trained Retrievers","item":"https://mookjsi.github.io/posts/paper-review-promptriever/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"My Paper Review: Promptriever - Instruction-Trained Retrievers","name":"My Paper Review: Promptriever - Instruction-Trained Retrievers","description":"Here\u0026rsquo;s my full 35-slide review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","keywords":["Paper Review","LLM","RAG","Retrieval","ICLR 2025"],"articleBody":"[cite_start]I’m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025. [cite: 65]\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like “not A, but B”). The authors’ solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nLet’s go through my 35 slides to see how they did it.\nSlide 1: Title Slide [cite_start]This is my presentation on “Promptriever: Instruction-Trained Retrievers,” which I put together for the Information Theory and Machine Learning Lab. [cite: 58, 59, 61, 62, 63, 64]\nSlide 2: Authors and Affiliations (1) First, let’s acknowledge the researchers. [cite_start]The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. [cite: 67, 68, 69, 70] [cite_start]It’s also worth noting this work was accepted as a poster at ICLR 2025. [cite: 65, 66]\nSlide 3: The Core Message I want to start with the paper’s core claim, which I think is really powerful. [cite_start]The authors state that their new training method is the first to prove that search models can be “intelligent, instruction-following partners, not just data finders.” [cite: 71, 72]\nSlide 4: Authors and Affiliations (2) [cite_start]Here are the other co-authors who contributed to this research. [cite: 75, 76, 78, 79, 81]\nSlide 5: Table of Contents (Motivation) I’ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. [cite_start]We’ll start with the motivation behind the research. [cite: 73, 74, 77, 80]\nSlide 6: The Problem with Modern Search So, how do current search engines “think”? [cite_start]They mostly use a retriever based on semantic similarity to rank documents. [cite: 84, 86, 87, 89] [cite_start]On the surface, this seems fine, but what’s the problem? [cite: 88]\nSlide 7: A Concrete Example of the Problem This example makes the problem obvious. [cite_start]Imagine you need a laptop that is not a MacBook and costs under $1000. [cite: 92] [cite_start]A standard retriever sees the keywords “MacBook” and “under $1000” in an article about the MacBook Air and incorrectly flags it as highly relevant. [cite: 93, 94, 95, 96, 97, 98, 99, 100, 101]\nSlide 8: The Flawed User Experience This leads to a frustrating user experience. [cite_start]You’re forced to keep tweaking keywords and filters just to find what you want. [cite: 109, 110, 111, 112]\nSlide 9: The Solution: How Promptriever Thinks This is where Promptriever comes in. It doesn’t just use semantic similarity. [cite_start]Instead, it operates on “dynamic relevance definitions,” which allows for a much more intelligent process. [cite: 113, 114, 117, 118]\nSlide 10: Promptriever in Action Let’s look at the same query again, but with Promptriever. [cite_start]It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. [cite: 122, 123, 124] [cite_start]Because of this, it successfully returns a relevant document about a Dell XPS 13. [cite: 125, 126, 127]\nSlide 11: The Power of Dynamic Relevance [cite_start]The key idea here is that Promptriever “dynamically adjusts relevance based on your natural language instructions.” [cite: 133] It’s not just matching words; it’s understanding commands.\nSlide 12: The Crucial Question [cite_start]We’ve seen what it does, which leads to the next question: “But how on earth was this made?” [cite: 137] Let’s get into the technical details.\nSlide 13: Table of Contents (Promptriever) [cite_start]Now, we’ll dive into the second section, where I’ll break down the Promptriever model’s architecture and training. [cite: 138]\nSlide 14: The Components of Promptriever [cite_start]The architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder. [cite: 139, 140, 141, 142, 143]\nSlide 15: The Core Technical Challenge [cite_start]The main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model’s instruction-following ability. [cite: 149, 150] So how did they keep the model intelligent?\nSlide 16: The Key to the Solution [cite_start]The answer, as they stated in their core message, lies in their “novel training data, which makes ignoring commands impossible for correct answers.” [cite: 157]\nSlide 17: Traditional Training Data [cite_start]For context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO. [cite: 158, 159, 160, 161]\nSlide 18: Promptriever’s Advanced Training Data [cite_start]Promptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. [cite: 165, 182] [cite_start]This is what lets them train the model on complex, instruction-based prompts. [cite: 176, 180]\nSlide 19: The “Instruction-Negative” Concept The most clever part of their training data is the Instruction-Negative. [cite_start]This is a document that’s correct for the query alone, but becomes incorrect when the instruction is added. [cite: 189, 190] This is what forces the model to pay attention.\nSlide 20: Example of an Instruction-Negative Here’s a perfect example. [cite_start]For the query “What is the capital of France?,” a general article about Paris is a good result. [cite: 216] [cite_start]But if you add the instruction “mention its average annual rainfall,” that article is now an instruction-negative, and a new document with rainfall data becomes the right answer. [cite: 212, 217]\nSlide 21: How the Training Works [cite_start]Through this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. [cite: 226] [cite_start]To get the right answer, it has to carefully read and follow the command. [cite: 227, 228]\nSlide 22: Ensuring Dataset Quality (1) The authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. [cite_start]For those cases, they used an LLM to generate a new, correct document as a substitute. [cite: 237, 238, 239, 240]\nSlide 23: Ensuring Dataset Quality (2) Creating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. [cite_start]The negatives guarantee true instruction-following. [cite: 249, 250, 251]\nSlide 24: Table of Contents (Experiments) [cite_start]Now for the final section, “Experiments,” where we’ll look at the results. [cite: 252]\nSlide 25: Experiment Settings (1) [cite_start]For a fair comparison, they ran an “Apples-to-Apples” test against RepLLaMA, using the exact same data and hyperparameters. [cite: 254, 255, 256, 257, 258]\nSlide 26: Experiment Settings (2) [cite_start]They used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions. [cite: 260, 265, 266, 267, 270, 272]\nSlide 27: Summary of Results ![Slide 27](/images/posts/promptriever/slide-27.jpg\u003e\n[cite_start]The results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting. [cite: 274, 275, 276, 277, 278]\nSlide 28: Detailed Results (Table 1) This table gives a detailed breakdown. [cite_start]You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability. [cite: 279, 280]\nSlide 29: In-Domain Performance On the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nSlide 30: Out-of-Domain Performance (BEIR) This is where it gets interesting. When given a helpful prompt, Promptriever’s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely “promptable.”\nSlide 31: Robustness Results This table looks at the standard deviation of scores across different prompts. Promptriever’s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nSlide 32: Ablation Study The ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nSlide 33: Generalization to Other Models The authors also proved their training “recipe” is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, “A golden recipe doesn’t discriminate against ingredients!”\nSlide 34: Rebuttal (1) Finally, let’s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper’s scope.\nSlide 35: Rebuttal (2) In the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","wordCount":"1400","inLanguage":"en","datePublished":"2025-06-12T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-promptriever/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">My Paper Review: Promptriever - Instruction-Trained Retrievers</h1><div class=post-meta><span title='2025-06-12 00:00:00 +0000 UTC'>June 12, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1400 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>[cite_start]I&rsquo;m sharing my full slide-by-slide review of the paper <strong>Promptriever: Instruction-Trained Retrievers</strong>, which was presented at ICLR 2025. [cite: 65]</p><p>The paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like &ldquo;not A, but B&rdquo;). The authors&rsquo; solution is <strong>Promptriever</strong>, a new model trained with a special dataset that forces it to actually follow instructions.</p><p>Let&rsquo;s go through my 35 slides to see how they did it.</p><hr><h3 id=slide-1-title-slide>Slide 1: Title Slide<a hidden class=anchor aria-hidden=true href=#slide-1-title-slide>#</a></h3><p><img alt="Slide 1" loading=lazy src=/images/posts/promptriever/slide-01.jpg></p><blockquote><p>[cite_start]This is my presentation on &ldquo;Promptriever: Instruction-Trained Retrievers,&rdquo; which I put together for the Information Theory and Machine Learning Lab. [cite: 58, 59, 61, 62, 63, 64]</p></blockquote><h3 id=slide-2-authors-and-affiliations-1>Slide 2: Authors and Affiliations (1)<a hidden class=anchor aria-hidden=true href=#slide-2-authors-and-affiliations-1>#</a></h3><p><img alt="Slide 2" loading=lazy src=/images/posts/promptriever/slide-02.jpg></p><blockquote><p>First, let&rsquo;s acknowledge the researchers. [cite_start]The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. [cite: 67, 68, 69, 70] [cite_start]It&rsquo;s also worth noting this work was accepted as a poster at ICLR 2025. [cite: 65, 66]</p></blockquote><h3 id=slide-3-the-core-message>Slide 3: The Core Message<a hidden class=anchor aria-hidden=true href=#slide-3-the-core-message>#</a></h3><p><img alt="Slide 3" loading=lazy src=/images/posts/promptriever/slide-03.jpg></p><blockquote><p>I want to start with the paper&rsquo;s core claim, which I think is really powerful. [cite_start]The authors state that their new training method is the first to prove that search models can be &ldquo;intelligent, instruction-following partners, not just data finders.&rdquo; [cite: 71, 72]</p></blockquote><h3 id=slide-4-authors-and-affiliations-2>Slide 4: Authors and Affiliations (2)<a hidden class=anchor aria-hidden=true href=#slide-4-authors-and-affiliations-2>#</a></h3><p><img alt="Slide 4" loading=lazy src=/images/posts/promptriever/slide-04.jpg></p><blockquote><p>[cite_start]Here are the other co-authors who contributed to this research. [cite: 75, 76, 78, 79, 81]</p></blockquote><h3 id=slide-5-table-of-contents-motivation>Slide 5: Table of Contents (Motivation)<a hidden class=anchor aria-hidden=true href=#slide-5-table-of-contents-motivation>#</a></h3><p><img alt="Slide 5" loading=lazy src=/images/posts/promptriever/slide-05.jpg></p><blockquote><p>I&rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. [cite_start]We&rsquo;ll start with the motivation behind the research. [cite: 73, 74, 77, 80]</p></blockquote><h3 id=slide-6-the-problem-with-modern-search>Slide 6: The Problem with Modern Search<a hidden class=anchor aria-hidden=true href=#slide-6-the-problem-with-modern-search>#</a></h3><p><img alt="Slide 6" loading=lazy src=/images/posts/promptriever/slide-06.jpg></p><blockquote><p>So, how do current search engines &ldquo;think&rdquo;? [cite_start]They mostly use a retriever based on semantic similarity to rank documents. [cite: 84, 86, 87, 89] [cite_start]On the surface, this seems fine, but what&rsquo;s the problem? [cite: 88]</p></blockquote><h3 id=slide-7-a-concrete-example-of-the-problem>Slide 7: A Concrete Example of the Problem<a hidden class=anchor aria-hidden=true href=#slide-7-a-concrete-example-of-the-problem>#</a></h3><p><img alt="Slide 7" loading=lazy src=/images/posts/promptriever/slide-07.jpg></p><blockquote><p>This example makes the problem obvious. [cite_start]Imagine you need a laptop that is <strong>not</strong> a MacBook and costs under $1000. [cite: 92] [cite_start]A standard retriever sees the keywords &ldquo;MacBook&rdquo; and &ldquo;under $1000&rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant. [cite: 93, 94, 95, 96, 97, 98, 99, 100, 101]</p></blockquote><h3 id=slide-8-the-flawed-user-experience>Slide 8: The Flawed User Experience<a hidden class=anchor aria-hidden=true href=#slide-8-the-flawed-user-experience>#</a></h3><p><img alt="Slide 8" loading=lazy src=/images/posts/promptriever/slide-08.jpg></p><blockquote><p>This leads to a frustrating user experience. [cite_start]You&rsquo;re forced to keep tweaking keywords and filters just to find what you want. [cite: 109, 110, 111, 112]</p></blockquote><h3 id=slide-9-the-solution-how-promptriever-thinks>Slide 9: The Solution: How Promptriever Thinks<a hidden class=anchor aria-hidden=true href=#slide-9-the-solution-how-promptriever-thinks>#</a></h3><p><img alt="Slide 9" loading=lazy src=/images/posts/promptriever/slide-09.jpg></p><blockquote><p>This is where Promptriever comes in. It doesn&rsquo;t just use semantic similarity. [cite_start]Instead, it operates on &ldquo;dynamic relevance definitions,&rdquo; which allows for a much more intelligent process. [cite: 113, 114, 117, 118]</p></blockquote><h3 id=slide-10-promptriever-in-action>Slide 10: Promptriever in Action<a hidden class=anchor aria-hidden=true href=#slide-10-promptriever-in-action>#</a></h3><p><img alt="Slide 10" loading=lazy src=/images/posts/promptriever/slide-10.jpg></p><blockquote><p>Let&rsquo;s look at the same query again, but with Promptriever. [cite_start]It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. [cite: 122, 123, 124] [cite_start]Because of this, it successfully returns a relevant document about a Dell XPS 13. [cite: 125, 126, 127]</p></blockquote><h3 id=slide-11-the-power-of-dynamic-relevance>Slide 11: The Power of Dynamic Relevance<a hidden class=anchor aria-hidden=true href=#slide-11-the-power-of-dynamic-relevance>#</a></h3><p><img alt="Slide 11" loading=lazy src=/images/posts/promptriever/slide-11.jpg></p><blockquote><p>[cite_start]The key idea here is that Promptriever &ldquo;dynamically adjusts relevance based on your natural language instructions.&rdquo; [cite: 133] It&rsquo;s not just matching words; it&rsquo;s understanding commands.</p></blockquote><h3 id=slide-12-the-crucial-question>Slide 12: The Crucial Question<a hidden class=anchor aria-hidden=true href=#slide-12-the-crucial-question>#</a></h3><p><img alt="Slide 12" loading=lazy src=/images/posts/promptriever/slide-12.jpg></p><blockquote><p>[cite_start]We&rsquo;ve seen <em>what</em> it does, which leads to the next question: &ldquo;But how on earth was this made?&rdquo; [cite: 137] Let&rsquo;s get into the technical details.</p></blockquote><h3 id=slide-13-table-of-contents-promptriever>Slide 13: Table of Contents (Promptriever)<a hidden class=anchor aria-hidden=true href=#slide-13-table-of-contents-promptriever>#</a></h3><p><img alt="Slide 13" loading=lazy src=/images/posts/promptriever/slide-13.jpg></p><blockquote><p>[cite_start]Now, we&rsquo;ll dive into the second section, where I&rsquo;ll break down the Promptriever model&rsquo;s architecture and training. [cite: 138]</p></blockquote><h3 id=slide-14-the-components-of-promptriever>Slide 14: The Components of Promptriever<a hidden class=anchor aria-hidden=true href=#slide-14-the-components-of-promptriever>#</a></h3><p><img alt="Slide 14" loading=lazy src=/images/posts/promptriever/slide-14.jpg></p><blockquote><p>[cite_start]The architecture is a combination of the <strong>LLaMA-2 7B</strong> language model and a <strong>Bi-encoder</strong>. [cite: 139, 140, 141, 142, 143]</p></blockquote><h3 id=slide-15-the-core-technical-challenge>Slide 15: The Core Technical Challenge<a hidden class=anchor aria-hidden=true href=#slide-15-the-core-technical-challenge>#</a></h3><p><img alt="Slide 15" loading=lazy src=/images/posts/promptriever/slide-15.jpg></p><blockquote><p>[cite_start]The main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model&rsquo;s instruction-following ability. [cite: 149, 150] So how did they keep the model intelligent?</p></blockquote><h3 id=slide-16-the-key-to-the-solution>Slide 16: The Key to the Solution<a hidden class=anchor aria-hidden=true href=#slide-16-the-key-to-the-solution>#</a></h3><p><img alt="Slide 16" loading=lazy src=/images/posts/promptriever/slide-16.jpg></p><blockquote><p>[cite_start]The answer, as they stated in their core message, lies in their &ldquo;novel training data, which makes ignoring commands impossible for correct answers.&rdquo; [cite: 157]</p></blockquote><h3 id=slide-17-traditional-training-data>Slide 17: Traditional Training Data<a hidden class=anchor aria-hidden=true href=#slide-17-traditional-training-data>#</a></h3><p><img alt="Slide 17" loading=lazy src=/images/posts/promptriever/slide-17.jpg></p><blockquote><p>[cite_start]For context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO. [cite: 158, 159, 160, 161]</p></blockquote><h3 id=slide-18-promptrievers-advanced-training-data>Slide 18: Promptriever&rsquo;s Advanced Training Data<a hidden class=anchor aria-hidden=true href=#slide-18-promptrievers-advanced-training-data>#</a></h3><p><img alt="Slide 18" loading=lazy src=/images/posts/promptriever/slide-18.jpg></p><blockquote><p>[cite_start]Promptriever, however, uses a much richer format: <code>Query + Instruction</code> paired with <code>Synthetic documents</code>. [cite: 165, 182] [cite_start]This is what lets them train the model on complex, instruction-based prompts. [cite: 176, 180]</p></blockquote><h3 id=slide-19-the-instruction-negative-concept>Slide 19: The &ldquo;Instruction-Negative&rdquo; Concept<a hidden class=anchor aria-hidden=true href=#slide-19-the-instruction-negative-concept>#</a></h3><p><img alt="Slide 19" loading=lazy src=/images/posts/promptriever/slide-19.jpg></p><blockquote><p>The most clever part of their training data is the <strong>Instruction-Negative</strong>. [cite_start]This is a document that&rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. [cite: 189, 190] This is what forces the model to pay attention.</p></blockquote><h3 id=slide-20-example-of-an-instruction-negative>Slide 20: Example of an Instruction-Negative<a hidden class=anchor aria-hidden=true href=#slide-20-example-of-an-instruction-negative>#</a></h3><p><img alt="Slide 20" loading=lazy src=/images/posts/promptriever/slide-20.jpg></p><blockquote><p>Here’s a perfect example. [cite_start]For the query &ldquo;What is the capital of France?,&rdquo; a general article about Paris is a good result. [cite: 216] [cite_start]But if you add the instruction &ldquo;mention its average annual rainfall,&rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer. [cite: 212, 217]</p></blockquote><h3 id=slide-21-how-the-training-works>Slide 21: How the Training Works<a hidden class=anchor aria-hidden=true href=#slide-21-how-the-training-works>#</a></h3><p><img alt="Slide 21" loading=lazy src=/images/posts/promptriever/slide-21.jpg></p><blockquote><p>[cite_start]Through this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. [cite: 226] [cite_start]To get the right answer, it has to carefully read and follow the command. [cite: 227, 228]</p></blockquote><h3 id=slide-22-ensuring-dataset-quality-1>Slide 22: Ensuring Dataset Quality (1)<a hidden class=anchor aria-hidden=true href=#slide-22-ensuring-dataset-quality-1>#</a></h3><p><img alt="Slide 22" loading=lazy src=/images/posts/promptriever/slide-22.jpg></p><blockquote><p>The authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. [cite_start]For those cases, they used an LLM to generate a new, correct document as a substitute. [cite: 237, 238, 239, 240]</p></blockquote><h3 id=slide-23-ensuring-dataset-quality-2>Slide 23: Ensuring Dataset Quality (2)<a hidden class=anchor aria-hidden=true href=#slide-23-ensuring-dataset-quality-2>#</a></h3><p><img alt="Slide 23" loading=lazy src=/images/posts/promptriever/slide-23.jpg></p><blockquote><p>Creating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. [cite_start]The negatives guarantee true instruction-following. [cite: 249, 250, 251]</p></blockquote><h3 id=slide-24-table-of-contents-experiments>Slide 24: Table of Contents (Experiments)<a hidden class=anchor aria-hidden=true href=#slide-24-table-of-contents-experiments>#</a></h3><p><img alt="Slide 24" loading=lazy src=/images/posts/promptriever/slide-24.jpg></p><blockquote><p>[cite_start]Now for the final section, &ldquo;Experiments,&rdquo; where we&rsquo;ll look at the results. [cite: 252]</p></blockquote><h3 id=slide-25-experiment-settings-1>Slide 25: Experiment Settings (1)<a hidden class=anchor aria-hidden=true href=#slide-25-experiment-settings-1>#</a></h3><p><img alt="Slide 25" loading=lazy src=/images/posts/promptriever/slide-25.jpg></p><blockquote><p>[cite_start]For a fair comparison, they ran an &ldquo;Apples-to-Apples&rdquo; test against RepLLaMA, using the exact same data and hyperparameters. [cite: 254, 255, 256, 257, 258]</p></blockquote><h3 id=slide-26-experiment-settings-2>Slide 26: Experiment Settings (2)<a hidden class=anchor aria-hidden=true href=#slide-26-experiment-settings-2>#</a></h3><p><img alt="Slide 26" loading=lazy src=/images/posts/promptriever/slide-26.jpg></p><blockquote><p>[cite_start]They used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, <strong>p-MRR</strong>, which is designed to measure sensitivity to instructions. [cite: 260, 265, 266, 267, 270, 272]</p></blockquote><h3 id=slide-27-summary-of-results>Slide 27: Summary of Results<a hidden class=anchor aria-hidden=true href=#slide-27-summary-of-results>#</a></h3><p>![Slide 27](/images/posts/promptriever/slide-27.jpg></p><blockquote><p>[cite_start]The results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting. [cite: 274, 275, 276, 277, 278]</p></blockquote><h3 id=slide-28-detailed-results-table-1>Slide 28: Detailed Results (Table 1)<a hidden class=anchor aria-hidden=true href=#slide-28-detailed-results-table-1>#</a></h3><p><img alt="Slide 28" loading=lazy src=/images/posts/promptriever/slide-28.jpg></p><blockquote><p>This table gives a detailed breakdown. [cite_start]You can see that Promptriever gets high scores across the board, but it really shines in the <strong>p-MRR</strong> metric, which confirms its superior instruction-following ability. [cite: 279, 280]</p></blockquote><h3 id=slide-29-in-domain-performance>Slide 29: In-Domain Performance<a hidden class=anchor aria-hidden=true href=#slide-29-in-domain-performance>#</a></h3><p><img alt="Slide 29" loading=lazy src=/images/posts/promptriever/slide-29.jpg></p><blockquote><p>On the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.</p></blockquote><h3 id=slide-30-out-of-domain-performance-beir>Slide 30: Out-of-Domain Performance (BEIR)<a hidden class=anchor aria-hidden=true href=#slide-30-out-of-domain-performance-beir>#</a></h3><p><img alt="Slide 30" loading=lazy src=/images/posts/promptriever/slide-30.jpg></p><blockquote><p>This is where it gets interesting. When given a helpful prompt, Promptriever&rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely &ldquo;promptable.&rdquo;</p></blockquote><h3 id=slide-31-robustness-results>Slide 31: Robustness Results<a hidden class=anchor aria-hidden=true href=#slide-31-robustness-results>#</a></h3><p><img alt="Slide 31" loading=lazy src=/images/posts/promptriever/slide-31.jpg></p><blockquote><p>This table looks at the standard deviation of scores across different prompts. Promptriever&rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.</p></blockquote><h3 id=slide-32-ablation-study>Slide 32: Ablation Study<a hidden class=anchor aria-hidden=true href=#slide-32-ablation-study>#</a></h3><p><img alt="Slide 32" loading=lazy src=/images/posts/promptriever/slide-32.jpg></p><blockquote><p>The ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.</p></blockquote><h3 id=slide-33-generalization-to-other-models>Slide 33: Generalization to Other Models<a hidden class=anchor aria-hidden=true href=#slide-33-generalization-to-other-models>#</a></h3><p><img alt="Slide 33" loading=lazy src=/images/posts/promptriever/slide-33.jpg></p><blockquote><p>The authors also proved their training &ldquo;recipe&rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, &ldquo;A golden recipe doesn&rsquo;t discriminate against ingredients!&rdquo;</p></blockquote><h3 id=slide-34-rebuttal-1>Slide 34: Rebuttal (1)<a hidden class=anchor aria-hidden=true href=#slide-34-rebuttal-1>#</a></h3><p><img alt="Slide 34" loading=lazy src=/images/posts/promptriever/slide-34.jpg></p><blockquote><p>Finally, let&rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method <em>is</em> their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper&rsquo;s scope.</p></blockquote><h3 id=slide-35-rebuttal-2>Slide 35: Rebuttal (2)<a hidden class=anchor aria-hidden=true href=#slide-35-rebuttal-2>#</a></h3><p><img alt="Slide 35" loading=lazy src=/images/posts/promptriever/slide-35.jpg></p><blockquote><p>In the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/llm/>LLM</a></li><li><a href=https://mookjsi.github.io/tags/rag/>RAG</a></li><li><a href=https://mookjsi.github.io/tags/retrieval/>Retrieval</a></li><li><a href=https://mookjsi.github.io/tags/iclr-2025/>ICLR 2025</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on x" href="https://x.com/intent/tweet/?text=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&amp;hashtags=PaperReview%2cLLM%2cRAG%2cRetrieval%2cICLR2025"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&amp;title=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers&amp;summary=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&title=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on whatsapp" href="https://api.whatsapp.com/send?text=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on telegram" href="https://telegram.me/share/url?text=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My Paper Review: Promptriever - Instruction-Trained Retrievers on ycombinator" href="https://news.ycombinator.com/submitlink?t=My%20Paper%20Review%3a%20Promptriever%20-%20Instruction-Trained%20Retrievers&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>