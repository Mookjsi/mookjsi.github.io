<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Promptriever - Instruction-Trained Retrievers | MookStudy</title><meta name=keywords content="Paper Review,LLM,RAG,Retrieval,ICLR 2025"><meta name=description content="A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that&rsquo;s trained to follow natural language instructions."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-promptriever/><link crossorigin=anonymous href=/assets/css/stylesheet.11ee013dd5a386759d3b4c965ae95ae1ca0f4ee553d0b1703ffeb46d15507aee.css integrity="sha256-Ee4BPdWjhnWdO0yWWula4coPTuVT0LFwP/60bRVQeu4=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-promptriever/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-promptriever/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="Promptriever - Instruction-Trained Retrievers"><meta property="og:description" content="A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that’s trained to follow natural language instructions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-12T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RAG"><meta property="article:tag" content="Retrieval"><meta property="article:tag" content="ICLR 2025"><meta name=twitter:card content="summary"><meta name=twitter:title content="Promptriever - Instruction-Trained Retrievers"><meta name=twitter:description content="A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that&rsquo;s trained to follow natural language instructions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Promptriever - Instruction-Trained Retrievers","item":"https://mookjsi.github.io/posts/paper-review-promptriever/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Promptriever - Instruction-Trained Retrievers","name":"Promptriever - Instruction-Trained Retrievers","description":"A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","keywords":["Paper Review","LLM","RAG","Retrieval","ICLR 2025"],"articleBody":"I’m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025.\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like “not A, but B”). The authors’ solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nThis is my presentation on “Promptriever: Instruction-Trained Retrievers,” which I put together for the Information Theory and Machine Learning Lab.\nFirst, let’s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It’s also worth noting this work was accepted as a poster at ICLR 2025.\nI want to start with the paper’s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be “intelligent, instruction-following partners, not just data finders.”\nHere are the other co-authors who contributed to this research.\nI’ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We’ll start with the motivation behind the research.\nSo, how do current search engines “think”? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what’s the problem?\nThis example makes the problem obvious. Imagine you need a laptop that is not a MacBook and costs under $1000. A standard retriever sees the keywords “MacBook” and “under $1000” in an article about the MacBook Air and incorrectly flags it as highly relevant.\nThis leads to a frustrating user experience. You’re forced to keep tweaking keywords and filters just to find what you want.\nThis is where Promptriever comes in. It doesn’t just use semantic similarity. Instead, it operates on “dynamic relevance definitions,” which allows for a much more intelligent process.\nLet’s look at the same query again, but with Promptriever. It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.\nThe key idea here is that Promptriever “dynamically adjusts relevance based on your natural language instructions.” It’s not just matching words; it’s understanding commands.\nWe’ve seen what it does, which leads to the next question: “But how on earth was this made?” Let’s get into the technical details.\nNow, we’ll dive into the second section, where I’ll break down the Promptriever model’s architecture and training.\nThe architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder.\nThe main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model’s instruction-following ability. So how did they keep the model intelligent?\nThe answer, as they stated in their core message, lies in their “novel training data, which makes ignoring commands impossible for correct answers.”\nFor context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.\nPromptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. This is what lets them train the model on complex, instruction-based prompts.\nThe most clever part of their training data is the Instruction-Negative. This is a document that’s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.\nHere’s a perfect example. For the query “What is the capital of France?,” a general article about Paris is a good result. But if you add the instruction “mention its average annual rainfall,” that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.\nThrough this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.\nThe authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.\nCreating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.\nNow for the final section, “Experiments,” where we’ll look at the results.\nFor a fair comparison, they ran an “Apples-to-Apples” test against RepLLaMA, using the exact same data and hyperparameters.\nThey used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions.\nThe results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.\nThis table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability.\nOn the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nThis is where it gets interesting. When given a helpful prompt, Promptriever’s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely “promptable.”\nThis table looks at the standard deviation of scores across different prompts. Promptriever’s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nThe ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nThe authors also proved their training “recipe” is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, “A golden recipe doesn’t discriminate against ingredients!”\nFinally, let’s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper’s scope.\nIn the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","wordCount":"1049","inLanguage":"en","datePublished":"2025-06-12T00:00:00Z","dateModified":"2025-06-12T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-promptriever/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">Promptriever - Instruction-Trained Retrievers</h1><div class=post-meta><span title='2025-06-12 00:00:00 +0000 UTC'>June 12, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1049 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><p>I&rsquo;m sharing my full slide-by-slide review of the paper <strong>Promptriever: Instruction-Trained Retrievers</strong>, which was presented at ICLR 2025.</p><p>The paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like &ldquo;not A, but B&rdquo;). The authors&rsquo; solution is <strong>Promptriever</strong>, a new model trained with a special dataset that forces it to actually follow instructions.</p><hr><p><img alt="Slide 1" loading=lazy src=/images/posts/promptriever/slide-01.jpg></p><blockquote><p>This is my presentation on &ldquo;Promptriever: Instruction-Trained Retrievers,&rdquo; which I put together for the Information Theory and Machine Learning Lab.</p></blockquote><p><img alt="Slide 2" loading=lazy src=/images/posts/promptriever/slide-02.jpg></p><blockquote><p>First, let&rsquo;s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It&rsquo;s also worth noting this work was accepted as a poster at ICLR 2025.</p></blockquote><p><img alt="Slide 3" loading=lazy src=/images/posts/promptriever/slide-03.jpg></p><blockquote><p>I want to start with the paper&rsquo;s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be &ldquo;intelligent, instruction-following partners, not just data finders.&rdquo;</p></blockquote><p><img alt="Slide 4" loading=lazy src=/images/posts/promptriever/slide-04.jpg></p><blockquote><p>Here are the other co-authors who contributed to this research.</p></blockquote><p><img alt="Slide 5" loading=lazy src=/images/posts/promptriever/slide-05.jpg></p><blockquote><p>I&rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We&rsquo;ll start with the motivation behind the research.</p></blockquote><p><img alt="Slide 6" loading=lazy src=/images/posts/promptriever/slide-06.jpg></p><blockquote><p>So, how do current search engines &ldquo;think&rdquo;? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what&rsquo;s the problem?</p></blockquote><p><img alt="Slide 7" loading=lazy src=/images/posts/promptriever/slide-07.jpg></p><blockquote><p>This example makes the problem obvious. Imagine you need a laptop that is <strong>not</strong> a MacBook and costs under $1000. A standard retriever sees the keywords &ldquo;MacBook&rdquo; and &ldquo;under $1000&rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant.</p></blockquote><p><img alt="Slide 8" loading=lazy src=/images/posts/promptriever/slide-08.jpg></p><blockquote><p>This leads to a frustrating user experience. You&rsquo;re forced to keep tweaking keywords and filters just to find what you want.</p></blockquote><p><img alt="Slide 9" loading=lazy src=/images/posts/promptriever/slide-09.jpg></p><blockquote><p>This is where Promptriever comes in. It doesn&rsquo;t just use semantic similarity. Instead, it operates on &ldquo;dynamic relevance definitions,&rdquo; which allows for a much more intelligent process.</p></blockquote><p><img alt="Slide 10" loading=lazy src=/images/posts/promptriever/slide-10.jpg></p><blockquote><p>Let&rsquo;s look at the same query again, but with Promptriever. It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.</p></blockquote><p><img alt="Slide 11" loading=lazy src=/images/posts/promptriever/slide-11.jpg></p><blockquote><p>The key idea here is that Promptriever &ldquo;dynamically adjusts relevance based on your natural language instructions.&rdquo; It&rsquo;s not just matching words; it&rsquo;s understanding commands.</p></blockquote><p><img alt="Slide 12" loading=lazy src=/images/posts/promptriever/slide-12.jpg></p><blockquote><p>We&rsquo;ve seen <em>what</em> it does, which leads to the next question: &ldquo;But how on earth was this made?&rdquo; Let&rsquo;s get into the technical details.</p></blockquote><p><img alt="Slide 13" loading=lazy src=/images/posts/promptriever/slide-13.jpg></p><blockquote><p>Now, we&rsquo;ll dive into the second section, where I&rsquo;ll break down the Promptriever model&rsquo;s architecture and training.</p></blockquote><p><img alt="Slide 14" loading=lazy src=/images/posts/promptriever/slide-14.jpg></p><blockquote><p>The architecture is a combination of the <strong>LLaMA-2 7B</strong> language model and a <strong>Bi-encoder</strong>.</p></blockquote><p><img alt="Slide 15" loading=lazy src=/images/posts/promptriever/slide-15.jpg></p><blockquote><p>The main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model&rsquo;s instruction-following ability. So how did they keep the model intelligent?</p></blockquote><p><img alt="Slide 16" loading=lazy src=/images/posts/promptriever/slide-16.jpg></p><blockquote><p>The answer, as they stated in their core message, lies in their &ldquo;novel training data, which makes ignoring commands impossible for correct answers.&rdquo;</p></blockquote><p><img alt="Slide 17" loading=lazy src=/images/posts/promptriever/slide-17.jpg></p><blockquote><p>For context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.</p></blockquote><p><img alt="Slide 18" loading=lazy src=/images/posts/promptriever/slide-18.jpg></p><blockquote><p>Promptriever, however, uses a much richer format: <code>Query + Instruction</code> paired with <code>Synthetic documents</code>. This is what lets them train the model on complex, instruction-based prompts.</p></blockquote><p><img alt="Slide 19" loading=lazy src=/images/posts/promptriever/slide-19.jpg></p><blockquote><p>The most clever part of their training data is the <strong>Instruction-Negative</strong>. This is a document that&rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.</p></blockquote><p><img alt="Slide 20" loading=lazy src=/images/posts/promptriever/slide-20.jpg></p><blockquote><p>Here’s a perfect example. For the query &ldquo;What is the capital of France?,&rdquo; a general article about Paris is a good result. But if you add the instruction &ldquo;mention its average annual rainfall,&rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.</p></blockquote><p><img alt="Slide 21" loading=lazy src=/images/posts/promptriever/slide-21.jpg></p><blockquote><p>Through this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.</p></blockquote><p><img alt="Slide 22" loading=lazy src=/images/posts/promptriever/slide-22.jpg></p><blockquote><p>The authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.</p></blockquote><p><img alt="Slide 23" loading=lazy src=/images/posts/promptriever/slide-23.jpg></p><blockquote><p>Creating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.</p></blockquote><p><img alt="Slide 24" loading=lazy src=/images/posts/promptriever/slide-24.jpg></p><blockquote><p>Now for the final section, &ldquo;Experiments,&rdquo; where we&rsquo;ll look at the results.</p></blockquote><p><img alt="Slide 25" loading=lazy src=/images/posts/promptriever/slide-25.jpg></p><blockquote><p>For a fair comparison, they ran an &ldquo;Apples-to-Apples&rdquo; test against RepLLaMA, using the exact same data and hyperparameters.</p></blockquote><p><img alt="Slide 26" loading=lazy src=/images/posts/promptriever/slide-26.jpg></p><blockquote><p>They used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, <strong>p-MRR</strong>, which is designed to measure sensitivity to instructions.</p></blockquote><p><img alt="Slide 27" loading=lazy src=/images/posts/promptriever/slide-27.jpg></p><blockquote><p>The results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.</p></blockquote><p><img alt="Slide 28" loading=lazy src=/images/posts/promptriever/slide-28.jpg></p><blockquote><p>This table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the <strong>p-MRR</strong> metric, which confirms its superior instruction-following ability.</p></blockquote><p><img alt="Slide 29" loading=lazy src=/images/posts/promptriever/slide-29.jpg></p><blockquote><p>On the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.</p></blockquote><p><img alt="Slide 30" loading=lazy src=/images/posts/promptriever/slide-30.jpg></p><blockquote><p>This is where it gets interesting. When given a helpful prompt, Promptriever&rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely &ldquo;promptable.&rdquo;</p></blockquote><p><img alt="Slide 31" loading=lazy src=/images/posts/promptriever/slide-31.jpg></p><blockquote><p>This table looks at the standard deviation of scores across different prompts. Promptriever&rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.</p></blockquote><p><img alt="Slide 32" loading=lazy src=/images/posts/promptriever/slide-32.jpg></p><blockquote><p>The ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.</p></blockquote><p><img alt="Slide 33" loading=lazy src=/images/posts/promptriever/slide-33.jpg></p><blockquote><p>The authors also proved their training &ldquo;recipe&rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, &ldquo;A golden recipe doesn&rsquo;t discriminate against ingredients!&rdquo;</p></blockquote><p><img alt="Slide 34" loading=lazy src=/images/posts/promptriever/slide-34.jpg></p><blockquote><p>Finally, let&rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method <em>is</em> their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper&rsquo;s scope.</p></blockquote><p><img alt="Slide 35" loading=lazy src=/images/posts/promptriever/slide-35.jpg></p><blockquote><p>In the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/llm/>LLM</a></li><li><a href=https://mookjsi.github.io/tags/rag/>RAG</a></li><li><a href=https://mookjsi.github.io/tags/retrieval/>Retrieval</a></li><li><a href=https://mookjsi.github.io/tags/iclr-2025/>ICLR 2025</a></li></ul><nav class=paginav><a class=prev href=https://mookjsi.github.io/posts/trpoppo/><span class=title>« Prev</span><br><span>Advanced Policy Gradient Methods: TRPO & PPO</span>
</a><a class=next href=https://mookjsi.github.io/posts/paper-review-flatminima/><span class=title>Next »</span><br><span>Simplifying Neural Nets by Discovering Flat Minima</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on x" href="https://x.com/intent/tweet/?text=Promptriever%20-%20Instruction-Trained%20Retrievers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&amp;hashtags=PaperReview%2cLLM%2cRAG%2cRetrieval%2cICLR2025"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&amp;title=Promptriever%20-%20Instruction-Trained%20Retrievers&amp;summary=Promptriever%20-%20Instruction-Trained%20Retrievers&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f&title=Promptriever%20-%20Instruction-Trained%20Retrievers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on whatsapp" href="https://api.whatsapp.com/send?text=Promptriever%20-%20Instruction-Trained%20Retrievers%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on telegram" href="https://telegram.me/share/url?text=Promptriever%20-%20Instruction-Trained%20Retrievers&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Promptriever - Instruction-Trained Retrievers on ycombinator" href="https://news.ycombinator.com/submitlink?t=Promptriever%20-%20Instruction-Trained%20Retrievers&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-promptriever%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>