<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction | MookStudy</title><meta name=keywords content="Paper Review,Computer Vision,3D Gaussian Splatting,TimeFormer,Deformation Field,Temporal Attention,ICCV 2025"><meta name=description content="TimeFormer 논문 리뷰입니다. 이 논문은 시간적 관계(Temporal Relationships)를 학습하여 기존 3DGS의 한계를 극복하고, 추론 비용 증가 없이 고품질 렌더링을 가능하게 하는 새로운 방법론을 제안합니다."><meta name=author content="Jungmook Kang"><link rel=canonical href=https://mookjsi.github.io/posts/paper-review-timeformer/><link crossorigin=anonymous href=/assets/css/stylesheet.03596ecd86a161ae014a0dfa94c2124c406fa319ff0dbb5cccfcd08aa1787188.css integrity="sha256-A1luzYahYa4BSg36lMISTEBvoxn/DbtczPzQiqF4cYg=" rel="preload stylesheet" as=style><link rel=icon href=https://mookjsi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mookjsi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mookjsi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mookjsi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mookjsi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mookjsi.github.io/posts/paper-review-timeformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://mookjsi.github.io/posts/paper-review-timeformer/"><meta property="og:site_name" content="MookStudy"><meta property="og:title" content="TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction"><meta property="og:description" content="TimeFormer 논문 리뷰입니다. 이 논문은 시간적 관계(Temporal Relationships)를 학습하여 기존 3DGS의 한계를 극복하고, 추론 비용 증가 없이 고품질 렌더링을 가능하게 하는 새로운 방법론을 제안합니다."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-01T00:00:00+00:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="3D Gaussian Splatting"><meta property="article:tag" content="TimeFormer"><meta property="article:tag" content="Deformation Field"><meta property="article:tag" content="Temporal Attention"><meta name=twitter:card content="summary"><meta name=twitter:title content="TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction"><meta name=twitter:description content="TimeFormer 논문 리뷰입니다. 이 논문은 시간적 관계(Temporal Relationships)를 학습하여 기존 3DGS의 한계를 극복하고, 추론 비용 증가 없이 고품질 렌더링을 가능하게 하는 새로운 방법론을 제안합니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mookjsi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction","item":"https://mookjsi.github.io/posts/paper-review-timeformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction","name":"TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction","description":"TimeFormer 논문 리뷰입니다. 이 논문은 시간적 관계(Temporal Relationships)를 학습하여 기존 3DGS의 한계를 극복하고, 추론 비용 증가 없이 고품질 렌더링을 가능하게 하는 새로운 방법론을 제안합니다.","keywords":["Paper Review","Computer Vision","3D Gaussian Splatting","TimeFormer","Deformation Field","Temporal Attention","ICCV 2025"],"articleBody":"1. 서론 (Introduction) 여러분이 춤 동작을 배운다고 상상해 보세요. 기존의 방식(기존 Dynamic 3DGS)은 마치 춤추는 사람의 사진을 한 장씩 따로따로 보고 동작을 유추하는 것과 같습니다. 이 사진과 다음 사진 사이의 연결 고리를 약하게만 파악하죠. 그래서 동작이 너무 빠르거나 조명이 번쩍이면(반사 재질) 3D 모델이 깨지거나 흐릿하게 보입니다.\n이 논문에서 제안하는 TimeFormer는 춤 동작 전체가 담긴 비디오를 한 번에 훑어보는 것과 같습니다. 여러 시간대의 장면(Timestamp)을 동시에 참고하여 “아, 이 팔은 이렇게 움직이는 중이구나\"라고 전체적인 흐름(Motion Pattern)을 파악합니다. 가장 똑똑한 점은, 연습할 때만 이 ‘비디오 분석 능력’을 사용하고, 실제 무대(Inference)에서는 분석 도구를 떼어버려도 몸이 기억해서 똑같이 잘 춘다는 것입니다. 덕분에 속도는 그대로 유지하면서 퀄리티만 높일 수 있습니다.\n2. 기술 설명 (Method) 이 섹션에서는 TimeFormer가 어떻게 시간적 관계를 학습하고 최적화하는지 설명합니다.\n이 그림은 TimeFormer의 핵심 개념을 보여줍니다. 오른쪽 다이어그램처럼 여러 시간대($t_i, t_j, t_k$)의 정보를 Temporal Attention을 통해 서로 참고합니다. 이를 통해 Deformation Field가 전체적인 움직임 패턴을 학습하게 됩니다.\nTimeFormer의 전체 프레임워크입니다.\n위쪽 (Base Stream): 기존의 방식대로 Canonical Space의 3D Gaussian이 Deformation Field를 거쳐 변형됩니다. 아래쪽 (TimeFormer Stream): TimeFormer가 시간적 관계를 분석하여 Deformation Field를 돕습니다. 중요한 건 두 개의 Deformation Field가 가중치(Weight)를 공유한다는 점입니다(점선 화살표). 즉, 아래쪽에서 똑똑하게 학습한 내용을 위쪽 모델도 공유받게 되어, 나중에는 아래쪽 모듈을 제거해도 됩니다.\n2.1 3D Gaussian Splatting 기초 먼저 3D Gaussian Splatting의 기본 수식입니다.\n$$ G(x) = o \\cdot e^{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu)} $$ 수식 분석: 3D 공간의 위치 $x$에 대한 Gaussian의 영향력을 나타냅니다. $\\mu$는 중심 위치(Mean), $\\Sigma$는 공분산 행렬(Covariance Matrix)로 가우시안의 모양(타원체)을 결정합니다. $o$는 불투명도(Opacity)입니다. 이는 통계적 확률 분포 함수를 3D 렌더링의 기본 입자(Primitive)로 차용한 것으로, 미분 가능한 렌더링 파이프라인을 구축하는 핵심입니다.\n$$ C = \\sum_{i}^{N} c_i \\alpha_i \\prod_{j=1}^{i-1} (1-\\alpha_j) $$ 수식 분석: $\\alpha$-blending 식입니다. $c_i$는 색상, $\\alpha_i$는 밀도입니다. 시선 방향을 따라 정렬된 Gaussian들의 기여도를 누적하여 픽셀 색상 $C$를 결정합니다. NeRF의 Volume Rendering 적분식을 이산적(Discrete)인 Gaussian들의 합으로 근사하여 고속 렌더링을 가능하게 합니다.\n2.2 Deformation Field와 TimeFormer 동적인 장면을 표현하기 위해 Canonical Space(기준 공간)의 가우시안을 시간 $t$에 따라 변형시킵니다.\n$$ (\\Delta\\mu, \\Delta r, \\Delta s) = \\mathcal{D}(\\mu, t) $$ 수식 분석: Deformation Field $\\mathcal{D}$는 기준 위치 $\\mu$와 시간 $t$를 입력받아 위치, 회전, 크기의 변화량($\\Delta$)을 출력합니다. 이는 시공간 $R^4$에서 $R^3$로의 매핑 함수로 볼 수 있습니다.\nCross-Temporal Encoder의 구조입니다. Canonical Gaussian 위치 $\\mu$에 여러 시간대($t_0, …, t_{B-1}$)를 결합하여 Transformer에 입력하고, 각 시간대에 맞는 위치 오프셋($\\Delta p$)을 출력합니다.\nTimeFormer를 위해 위치 인코딩(Positional Encoding)을 사용합니다.\n$$ \\gamma(p) = (\\sin(2^0 \\pi p), \\cos(2^0 \\pi p), ..., \\sin(2^{L-1} \\pi p), \\cos(2^{L-1} \\pi p)) $$ 수식 분석: 저주파 신호인 좌표값 $p$를 고주파 영역으로 매핑하여 신경망이 고주파수 세부 정보(High-frequency details)를 더 잘 학습하도록 돕습니다. Fourier Feature Mapping의 일종입니다.\nTimeFormer의 출력은 다음과 같이 잔차(Residual) 형태로 더해집니다.\n$$ \\mathcal{O} = MLP(F_{M-1}), \\quad \\mathcal{G}_t = \\mathcal{G}_c + \\mathcal{O} $$ 2.3 Gradient Flow Analysis (핵심 이론) 이 논문의 이론적 기여는 Gradient 분석에 있습니다. 기존 방식의 Gradient는 다음과 같습니다.\n$$ \\frac{\\partial \\Delta\\mu_i}{\\partial \\mu} = \\frac{\\partial \\mathcal{D}(\\mu, t_i)}{\\partial \\mu} $$ 하지만 TimeFormer를 적용하면 Chain Rule에 의해 항이 추가됩니다.\n$$ \\frac{\\partial \\Delta\\mu_i}{\\partial \\mu} = \\frac{\\partial \\mathcal{D}(a, t_i)}{\\partial a} \\cdot (1 + \\frac{\\partial \\mathcal{P}(\\mu, \\mathcal{T}_s)}{\\partial \\mu}) $$ 수식 분석: 여기서 $\\mathcal{P}(\\mu, \\mathcal{T}_s)$는 TimeFormer 모듈입니다. 추가된 항 $\\frac{\\partial \\mathcal{P}}{\\partial \\mu}$는 현재 시간 $t_i$뿐만 아니라 전체 타임 배치 $\\mathcal{T}_s$ (과거 및 미래)의 정보를 포함합니다. 즉, Backpropagation 과정에서 특정 시점의 학습이 다른 모든 시점의 상태를 고려하여 업데이트되도록 유도합니다. 이것이 ‘Global View’를 갖게 하는 수학적 원리입니다.\n데이터 흐름의 변화를 보여줍니다. 점선은 TimeFormer에 의해 시간 샘플들($t_0…t_{B-1}$) 사이에 새로운 정보 흐름이 생겼음을 의미합니다.\nTimeFormer의 구현 코드와 Two-Stream 최적화 전략에 대한 의사 코드(Pseudo-code)입니다. 학습 시에는 두 스트림을 모두 계산하고, 추론 시에는 TimeFormer를 끕니다.\n2.4 Loss Function $$ \\mathcal{L} = \\lambda_c \\mathcal{L}_c + \\lambda_t \\mathcal{L}_t $$ 수식 분석: 전체 손실 함수는 Original Branch의 손실 $\\mathcal{L}_c$와 TimeFormer Branch의 손실 $\\mathcal{L}_t$의 가중 합입니다. $\\lambda_t$를 약간 작게 설정하여 TimeFormer 쪽에 과적합(Overfitting)되는 것을 방지합니다.\n3. 실험 (Experiment) TimeFormer의 성능을 검증하기 위해 다양한 데이터셋에서 실험을 진행했습니다.\nTimeFormer가 적용된 모델(오른쪽)이 더 적은 수의 점(Points)을 사용하면서도 더 높은 FPS(초당 프레임 수)와 품질(PSNR)을 보여줍니다. Canonical Space가 더 효율적으로 구성되었음을 의미합니다.\nN3DV 데이터셋(다시점 비디오)에서의 정량적 비교입니다. TimeFormer를 추가했을 때 4DGS와 STGS 모델 모두에서 PSNR(화질) 수치가 향상되었습니다.\nN3DV 데이터셋의 시각적 비교입니다. 요리 장면에서 흐릿한 부분 없이 선명한 기하학적 구조를 복원했습니다.\n프레임별 PSNR 변화 그래프입니다. TimeFormer(주황색)가 전체적으로 파란색(기본 모델)보다 위에 있으며, 특히 영상의 시작과 끝부분에서도 안정적인 품질을 유지합니다.\n학습 곡선(Convergence Speed)입니다. TimeFormer를 사용하면 Loss가 더 빠르게 감소하여 학습 속도가 빨라집니다. 두 Branch의 Loss가 비슷하게 떨어지는 것은 지식 전이(Knowledge Transfer)가 잘 되고 있음을 보여줍니다.\nHyperNeRF 데이터셋(단안 비디오) 비교입니다. 손가락 사이나 빗자루 같은 복잡한 구조를 더 명확하게 표현합니다.\nNeRF-DS 데이터셋(반사체 포함) 비교입니다. 반짝이는 컵이나 유리병의 표면 반사를 훨씬 깨끗하게 렌더링합니다.\n학습 시간과 FPS, 가우시안 개수 비교입니다. TimeFormer를 쓰면 학습 시간은 조금 늘어나지만, 추론 FPS는 오히려 높아지고 가우시안 개수는 줄어듭니다(효율성 증가).\n이 그림은 매우 흥미롭습니다. 움직임의 변화(Motion Bias)를 히트맵으로 시각화했습니다. 칼이 레몬을 자르는 순간이나 불꽃이 흔들리는 미세한 움직임을 TimeFormer가 정확히 포착하고 있음을 보여줍니다.\nHyperNeRF와 NeRF-DS 데이터셋에 대한 추가적인 정량적 수치 표입니다. 모든 지표(PSNR, SSIM)에서 성능 향상을 확인했습니다.\nAblation Study(소거법 연구) 결과입니다. Transformer 레이어 수(M)나 시간 배치 크기(B)에 크게 민감하지 않으며, “Shared Weights(가중치 공유)” 전략이 없으면 성능이 크게 떨어짐을 보여줍니다.\nCanonical Space의 가우시안 분포와 FPS를 비교한 추가 자료입니다. TimeFormer가 불필요한 가우시안을 제거하여 더 깔끔한 공간 분포를 만듭니다.\n4. 결론 (Conclusion) 이 논문은 TimeFormer라는 플러그 앤 플레이(Plug-and-play) 모듈을 제안했습니다. 이 모듈은 Global Temporal Attention을 통해 복잡한 움직임과 반사 재질을 효과적으로 학습합니다. 특히 Two-stream Optimization 전략 덕분에, 추론 시에는 추가적인 연산 비용 없이 기존 모델보다 더 빠르고 정확한 렌더링이 가능합니다. 한계점으로는 일부 복잡한 디테일이 너무 부드럽게(Overly smooth) 표현될 수 있다는 점이 있습니다.\n","wordCount":"870","inLanguage":"en","datePublished":"2025-12-01T00:00:00Z","dateModified":"2025-12-01T00:00:00Z","author":{"@type":"Person","name":"Jungmook Kang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mookjsi.github.io/posts/paper-review-timeformer/"},"publisher":{"@type":"Organization","name":"MookStudy","logo":{"@type":"ImageObject","url":"https://mookjsi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mookjsi.github.io/ accesskey=h title="MookStudy (Alt + H)">MookStudy</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mookjsi.github.io/about/ title=About><span>About</span></a></li><li><a href=https://mookjsi.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://mookjsi.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://mookjsi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mookjsi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mookjsi.github.io/posts/>Blog</a></div><h1 class="post-title entry-hint-parent">TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction</h1><div class=post-meta><span title='2025-12-01 00:00:00 +0000 UTC'>December 1, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;870 words&nbsp;·&nbsp;Jungmook Kang</div></header><div class=post-content><h3 id=1-서론-introduction>1. 서론 (Introduction)<a hidden class=anchor aria-hidden=true href=#1-서론-introduction>#</a></h3><p>여러분이 춤 동작을 배운다고 상상해 보세요. 기존의 방식(기존 Dynamic 3DGS)은 마치 춤추는 사람의 사진을 한 장씩 따로따로 보고 동작을 유추하는 것과 같습니다. 이 사진과 다음 사진 사이의 연결 고리를 약하게만 파악하죠. 그래서 동작이 너무 빠르거나 조명이 번쩍이면(반사 재질) 3D 모델이 깨지거나 흐릿하게 보입니다.</p><p>이 논문에서 제안하는 TimeFormer는 춤 동작 전체가 담긴 비디오를 한 번에 훑어보는 것과 같습니다. 여러 시간대의 장면(Timestamp)을 동시에 참고하여 &ldquo;아, 이 팔은 이렇게 움직이는 중이구나"라고 전체적인 흐름(Motion Pattern)을 파악합니다. 가장 똑똑한 점은, 연습할 때만 이 &lsquo;비디오 분석 능력&rsquo;을 사용하고, 실제 무대(Inference)에서는 분석 도구를 떼어버려도 몸이 기억해서 똑같이 잘 춘다는 것입니다. 덕분에 속도는 그대로 유지하면서 퀄리티만 높일 수 있습니다.</p><hr><h3 id=2-기술-설명-method>2. 기술 설명 (Method)<a hidden class=anchor aria-hidden=true href=#2-기술-설명-method>#</a></h3><p>이 섹션에서는 TimeFormer가 어떻게 시간적 관계를 학습하고 최적화하는지 설명합니다.</p><p><img alt="Figure 1: TimeFormer의 핵심 개념 및 Temporal Attention" loading=lazy src=/images/posts/timeformer/figure1.png></p><p>이 그림은 TimeFormer의 핵심 개념을 보여줍니다. 오른쪽 다이어그램처럼 여러 시간대($t_i, t_j, t_k$)의 정보를 Temporal Attention을 통해 서로 참고합니다. 이를 통해 Deformation Field가 전체적인 움직임 패턴을 학습하게 됩니다.</p><p><img alt="Figure 3: TimeFormer의 전체 프레임워크" loading=lazy src=/images/posts/timeformer/figure3.png></p><p>TimeFormer의 전체 프레임워크입니다.</p><ul><li><strong>위쪽 (Base Stream):</strong> 기존의 방식대로 Canonical Space의 3D Gaussian이 Deformation Field를 거쳐 변형됩니다.</li><li><strong>아래쪽 (TimeFormer Stream):</strong> TimeFormer가 시간적 관계를 분석하여 Deformation Field를 돕습니다.</li></ul><p>중요한 건 두 개의 Deformation Field가 가중치(Weight)를 공유한다는 점입니다(점선 화살표). 즉, 아래쪽에서 똑똑하게 학습한 내용을 위쪽 모델도 공유받게 되어, 나중에는 아래쪽 모듈을 제거해도 됩니다.</p><h4 id=21-3d-gaussian-splatting-기초>2.1 3D Gaussian Splatting 기초<a hidden class=anchor aria-hidden=true href=#21-3d-gaussian-splatting-기초>#</a></h4><p>먼저 3D Gaussian Splatting의 기본 수식입니다.</p><div class=tex2jax_process>$$
G(x) = o \cdot e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}
$$</div><p><strong>수식 분석:</strong> 3D 공간의 위치 $x$에 대한 Gaussian의 영향력을 나타냅니다. $\mu$는 중심 위치(Mean), $\Sigma$는 공분산 행렬(Covariance Matrix)로 가우시안의 모양(타원체)을 결정합니다. $o$는 불투명도(Opacity)입니다. 이는 통계적 확률 분포 함수를 3D 렌더링의 기본 입자(Primitive)로 차용한 것으로, 미분 가능한 렌더링 파이프라인을 구축하는 핵심입니다.</p><div class=tex2jax_process>$$
C = \sum_{i}^{N} c_i \alpha_i \prod_{j=1}^{i-1} (1-\alpha_j)
$$</div><p><strong>수식 분석:</strong> $\alpha$-blending 식입니다. $c_i$는 색상, $\alpha_i$는 밀도입니다. 시선 방향을 따라 정렬된 Gaussian들의 기여도를 누적하여 픽셀 색상 $C$를 결정합니다. NeRF의 Volume Rendering 적분식을 이산적(Discrete)인 Gaussian들의 합으로 근사하여 고속 렌더링을 가능하게 합니다.</p><h4 id=22-deformation-field와-timeformer>2.2 Deformation Field와 TimeFormer<a hidden class=anchor aria-hidden=true href=#22-deformation-field와-timeformer>#</a></h4><p>동적인 장면을 표현하기 위해 Canonical Space(기준 공간)의 가우시안을 시간 $t$에 따라 변형시킵니다.</p><div class=tex2jax_process>$$
(\Delta\mu, \Delta r, \Delta s) = \mathcal{D}(\mu, t)
$$</div><p><strong>수식 분석:</strong> Deformation Field $\mathcal{D}$는 기준 위치 $\mu$와 시간 $t$를 입력받아 위치, 회전, 크기의 변화량($\Delta$)을 출력합니다. 이는 시공간 $R^4$에서 $R^3$로의 매핑 함수로 볼 수 있습니다.</p><p><img alt="Figure 4: Cross-Temporal Encoder의 구조" loading=lazy src=/images/posts/timeformer/figure4.png></p><p>Cross-Temporal Encoder의 구조입니다. Canonical Gaussian 위치 $\mu$에 여러 시간대($t_0, &mldr;, t_{B-1}$)를 결합하여 Transformer에 입력하고, 각 시간대에 맞는 위치 오프셋($\Delta p$)을 출력합니다.</p><p>TimeFormer를 위해 위치 인코딩(Positional Encoding)을 사용합니다.</p><div class=tex2jax_process>$$
\gamma(p) = (\sin(2^0 \pi p), \cos(2^0 \pi p), ..., \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p))
$$</div><p><strong>수식 분석:</strong> 저주파 신호인 좌표값 $p$를 고주파 영역으로 매핑하여 신경망이 고주파수 세부 정보(High-frequency details)를 더 잘 학습하도록 돕습니다. Fourier Feature Mapping의 일종입니다.</p><p>TimeFormer의 출력은 다음과 같이 잔차(Residual) 형태로 더해집니다.</p><div class=tex2jax_process>$$
\mathcal{O} = MLP(F_{M-1}), \quad \mathcal{G}_t = \mathcal{G}_c + \mathcal{O}
$$</div><h4 id=23-gradient-flow-analysis-핵심-이론>2.3 Gradient Flow Analysis (핵심 이론)<a hidden class=anchor aria-hidden=true href=#23-gradient-flow-analysis-핵심-이론>#</a></h4><p>이 논문의 이론적 기여는 Gradient 분석에 있습니다. 기존 방식의 Gradient는 다음과 같습니다.</p><div class=tex2jax_process>$$
\frac{\partial \Delta\mu_i}{\partial \mu} = \frac{\partial \mathcal{D}(\mu, t_i)}{\partial \mu}
$$</div><p>하지만 TimeFormer를 적용하면 Chain Rule에 의해 항이 추가됩니다.</p><div class=tex2jax_process>$$
\frac{\partial \Delta\mu_i}{\partial \mu} = \frac{\partial \mathcal{D}(a, t_i)}{\partial a} \cdot (1 + \frac{\partial \mathcal{P}(\mu, \mathcal{T}_s)}{\partial \mu})
$$</div><p><strong>수식 분석:</strong> 여기서 $\mathcal{P}(\mu, \mathcal{T}_s)$는 TimeFormer 모듈입니다. 추가된 항 $\frac{\partial \mathcal{P}}{\partial \mu}$는 현재 시간 $t_i$뿐만 아니라 전체 타임 배치 $\mathcal{T}_s$ (과거 및 미래)의 정보를 포함합니다. 즉, Backpropagation 과정에서 특정 시점의 학습이 다른 모든 시점의 상태를 고려하여 업데이트되도록 유도합니다. 이것이 &lsquo;Global View&rsquo;를 갖게 하는 수학적 원리입니다.</p><p><img alt="Figure 5: 데이터 흐름 및 Gradient Flow 변화" loading=lazy src=/images/posts/timeformer/figure5.png></p><p>데이터 흐름의 변화를 보여줍니다. 점선은 TimeFormer에 의해 시간 샘플들($t_0&mldr;t_{B-1}$) 사이에 새로운 정보 흐름이 생겼음을 의미합니다.</p><p><img alt="Algorithm 1: TimeFormer 구현 및 Two-Stream 최적화 전략" loading=lazy src=/images/posts/timeformer/algorithm1.png>
<img alt="Algorithm 2: TimeFormer 구현 및 Two-Stream 최적화 전략" loading=lazy src=/images/posts/timeformer/algorithm2.png></p><p>TimeFormer의 구현 코드와 Two-Stream 최적화 전략에 대한 의사 코드(Pseudo-code)입니다. 학습 시에는 두 스트림을 모두 계산하고, 추론 시에는 TimeFormer를 끕니다.</p><h4 id=24-loss-function>2.4 Loss Function<a hidden class=anchor aria-hidden=true href=#24-loss-function>#</a></h4><div class=tex2jax_process>$$
\mathcal{L} = \lambda_c \mathcal{L}_c + \lambda_t \mathcal{L}_t
$$</div><p><strong>수식 분석:</strong> 전체 손실 함수는 Original Branch의 손실 $\mathcal{L}_c$와 TimeFormer Branch의 손실 $\mathcal{L}_t$의 가중 합입니다. $\lambda_t$를 약간 작게 설정하여 TimeFormer 쪽에 과적합(Overfitting)되는 것을 방지합니다.</p><hr><h3 id=3-실험-experiment>3. 실험 (Experiment)<a hidden class=anchor aria-hidden=true href=#3-실험-experiment>#</a></h3><p>TimeFormer의 성능을 검증하기 위해 다양한 데이터셋에서 실험을 진행했습니다.</p><p><img alt="Figure 2: Points 수 대비 FPS 및 PSNR 비교" loading=lazy src=/images/posts/timeformer/figure2.png></p><p>TimeFormer가 적용된 모델(오른쪽)이 더 적은 수의 점(Points)을 사용하면서도 더 높은 FPS(초당 프레임 수)와 품질(PSNR)을 보여줍니다. Canonical Space가 더 효율적으로 구성되었음을 의미합니다.</p><p><img alt="Table 1: N3DV 데이터셋에서의 정량적 비교" loading=lazy src=/images/posts/timeformer/table1.png></p><p>N3DV 데이터셋(다시점 비디오)에서의 정량적 비교입니다. TimeFormer를 추가했을 때 4DGS와 STGS 모델 모두에서 PSNR(화질) 수치가 향상되었습니다.</p><p><img alt="Figure 6: N3DV 데이터셋 시각적 비교" loading=lazy src=/images/posts/timeformer/figure6.png></p><p>N3DV 데이터셋의 시각적 비교입니다. 요리 장면에서 흐릿한 부분 없이 선명한 기하학적 구조를 복원했습니다.</p><p><img alt="Figure 7: 프레임별 PSNR 변화 그래프" loading=lazy src=/images/posts/timeformer/figure7.png></p><p>프레임별 PSNR 변화 그래프입니다. TimeFormer(주황색)가 전체적으로 파란색(기본 모델)보다 위에 있으며, 특히 영상의 시작과 끝부분에서도 안정적인 품질을 유지합니다.</p><p><img alt="Figure 8: 학습 곡선 (Convergence Speed)" loading=lazy src=/images/posts/timeformer/figure8.png></p><p>학습 곡선(Convergence Speed)입니다. TimeFormer를 사용하면 Loss가 더 빠르게 감소하여 학습 속도가 빨라집니다. 두 Branch의 Loss가 비슷하게 떨어지는 것은 지식 전이(Knowledge Transfer)가 잘 되고 있음을 보여줍니다.</p><p><img alt="Figure 9: HyperNeRF 데이터셋 비교" loading=lazy src=/images/posts/timeformer/figure9.png></p><p>HyperNeRF 데이터셋(단안 비디오) 비교입니다. 손가락 사이나 빗자루 같은 복잡한 구조를 더 명확하게 표현합니다.</p><p><img alt="Figure 10: NeRF-DS 데이터셋 비교" loading=lazy src=/images/posts/timeformer/figure10.png></p><p>NeRF-DS 데이터셋(반사체 포함) 비교입니다. 반짝이는 컵이나 유리병의 표면 반사를 훨씬 깨끗하게 렌더링합니다.</p><p><img alt="Table 2: 학습 시간, FPS, 가우시안 개수 비교" loading=lazy src=/images/posts/timeformer/table2.png></p><p>학습 시간과 FPS, 가우시안 개수 비교입니다. TimeFormer를 쓰면 학습 시간은 조금 늘어나지만, 추론 FPS는 오히려 높아지고 가우시안 개수는 줄어듭니다(효율성 증가).</p><p><img alt="Figure 11: Motion Bias 히트맵 시각화" loading=lazy src=/images/posts/timeformer/figure11.png></p><p>이 그림은 매우 흥미롭습니다. 움직임의 변화(Motion Bias)를 히트맵으로 시각화했습니다. 칼이 레몬을 자르는 순간이나 불꽃이 흔들리는 미세한 움직임을 TimeFormer가 정확히 포착하고 있음을 보여줍니다.</p><p><img alt="Table 3 & 4: HyperNeRF 및 NeRF-DS 데이터셋 정량적 수치" loading=lazy src=/images/posts/timeformer/table3_4.png></p><p>HyperNeRF와 NeRF-DS 데이터셋에 대한 추가적인 정량적 수치 표입니다. 모든 지표(PSNR, SSIM)에서 성능 향상을 확인했습니다.</p><p><img alt="Table 5: Ablation Study 결과" loading=lazy src=/images/posts/timeformer/table5.png>
<img alt="Table 6: Ablation Study 결과" loading=lazy src=/images/posts/timeformer/table6.png></p><p>Ablation Study(소거법 연구) 결과입니다. Transformer 레이어 수(M)나 시간 배치 크기(B)에 크게 민감하지 않으며, &ldquo;Shared Weights(가중치 공유)&rdquo; 전략이 없으면 성능이 크게 떨어짐을 보여줍니다.</p><p><img alt="Figure 12 & 13: Canonical Space 가우시안 분포 비교" loading=lazy src=/images/posts/timeformer/figure12_13.png></p><p>Canonical Space의 가우시안 분포와 FPS를 비교한 추가 자료입니다. TimeFormer가 불필요한 가우시안을 제거하여 더 깔끔한 공간 분포를 만듭니다.</p><hr><h3 id=4-결론-conclusion>4. 결론 (Conclusion)<a hidden class=anchor aria-hidden=true href=#4-결론-conclusion>#</a></h3><p>이 논문은 TimeFormer라는 플러그 앤 플레이(Plug-and-play) 모듈을 제안했습니다. 이 모듈은 Global Temporal Attention을 통해 복잡한 움직임과 반사 재질을 효과적으로 학습합니다. 특히 Two-stream Optimization 전략 덕분에, 추론 시에는 추가적인 연산 비용 없이 기존 모델보다 더 빠르고 정확한 렌더링이 가능합니다. 한계점으로는 일부 복잡한 디테일이 너무 부드럽게(Overly smooth) 표현될 수 있다는 점이 있습니다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mookjsi.github.io/tags/paper-review/>Paper Review</a></li><li><a href=https://mookjsi.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://mookjsi.github.io/tags/3d-gaussian-splatting/>3D Gaussian Splatting</a></li><li><a href=https://mookjsi.github.io/tags/timeformer/>TimeFormer</a></li><li><a href=https://mookjsi.github.io/tags/deformation-field/>Deformation Field</a></li><li><a href=https://mookjsi.github.io/tags/temporal-attention/>Temporal Attention</a></li><li><a href=https://mookjsi.github.io/tags/iccv-2025/>ICCV 2025</a></li></ul><nav class=paginav><a class=next href=https://mookjsi.github.io/posts/diffusion/><span class=title>Next »</span><br><span>Diffusion Models: From VAEs to DDPM Derivation</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on x" href="https://x.com/intent/tweet/?text=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f&amp;hashtags=PaperReview%2cComputerVision%2c3DGaussianSplatting%2cTimeFormer%2cDeformationField%2cTemporalAttention%2cICCV2025"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f&amp;title=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction&amp;summary=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction&amp;source=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f&title=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on whatsapp" href="https://api.whatsapp.com/send?text=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction%20-%20https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on telegram" href="https://telegram.me/share/url?text=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction&amp;url=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction on ycombinator" href="https://news.ycombinator.com/submitlink?t=TimeFormer%3a%20Capturing%20Temporal%20Relationships%20of%20Deformable%203D%20Gaussians%20for%20Robust%20Reconstruction&u=https%3a%2f%2fmookjsi.github.io%2fposts%2fpaper-review-timeformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 Jungmook Kang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>