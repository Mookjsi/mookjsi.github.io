[{"content":"1. 서론 객체 탐지란 이미지 속에서 우리가 관심 있는 물체(객체)가 무엇인지(분류, classification) 그리고 어디에 있는지(위치 파악, localization)를 알아내는 기술입니다. 예를 들어, 자율주행 자동차가 도로 위의 사람, 다른 자동차, 신호등을 정확히 인식하는 데 이 기술이 사용됩니다.\n기존의 객체 탐지 모델들은 이 문제를 약간 간접적인 방식으로 풀어왔습니다. 이미지 위에 수많은 가상의 사각형(앵커 박스, anchor box)이나 **후보 영역(proposal)**을 미리 뿌려놓고, 이 중에서 어떤 사각형이 실제 물체를 잘 감싸고 있는지, 그리고 그 물체는 무엇인지를 맞추는 방식이었습니다. 이 방법은 꽤 성공적이었지만 몇 가지 번거로운 과정이 필요했습니다.\n수작업 설계 (Hand-designed components): 앵커 박스의 크기나 비율을 미리 정해야 하는 등 사람이 직접 설정해야 하는 부분이 많았습니다. 후처리 (Post-processing): 모델이 하나의 물체에 대해 여러 개의 겹치는 박스를 예측하는 경우가 많아서, 이 중 가장 정확한 박스 하나만 남기는 **\u0026lsquo;비최대 억제(Non-Maximum Suppression, NMS)\u0026rsquo;**라는 복잡한 후처리 과정이 필수적이었습니다. 이 논문에서 제안하는 DETR (DEtection TRansformer) 은 이러한 복잡한 과정들을 모두 없애고, 객체 탐지를 \u0026ldquo;정답 집합을 한 번에 예측하는 문제\u0026quot;로 재정의했습니다. 마치 사람이 이미지를 한 번 쓱 보고 \u0026ldquo;여기엔 고양이 한 마리, 저기엔 강아지 두 마리가 있네\u0026quot;라고 말하는 것처럼, 모델이 직접 최종 예측 결과의 **\u0026lsquo;집합(set)\u0026rsquo;**을 출력하도록 만든 것입니다.\n이러한 혁신은 두 가지 핵심 요소 덕분에 가능했습니다.\n이분 매칭 손실 (Bipartite Matching Loss): 모델의 예측 결과와 실제 정답을 일대일로 짝지어주는 손실 함수입니다. 이를 통해 중복된 예측을 자연스럽게 방지합니다. 트랜스포머 (Transformer) 구조: 원래 번역 분야에서 뛰어난 성능을 보인 아키텍처로, 이미지 전체의 맥락을 종합적으로 이해하는 데 탁월한 능력을 보입니다. 위 그림은 DETR의 전체적인 흐름을 보여줍니다.\n먼저, 일반적인 CNN을 사용해 이미지의 특징(feature)을 추출합니다. 이는 이미지의 중요한 시각적 정보를 압축하는 과정입니다. 이 특징 정보를 트랜스포머 인코더-디코더에 입력합니다. 트랜스포머는 이미지 전체의 관계를 파악하여 고정된 개수의 예측 상자(box prediction) 집합을 출력합니다. 학습 과정에서는 **이분 매칭 손실(bipartite matching loss)**을 사용해 모델이 예측한 상자들과 실제 정답 상자들을 가장 효율적으로 일대일 매칭시킵니다. 매칭되지 못한 예측 상자는 **\u0026ldquo;물체 없음(no object)\u0026rdquo;**으로 학습됩니다. 2. 기술 설명 (The DETR model) 위 그림은 DETR의 상세한 구조를 보여줍니다.\n백본 (Backbone) ResNet과 같은 표준 CNN 모델을 사용하여 이미지에서 저해상도의 특징 맵(feature map)을 추출합니다. 예를 들어, 2048개의 채널을 가진 특징 맵을 만들어냅니다.\n트랜스포머 인코더 (Transformer Encoder) CNN이 만든 특징 맵은 2D 형태인데, 트랜스포머에 넣기 위해 1D 시퀀스(순서가 있는 데이터) 형태로 길게 펼칩니다. 각 특징의 위치 정보를 알려주기 위해 **공간적 위치 인코딩(spatial positional encoding)**을 더해줍니다. 트랜스포머는 본래 순서 개념이 없기 때문에, \u0026ldquo;이 특징은 이미지의 왼쪽 위에 있다\u0026quot;와 같은 위치 정보를 인위적으로 주입해야 합니다. 인코더는 셀프 어텐션(self-attention) 메커니즘을 통해 이미지의 모든 픽셀 영역들이 서로 어떻게 연관되어 있는지를 학습합니다. 예를 들어, 이미지에 얼룩말 두 마리가 있다면, 인코더는 얼룩말 무늬를 가진 픽셀들이 서로 강하게 연관되어 있음을 파악하고, 각 얼룩말을 별개의 개체로 분리해내는 역할을 합니다. 트랜스포머 디코더 (Transformer Decoder) 객체 쿼리란? - 100명의 전문 탐정단\nDETR의 **객체 쿼리(object queries)**를 가장 쉽게 이해하는 방법은, 이미지를 \u0026lsquo;사건 현장\u0026rsquo;이라고 보고 100명의 전문 탐정단이 각자 빈 수첩(=객체 쿼리)을 들고 파견된다고 상상하는 것입니다.\n이 탐정들은 처음에는 아무 정보도 없는 상태(빈 수첩)로 시작하지만, 학습을 거치며 각자 자신만의 전문 분야(특정 위치, 크기, 형태 등)를 갖게 됩니다. 각 탐정(객체 쿼리)은 이미지 전체(인코더의 출력)를 샅샅이 살피며, 자신이 맡을 단서(객체)를 찾으려고 합니다. 탐정들끼리도 계속 정보를 공유(셀프 어텐션)하여, 한 명이 하나의 단서만 맡도록 역할을 분담합니다. 이 덕분에 하나의 물체에 여러 박스가 중복 예측되는 문제가 자연스럽게 해결됩니다(NMS 불필요). 디코더에서의 객체 쿼리 역할:\n디코더는 이미지 특징을 직접 받는 대신, **객체 쿼리(object queries)**라는 고정된 개수(N=100)의 학습 가능한 임베딩을 입력으로 받습니다. 이 객체 쿼리는 \u0026ldquo;이미지에서 찾을 N개의 물체 슬롯\u0026quot;이자, \u0026lsquo;사건 현장에 파견된 100명의 탐정\u0026rsquo;에 해당합니다. 처음에는 빈 슬롯(빈 수첩)이지만, 학습을 통해 각각 특정 위치나 크기의 물체를 찾는 데 특화됩니다. 디코더는 이 객체 쿼리들과 인코더의 출력(이미지 전체의 문맥 정보)을 함께 사용하여 \u0026ldquo;물체가 어디에 있는지\u0026quot;를 추론합니다. 디코더의 셀프 어텐션은 N개의 예측 간의 관계를 모델링하여, 예를 들어 하나의 물체에 대해 여러 슬롯이 중복으로 예측하는 것을 억제하는 역할을 합니다(탐정들끼리 역할 분담). 하나의 중요한 특징은 이 N개의 객체를 병렬적으로(in parallel), 즉 한 번에 디코딩한다는 점입니다. 기존 트랜스포머가 단어를 하나씩 순서대로 생성하는 것과 대조적입니다. 예측 헤드 (Prediction Heads, FFNs) 디코더에서 나온 N개의 출력 임베딩은 각각 동일한 구조를 공유하는 작은 신경망(FFN)으로 전달됩니다. 이 FFN은 최종적으로 각 슬롯에 대해 물체의 **클래스(class)**와 바운딩 박스(bounding box) 좌표를 예측합니다. 만약 해당 슬롯이 찾은 물체가 없다면, **\u0026ldquo;물체 없음(no object)\u0026rdquo;**이라는 특별한 클래스를 예측하게 됩니다. 손실 함수 (Loss Function): 객체 탐지를 위한 집합 예측 DETR의 핵심 아이디어는 예측과 정답을 \u0026lsquo;집합 대 집합\u0026rsquo;으로 보고, 둘 사이의 최적의 짝을 찾아 손실을 계산하는 것입니다.\n이분 매칭 (Bipartite Matching) 모델이 100개의 예측(집합 $\\hat{y}$)을 내놓고, 실제 이미지에는 5개의 물체(정답 집합 y)가 있다고 가정해 봅시다. 100개의 예측 중 어떤 것이 5개의 정답 각각에 해당하는지를 결정해야 합니다. 이때 **헝가리안 알고리즘(Hungarian Algorithm)**을 사용하여 전체 매칭 비용이 최소가 되는 최적의 일대일 짝(optimal assignment)을 찾습니다.\n이 매칭 비용 $\\mathcal{L}_{match}$는 두 가지를 고려합니다: (1) 예측한 클래스가 정답 클래스와 일치할 확률, (2) 예측한 박스가 정답 박스와 얼마나 비슷한지.\n수식 1: 최적 할당 (Optimal Assignment) $$ \\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_N}{\\arg\\min} \\sum_{i}^{N} \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) $$ 최적의 순열(permutation) $\\hat{\\sigma}$를 찾는 과정을 나타냅니다. 여기서 $\\mathfrak{S}{N}$은 N개 원소의 모든 가능한 순열 집합을 의미합니다. $y{i}$는 i번째 실제 정답 객체(클래스 $c_i$, 박스 $b_i$)이고, $\\hat{y}{\\sigma(i)}$는 순열 $\\sigma$에 의해 재배열된 예측 중 i번째 위치에 온 예측입니다. 이 수식의 목표는 모든 N개의 쌍에 대한 매칭 비용($\\mathcal{L}{\\text{match}}$)의 총합을 최소화하는 순열 $\\hat{\\sigma}$를 찾는 것입니다. 이는 전형적인 할당 문제(assignment problem)이며, 헝가리안 알고리즘을 통해 다항 시간 내에 효율적으로 해결할 수 있습니다. 이 과정을 통해 각 정답 객체에 대해 가장 적절한 예측이 단 하나만 할당되도록 보장하여, 중복 탐지를 원천적으로 방지합니다.\n수식 2: 헝가리안 손실 (Hungarian Loss) $$ \\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^{N} \\left[ -\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbf{1}_{c_i \\ne \\emptyset} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)}) \\right] $$ 위에서 찾은 최적의 짝 $\\hat{\\sigma}$을 바탕으로 최종 손실을 계산합니다. 이 손실은 모든 N개의 매칭된 쌍에 대한 손실의 합입니다.\n클래스 예측 손실: $-\\log\\hat{p}{\\hat{\\sigma}(i)}(c{i})$는 표준적인 교차 엔트로피(cross-entropy) 손실입니다. 즉, 최적의 짝으로 매칭된 예측이 정답 클래스 $c_i$를 얼마나 정확하게 예측했는지를 측정합니다. 박스 손실: $\\mathbf{1}{{c{i}\\ne\\emptyset}}\\mathcal{L}{\\text{box}}(\u0026hellip;)$는 바운딩 박스에 대한 손실입니다. $\\mathbf{1}{{c_{i}\\ne\\emptyset}}$는 정답이 \u0026lsquo;물체 없음\u0026rsquo;이 아닐 경우에만 박스 손실을 계산하라는 의미의 지시 함수(indicator function)입니다. $\\mathcal{L}_{\\text{box}}$는 아래에서 설명할 L1 손실과 GIoU 손실의 조합으로 이루어집니다. 바운딩 박스 손실 ($\\mathcal{L}_{box}$) 기존의 L1 손실(좌표 차이의 절댓값 합)은 박스가 클 때 손실 값도 커지는 문제가 있어, 박스 크기에 따라 손실의 스케일이 달라집니다. 이를 해결하기 위해 DETR은 두 가지 손실을 조합합니다. $$ \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{L1}||b_i - \\hat{b}_{\\sigma(i)}||_1 $$ L1 손실: 예측 박스와 정답 박스의 중심점 좌표, 너비, 높이 간의 차이를 계산합니다. 일반화된 IoU (GIoU) 손실: 두 박스가 얼마나 겹치는지를 나타내는 IoU(Intersection over Union)를 일반화한 것으로, 박스 크기에 무관하게(scale-invariant) 손실을 측정할 수 있어 작은 물체와 큰 물체를 공평하게 학습할 수 있습니다. 3. 실험 (Experiments) DETR의 성능을 검증하기 위해 가장 널리 사용되는 COCO 데이터셋으로 다양한 실험을 진행했습니다.\n이 표는 DETR과 기존의 강력한 모델인 Faster R-CNN의 성능을 비교합니다.\n결론적으로, DETR은 매우 잘 최적화된 Faster R-CNN과 **비교할 만한 성능(AP 42.0 vs 42.0)**을 달성했습니다. 특히 주목할 점은, DETR이 **큰 물체에 대해서는 훨씬 뛰어난 성능($AP_L$ 61.1 vs 53.4)**을 보인다는 것입니다. 이는 트랜스포머 인코더가 이미지 전체의 넓은 문맥을 보기 때문에 가능한 것으로 분석됩니다. 반면, 작은 물체에 대해서는 성능이 다소 낮게($AP_S$ 20.5 vs 26.6) 나타났습니다.\n주요 구성 요소 분석 (Ablation Studies) DETR의 어떤 부분이 성능에 얼마나 기여하는지 알아보기 위해 여러 실험을 진행했습니다.\n인코더의 중요성 (Table 2, Figure 3): 인코더 층을 제거하자 전체 성능(AP)이 약 3.9점 하락했으며, 특히 큰 물체에 대한 성능이 6.0점이나 떨어졌습니다. 이는 이미지 전체의 맥락을 이해하는 인코더가 객체들을 서로 분리하는 데 중요한 역할을 함을 시사합니다.\nFigure 3은 인코더의 셀프 어텐션이 어떻게 동작하는지 시각화한 것입니다. 그림 중앙의 소 이미지 위에 찍힌 빨간 점이 특정 위치를 의미하고, 주변의 작은 이미지들은 해당 위치가 이미지의 다른 어떤 부분에 주목(attention)하는지를 보여줍니다. 각기 다른 소들이 서로 다른 영역으로 분리되어 주목받는 것을 볼 수 있는데, 이는 인코더가 이미 개별 인스턴스들을 분리하고 있음을 보여줍니다.\n디코더의 중요성 (Figure 4): 이 그래프는 디코더 층이 깊어질수록 성능(AP)이 꾸준히 향상되는 것을 보여줍니다. 첫 번째 디코더 층의 결과와 마지막 층의 결과를 비교하면 AP가 8점 이상 크게 향상되었습니다.\n흥미로운 점은, 첫 번째 디코더 층에서는 NMS 후처리를 적용하면 성능이 오르지만, 층이 깊어질수록 NMS의 효과가 줄어들고 마지막에는 오히려 성능을 약간 해친다는 것입니다. 이는 디코더의 셀프 어텐션이 스스로 중복 예측을 억제하는 법을 학습하기 때문에, DETR은 설계적으로 NMS가 필요 없다는 것을 실험적으로 증명합니다.\n그 외 요소들 (Table 3, 4): 위치 인코딩 (Table 3): 공간적 위치 인코딩을 제거하자 성능이 7.8 AP나 하락하여, 이 요소가 매우 중요함을 확인했습니다. 손실 함수 (Table 4): L1 손실과 GIoU 손실 중 GIoU 손실이 성능에 거의 절대적인 기여를 하며, L1 손실은 보조적인 역할만 한다는 것을 발견했습니다. Figure 6은 디코더가 각 물체를 예측할 때 이미지의 어느 부분을 주목하는지 보여줍니다. 코끼리나 얼룩말을 탐지할 때, 주로 머리, 다리 등 물체의 **윤곽을 결정하는 극단적인 부분(extremities)**에 주목하는 경향을 보입니다. 이는 인코더가 이미 \u0026ldquo;여기에 얼룩말이 있다\u0026quot;고 알려주면, 디코더는 그 경계만 정확히 그리는 데 집중한다는 가설을 뒷받침합니다.\nFigure 7은 100개의 객체 쿼리(슬롯)가 각각 어떤 종류의 박스를 예측하도록 학습되는지 보여줍니다. 각 슬롯은 특정 **위치(area)와 크기(box size)**를 전담하도록 특화되는 경향을 보입니다. 예를 들어 어떤 슬롯은 이미지 중앙의 큰 물체를, 다른 슬롯은 왼쪽 하단의 작은 물체를 주로 찾도록 학습됩니다.\n학습 데이터에는 기린이 13마리 이상 있는 이미지가 없었습니다. 이 모델이 학습 데이터에 없는 상황에서도 잘 동작하는지 알아보기 위해, 인공적으로 기린 24마리가 있는 이미지를 만들어 테스트했습니다.\nFigure 5에서 볼 수 있듯이, DETR은 놀랍게도 24마리의 기린을 모두 정확하게 찾아냈습니다. 이는 100개의 객체 쿼리가 특정 클래스(예: \u0026lsquo;기린 전용 쿼리\u0026rsquo;)에 과적합되지 않고, 일반적인 물체를 찾는 능력을 학습했음을 보여줍니다.\nPanoptic Segmentation으로의 확장 Panoptic Segmentation은 이미지의 모든 픽셀을 \u0026ldquo;어떤 물체에 속하는지(things)\u0026rdquo; 또는 \u0026ldquo;어떤 배경에 속하는지(stuff)\u0026ldquo;로 구분하는 더 복잡한 작업입니다.\nFigure 8은 DETR에 간단한 **마스크 헤드(mask head)**를 추가하여 이 작업을 수행하는 방법을 보여줍니다. 디코더의 출력 각각에 대해 해당 물체의 마스크를 예측하고, 이를 합쳐 최종 결과를 만듭니다.\nTable 5는 DETR이 PanopticFPN과 같은 기존의 강력한 모델들을 능가하는 성능을 보였음을 나타냅니다. 특히 배경(stuff) 클래스에서 강점을 보이는데, 이는 이미지 전체를 보는 인코더의 힘 덕분일 가능성이 높습니다.\nFigure 9는 DETR이 생성한 Panoptic Segmentation의 예시 이미지로, 물체와 배경 모두에 대해 깔끔한 결과를 보여줍니다.\n4. 결론 (Conclusion) 이 논문은 트랜스포머와 이분 매칭 손실을 기반으로 객체 탐지를 **\u0026lsquo;직접적인 집합 예측 문제\u0026rsquo;**로 풀어내는 새로운 패러다임인 DETR을 제시했습니다. DETR은 기존의 복잡한 파이프라인(앵커, NMS 등)을 제거하면서도, 고도로 최적화된 Faster R-CNN과 대등한 성능을 달성했습니다.\n또한, DETR의 구조는 매우 간단하고 유연하여 Panoptic Segmentation과 같은 더 복잡한 문제로도 쉽게 확장될 수 있음을 보여주었습니다. 특히 이미지의 전역적인 정보를 활용하는 능력 덕분에 큰 객체 탐지에서 뛰어난 성능을 보였습니다.\n물론 작은 객체에 대한 성능이나 긴 학습 시간과 같은 도전 과제들이 남아있지만, 이 연구는 객체 탐지 분야에 새로운 방향을 제시했으며, 향후 연구를 통해 이러한 문제들이 해결될 것으로 기대됩니다.\n","permalink":"https://mookjsi.github.io/posts/paper-review-detr/","summary":"\u0026lsquo;End-to-End Object Detection with Transformers\u0026rsquo; 논문 심층 리뷰","title":"DETR: End-to-End Object Detection with Transformers"},{"content":"CVPR 2016에서 발표되어 딥러닝 역사에 한 획을 그은 논문, Deep Residual Learning for Image Recognition에 대한 리뷰를 공유합니다.\n이 논문은 당시 딥러닝 모델의 큰 난제였던 \u0026lsquo;네트워크가 깊어질수록 오히려 성능이 저하되는\u0026rsquo; 문제를 해결했습니다. 저자들의 해법인 **ResNet (Residual Network)**은 \u0026lsquo;잔차 학습\u0026rsquo;이라는 혁신적인 구조를 도입하여 이전에는 불가능했던 100층 이상의 초심층 신경망 훈련을 가능하게 했습니다.\n서론 컴퓨터가 이미지를 인식하는 기술, 예를 들어 사진 속의 고양이를 알아보는 인공지능(AI)은 \u0026lsquo;심층 신경망(Deep Neural Network)\u0026lsquo;이라는 기술을 사용합니다. 이 신경망은 인간의 뇌가 정보를 처리하는 방식을 모방한 것으로, 여러 개의 층(layer)으로 이루어져 있습니다. 이론적으로는 이 층을 많이 쌓을수록, 즉 네트워크가 \u0026lsquo;깊어질수록\u0026rsquo; 더 똑똑해져야 합니다. 마치 우리가 여러 단계의 사고를 거쳐 복잡한 문제를 해결하는 것과 같죠.\n하지만 실제로는 무작정 층을 깊게 쌓기만 하면 오히려 성능이 떨어지는 성능 저하 (Degradation) 문제가 발생했습니다. 신기하게도 이는 단순히 계산이 너무 복잡해져서 생기는 과적합(overfitting) 문제도 아니었습니다. 더 깊은 모델이 그보다 얕은 모델보다 훈련 데이터에 대한 오류율(training error)이 더 높게 나타나는 이상한 현상이었습니다.\n위 그림은 이 문제를 명확히 보여줍니다. CIFAR-10이라는 이미지 데이터셋으로 20층 네트워크와 56층 네트워크를 학습시킨 결과입니다. 왼쪽 그래프(training error)를 보면, 더 깊은 56층 네트워크(붉은색 선)가 20층 네트워크(노란색 선)보다 훈련 오류가 더 높습니다. 당연히 오른쪽 그래프(test error)에서도 56층 네트워크의 테스트 오류가 더 높게 나타납니다. 상식적으로 더 깊은 네트워크가 최소한 얕은 네트워크만큼의 성능은 내야 하는데, 실제로는 그렇지 못했던 것입니다.\n이 논문은 바로 이 \u0026lsquo;성능 저하\u0026rsquo; 문제를 해결하기 위해 **깊은 잔차 학습 (Deep Residual Learning)**이라는 새로운 프레임워크를 제안합니다. 간단히 말해, 네트워크가 처음부터 정답을 완벽하게 맞추려고 애쓰는 대신, 이미 알고 있는 정보(입력값)를 바탕으로 \u0026lsquo;차이(residual)\u0026lsquo;만을 학습하도록 구조를 바꾼 것입니다. 이 혁신적인 방법으로 저자들은 이전에 불가능하다고 여겨졌던 152층 깊이의 네트워크를 성공적으로 훈련시켰고, 당시 이미지 인식 대회인 ILSVRC 2015에서 1위를 차지하는 쾌거를 이루었습니다.\n기술 설명 ResNet의 핵심 아이디어는 어떻게 \u0026lsquo;차이\u0026rsquo;만을 학습하게 만드는 것일까요? 바로 **지름길 연결 (Shortcut Connection)**이라는 구조를 통해 구현됩니다.\n기존의 신경망은 입력값 x가 여러 층을 순서대로 통과하며 복잡한 함수 H(x)를 학습하려고 했습니다. 예를 들어, 고양이 사진(x)을 보고 \u0026lsquo;고양이\u0026rsquo;라는 정답(H(x))을 바로 찾아내려는 방식이었죠.\n하지만 ResNet은 생각을 바꿨습니다. \u0026lsquo;어차피 입력값 x가 있는데, 굳이 H(x) 전체를 새로 배울 필요가 있을까? 그냥 x에다가 약간의 정보만 더해서 정답을 만들면 되지 않을까?\u0026rsquo; 라는 접근입니다. 그래서 네트워크가 학습해야 할 목표를 H(x)에서 F(x) = H(x) - x로 바꿉니다. 여기서 F(x)가 바로 입력값과 정답 사이의 \u0026lsquo;차이\u0026rsquo;, 즉 **잔차 (residual)**입니다. 네트워크는 이 잔차 F(x)를 학습한 뒤, 원래 입력값 x를 더해 최종 결과(F(x) + x)를 만들어냅니다.\n위 그림은 이 \u0026lsquo;잔차 학습\u0026rsquo;의 기본 단위를 보여줍니다.\n입력값 x가 두 갈래로 나뉩니다. 한쪽은 기존처럼 가중치 층(weight layer)을 통과하며 복잡한 변환, 즉 F(x)를 학습합니다. 다른 한쪽은 아무런 변환 없이 그대로 건너뛰는 \u0026lsquo;지름길(shortcut)\u0026lsquo;을 따라갑니다. 이를 **항등 매핑 (identity mapping)**이라고 부릅니다. 마지막에 두 결과(F(x)와 x)가 더해져 최종 출력(F(x) + x)이 됩니다. 이 구조가 왜 효과적일까요? 만약 어떤 층에서 아무것도 학습할 필요 없이 입력값을 그대로 전달하는 것이 최선인 상황이라면, 기존 네트워크는 여러 층을 거치며 입력과 출력이 똑같아지는 복잡한 변환을 학습해야만 했습니다. 이는 매우 어려운 일이었죠. 하지만 ResNet 구조에서는 네트워크가 잔차 F(x)를 그냥 \u0026lsquo;0\u0026rsquo;으로 만들기만 하면 됩니다. 즉, 가중치를 0으로 만들어 아무것도 바꾸지 않으면, 자연스럽게 입력 x가 그대로 출력으로 나가게 됩니다. 훨씬 쉬운 방법으로 최적의 해를 찾을 수 있는 것입니다.\n다음은 실제 네트워크 구조입니다.\nVGG-19 (왼쪽): 당시 표준으로 여겨지던 깊은 네트워크 구조입니다. 34-layer plain (중간): VGG-19의 모델을 따라 단순히 층을 깊게 쌓은 일반적인 네트워크입니다. 이 구조에서 성능 저하 문제가 발생합니다. 34-layer residual (오른쪽): 중간의 \u0026lsquo;plain\u0026rsquo; 네트워크와 똑같은 구조에 \u0026lsquo;지름길 연결(shortcut connection)\u0026lsquo;만 추가한 ResNet 구조입니다. 굽은 화살표들이 바로 이 지름길을 나타냅니다. 더 깊은 네트워크(50층 이상)를 효율적으로 만들기 위한 병목 (Bottleneck) 구조는 다음과 같습니다.\n기존의 블록(왼쪽)이 3x3 필터의 합성곱 층 두 개로 이루어졌다면, 병목 블록(오른쪽)은 1x1, 3x3, 1x1 합성곱 층 세 개로 구성됩니다. 첫 번째 1x1 합성곱은 채널 수를 줄여 계산량을 감소시키고(병목처럼 입구가 좁아짐), 3x3 합성곱으로 핵심적인 연산을 수행한 뒤, 마지막 1x1 합성곱으로 다시 채널 수를 원래대로 복원하는 방식입니다. 이 구조 덕분에 층은 더 깊게 쌓으면서도 전체적인 계산 복잡도는 VGG 네트워크보다 낮게 유지할 수 있었습니다. 실험 저자들은 제안한 ResNet의 효과를 증명하기 위해 이미지넷(ImageNet)과 CIFAR-10이라는 대표적인 이미지 데이터셋으로 다양한 실험을 진행했습니다.\n이미지넷(ImageNet) 분류 실험 이미지넷은 1000개의 클래스(종류)로 이루어진 128만 장의 방대한 이미지 데이터셋입니다.\n위 결과들은 ResNet이 성능 저하 문제를 어떻게 해결하는지 보여줍니다.\n왼쪽 그래프: 일반(plain) 네트워크의 경우, 34층(붉은색 선)이 18층(하늘색 선)보다 훈련 오류와 검증 오류 모두 더 높습니다. 전형적인 성능 저하 현상입니다. 오른쪽 그래프: ResNet의 경우, 상황이 역전됩니다. 34층 ResNet(붉은색 선)이 18층 ResNet(하늘색 선)보다 오류율이 훨씬 낮습니다. 깊이가 깊어질수록 성능이 향상된 것입니다. 위 표는 이 결과를 수치로 보여줍니다. 일반 네트워크는 18층(27.94%)에서 34층(28.54%)으로 갈 때 오류율이 높아졌지만, ResNet은 18층(27.88%)에서 34층(25.03%)으로 갈 때 오류율이 2.8%나 크게 감소했습니다.\n더 깊은 ResNet 모델들의 성능은 위 두 표와 같습니다. 저자들은 앞에서 설명한 \u0026lsquo;병목\u0026rsquo; 구조를 사용해 50층, 101층, 그리고 무려 152층에 달하는 ResNet을 만들었습니다. 34층(25.03%)부터 시작해 50층(22.85%), 101층(21.75%), 152층(21.43%)으로 깊어질수록 top-1 오류율이 꾸준히 감소하는 것을 볼 수 있습니다. 성능 저하 문제없이 깊이의 이점을 취한 것입니다.\n특히 152층 ResNet은 단일 모델만으로 4.49%의 top-5 검증 오류율을 달성했는데, 이는 당시 다른 여러 모델을 합친 앙상블(ensemble) 결과보다도 좋은 성적이었습니다.\n최종 대회 결과, 여러 ResNet 모델을 앙상블하여 이미지넷 테스트 데이터셋에서 **3.57%**라는 경이로운 top-5 오류율을 기록하며 ILSVRC 2015 대회에서 1위를 차지했습니다.\nCIFAR-10 분석 및 1000층 이상의 네트워크 CIFAR-10 데이터셋을 이용한 실험에서도 비슷한 결과가 나타났습니다. 위 그래프는 CIFAR-10에서의 훈련 과정을 보여줍니다.\n왼쪽 (plain networks): 일반 네트워크는 20층에서 56층으로 깊어질수록 오류율이 점점 높아지는 성능 저하 현상을 보입니다. 중간 (ResNets): 반면, ResNet은 20층부터 110층까지 깊이가 증가할수록 오류율이 꾸준히 감소합니다. 오른쪽 (110-layer vs 1202-layer): 저자들은 여기서 더 나아가 1202층이라는 극단적으로 깊은 네트워크를 훈련시키는 데 성공했습니다. 훈련 오류는 0.1% 미만으로 매우 낮았지만(오른쪽 그래프 아래쪽 선), 테스트 오류(7.93%)는 110층 모델(6.43%)보다 다소 높게 나타났습니다. 이는 작은 데이터셋에 비해 모델이 너무 거대해서 발생한 과적합(overfitting) 때문으로 분석됩니다. 위 그래프는 각 층의 응답(출력값)의 표준편차를 분석한 것입니다.\n그래프를 보면 전반적으로 ResNet(붉은색, 검은색 선)의 응답값이 일반 네트워크(노란색, 분홍색 선)보다 작습니다. 이는 ResNet의 잔차 함수가 일반적으로 \u0026lsquo;0\u0026rsquo;에 가까운 값을 갖는다는 가설을 뒷받침합니다. 즉, 각 층이 신호를 크게 바꾸기보다는 조금씩만 수정한다는 의미입니다. 또한 ResNet-20, 56, 110을 비교해보면, 네트워크가 깊어질수록 각 층의 응답값이 더 작아지는 경향을 보입니다. 더 많은 층이 협력하여 신호를 조금씩 점진적으로 바꾼다는 것을 시사합니다. 객체 탐지(Object Detection) 실험 ResNet은 단순히 이미지를 분류하는 것을 넘어, 이미지 속 특정 물체의 위치를 찾아내는 객체 탐지 과제에서도 뛰어난 성능을 보였습니다. 아래 표들은 기존의 VGG-16 네트워크를 ResNet-101로 교체했을 때의 성능 향상을 보여줍니다. 특히 어려운 COCO 데이터셋에서 mAP(객체 탐지 성능의 주요 척도)가 28%나 상대적으로 향상되었습니다. 이는 ResNet이 학습한 표현(representation) 자체가 매우 우수하다는 것을 증명합니다.\n결론 성능 저하 (Degradation) 문제 정의 및 해결: 이전까지는 명확하게 설명되지 않았던, 네트워크가 깊어질수록 훈련이 더 어려워지는 문제를 \u0026lsquo;성능 저하\u0026rsquo;로 명확히 정의하고, 이를 \u0026lsquo;잔차 학습(Residual Learning)\u0026lsquo;이라는 혁신적인 아이디어로 해결했습니다. 초심층 신경망 (Extremely Deep Network)의 가능성 제시: \u0026lsquo;지름길 연결(Shortcut Connection)\u0026lsquo;이라는 간단하면서도 강력한 구조를 통해 152층, 나아가 1000층이 넘는 매우 깊은 신경망의 훈련을 가능하게 했습니다. 이는 딥러닝 모델의 깊이에 대한 기존의 한계를 완전히 무너뜨린 것입니다. 다양한 분야에서의 SOTA (State-of-the-art) 달성: 제안된 ResNet은 이미지 분류뿐만 아니라 객체 탐지, 분할 등 다양한 컴퓨터 비전 분야에서 압도적인 성능을 보여주며 새로운 표준 모델로 자리 잡았습니다. FYI ResNet을 구현한 좋은 예시 링크가 있어서 가져와봤습니다!\nhttps://github.com/JayPatwardhan/ResNet-PyTorch\n","permalink":"https://mookjsi.github.io/posts/paper-review-resnet/","summary":"CVPR 2016에서 발표된 \u0026lsquo;Deep Residual Learning for Image Recognition\u0026rsquo; 논문에 대한 심층 리뷰입니다. 이 포스트에서는 딥러닝의 \u0026lsquo;성능 저하(Degradation)\u0026rsquo; 문제를 해결한 잔차 학습(Residual Learning)의 핵심 개념, 네트워크 구조, 그리고 실험 결과를 알기 쉽게 분석합니다.","title":"ResNet - 더 깊은 신경망을 위한 잔차 학습"},{"content":"\nSlide 1: Title Slide Title: Advanced Policy Gradient Methods: TRPO \u0026amp; PPO Session: YBIGTA SUMMER SESSION Presenter: DS 26 Jungmook Kang Date: 2025.08.26 Slide 2-5: Why does Reinforcement Learning sometimes fail? Main Question: \u0026ldquo;Why does reinforcement learning sometimes break down?\u0026rdquo; Problem: Basic policy gradient methods (like A2C) have limitations. Making drastic changes to the policy—the agent\u0026rsquo;s decision-making criteria—can cause significant problems. Key Issues: Unstable Learning: If an update step is too large and in the wrong direction, the agent\u0026rsquo;s performance can collapse, making recovery impossible. This is more damaging than in supervised learning because a bad policy affects the distribution of states and rewards the agent will see in the future. Changing Data Distribution: As the policy changes, the data (states, actions, rewards) it collects also changes. This non-stationarity of the input data makes learning difficult. Example Analogy: A robot learning to walk. If it\u0026rsquo;s walking well and tries a new strategy by swinging its legs too wide, it will fall. Once it has fallen, it can no longer collect useful data about walking, and learning stops. Slide 6-8: What is TRPO? \u0026ldquo;Let\u0026rsquo;s not change the policy too much!\u0026rdquo; Concept: Trust Region Policy Optimization (TRPO). Core Idea: When updating the policy, TRPO establishes a \u0026ldquo;trust region\u0026rdquo; or a \u0026ldquo;safe zone\u0026rdquo; to ensure the new policy does not stray too far from the old one. Goal: \u0026ldquo;Maximize policy performance under the constraint that the difference (distance) between the old and new policies does not exceed a certain value, δ.\u0026rdquo; This prevents the destructive, large policy updates that can lead to performance collapse. Slide 9-11: The Mathematical Expression of TRPO This slide introduces the core optimization problem of TRPO. The goal is to maximize an objective function, subject to a constraint.\nObjective Function (What to Maximize): The expression $\\mathbb{E}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}\\hat{A}]$ represents the expected advantage of the new policy $\\pi_{\\theta}$ relative to the old policy $\\pi_{\\theta_{old}}$. This is a surrogate objective that uses importance sampling to estimate the performance of the new policy using data collected from the old one. The term $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ is the importance sampling ratio. The full objective function is: $$ L_{\\theta_{old}}^{IS}(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}\\mid s_{t})}{\\pi_{\\theta_{old}}(a_{t}\\mid s_{t})}\\hat{A}_{t}\\right] $$ Constraint (The \u0026ldquo;Trust Region\u0026rdquo;): The term $\\mathbb{E}[D_{KL}(\\pi_{\\theta_{old}}||\\pi_{\\theta})]\\le\\delta$ is the constraint that defines the trust region. $D_{KL}$ stands for Kullback-Leibler (KL) Divergence. It\u0026rsquo;s a way to measure the \u0026ldquo;distance\u0026rdquo; or difference between two probability distributions. Here, it measures how much the new policy $\\pi_{\\theta}$ has changed from the old policy $\\pi_{\\theta_{old}}$. By keeping this value less than a small constant $\\delta$, we ensure the policy update is small and stays within a \u0026ldquo;trusted\u0026rdquo; region where the performance estimate is reliable. The full optimization problem solved by TRPO is:\n$$ \\underset{\\theta}{\\text{maximize}} \\quad \\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}\\hat{A}_{t}\\right] $$ $$ \\text{subject to} \\quad \\hat{\\mathbb{E}}_{t}\\left[KL\\big(\\pi_{\\theta_{old}}(\\cdot|s_{t}),\\,\\pi_{\\theta}(\\cdot|s_{t})\\big)\\right]\\le\\delta $$ Slide 12: Limitations of TRPO Problem: While TRPO\u0026rsquo;s core idea was groundbreaking for stabilizing RL, it wasn\u0026rsquo;t widely adopted in practice. Major Drawbacks: High Computational Cost: It uses a complex, second-order optimization technique called the \u0026ldquo;conjugate gradient method\u0026rdquo; which is computationally heavy. Difficult to Implement: The algorithm is complex to code and debug. Poor Performance on Certain Tasks: It struggled with tasks involving complex visual inputs (requiring deep CNNs), such as the Atari benchmark. It is also hard to use with architectures that have multiple outputs (e.g., a policy and a value function). Slide 13-14: PPO: The Practical Version of TRPO Concept: Proximal Policy Optimization (PPO). Core Idea: PPO inherits TRPO\u0026rsquo;s philosophy of \u0026ldquo;let\u0026rsquo;s change the policy a little at a time\u0026rdquo; but replaces the complex and strict trust region constraint with a simpler mechanism that is easier to implement and more efficient. How it Improves on TRPO: Instead of using a hard constraint, PPO modifies the objective function to penalize policies that move too far away from the old policy. The most common technique for this is called Clipping. Impact: PPO is now one of the most widely used policy optimization algorithms due to its simplicity, stability, and strong performance. Slide 15-19: PPO\u0026rsquo;s Core Idea: Clipping This section details the PPO-Clip objective function, which is the key to its success.\nPPO-Clip Objective Function: $$ L^{CLIP}(\\theta)=\\hat{\\mathbb{E}}\\left[\\min\\big(r_{t}(\\theta)\\hat{A}_{t},\\ \\operatorname{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t}\\big)\\right] $$ Breakdown of Terms:\n$r_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$: This is the same probability ratio used in TRPO. It indicates whether an action is more or less likely under the new policy compared to the old one. $clip(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)$: This is the clipping mechanism. It forces the probability ratio $r_t(\\theta)$ to stay within a small range, $[1-\\epsilon, 1+\\epsilon]$ (e.g., [0.8, 1.2]). This acts as a safety rail, preventing the ratio from becoming too large or too small. $min(\u0026hellip;)$: The final objective takes the minimum of two values: The original objective ($r_{t}(\\theta)\\hat{A}_{t}$) The clipped objective ($clip(\u0026hellip;)\\hat{A}_{t}$) By taking the minimum, PPO creates a pessimistic, lower-bound estimate of the policy\u0026rsquo;s performance. This effectively puts a cap on the benefit you can get from a single update, which discourages excessively large changes to the policy. Analogy: It\u0026rsquo;s like setting a study plan. No matter how much you enjoy one subject, you cap its study time at \u0026ldquo;a maximum of 3 hours per day.\u0026rdquo; This prevents you from neglecting other subjects and keeps your overall learning balanced and stable.\nSlide 20-21: The Power and Application of PPO Why it\u0026rsquo;s popular: PPO is simple, powerful, and stable, making it the de-facto standard for reinforcement learning. Where is PPO used?: Robotics: Training stable and precise movements. Game Playing: Achieving superhuman performance in complex games. Large Language Models (LLMs): Fine-tuning models like ChatGPT to produce more helpful and safer responses (a process known as Reinforcement Learning from Human Feedback or RLHF). Major Users: Almost all leading AI companies, including OpenAI, DeepMind, Meta, and NVIDIA, use PPO as a core algorithm. ","permalink":"https://mookjsi.github.io/posts/trpoppo/","summary":"Session notes that explain TRPO\u0026rsquo;s trust region and PPO\u0026rsquo;s clipping with intuition and equations, and summarize limitations and practical applications.","title":"Advanced Policy Gradient Methods: TRPO \u0026 PPO"},{"content":"I\u0026rsquo;m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025.\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like \u0026ldquo;not A, but B\u0026rdquo;). The authors\u0026rsquo; solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nThis is my presentation on \u0026ldquo;Promptriever: Instruction-Trained Retrievers,\u0026rdquo; which I put together for the Information Theory and Machine Learning Lab.\nFirst, let\u0026rsquo;s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It\u0026rsquo;s also worth noting this work was accepted as a poster at ICLR 2025.\nI want to start with the paper\u0026rsquo;s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be \u0026ldquo;intelligent, instruction-following partners, not just data finders.\u0026rdquo;\nHere are the other co-authors who contributed to this research.\nI\u0026rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We\u0026rsquo;ll start with the motivation behind the research.\nSo, how do current search engines \u0026ldquo;think\u0026rdquo;? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what\u0026rsquo;s the problem?\nThis example makes the problem obvious. Imagine you need a laptop that is not a MacBook and costs under $1000. A standard retriever sees the keywords \u0026ldquo;MacBook\u0026rdquo; and \u0026ldquo;under $1000\u0026rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant.\nThis leads to a frustrating user experience. You\u0026rsquo;re forced to keep tweaking keywords and filters just to find what you want.\nThis is where Promptriever comes in. It doesn\u0026rsquo;t just use semantic similarity. Instead, it operates on \u0026ldquo;dynamic relevance definitions,\u0026rdquo; which allows for a much more intelligent process.\nLet\u0026rsquo;s look at the same query again, but with Promptriever. It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.\nThe key idea here is that Promptriever \u0026ldquo;dynamically adjusts relevance based on your natural language instructions.\u0026rdquo; It\u0026rsquo;s not just matching words; it\u0026rsquo;s understanding commands.\nWe\u0026rsquo;ve seen what it does, which leads to the next question: \u0026ldquo;But how on earth was this made?\u0026rdquo; Let\u0026rsquo;s get into the technical details.\nNow, we\u0026rsquo;ll dive into the second section, where I\u0026rsquo;ll break down the Promptriever model\u0026rsquo;s architecture and training.\nThe architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder.\nThe main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model\u0026rsquo;s instruction-following ability. So how did they keep the model intelligent?\nThe answer, as they stated in their core message, lies in their \u0026ldquo;novel training data, which makes ignoring commands impossible for correct answers.\u0026rdquo;\nFor context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.\nPromptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. This is what lets them train the model on complex, instruction-based prompts.\nThe most clever part of their training data is the Instruction-Negative. This is a document that\u0026rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.\nHere’s a perfect example. For the query \u0026ldquo;What is the capital of France?,\u0026rdquo; a general article about Paris is a good result. But if you add the instruction \u0026ldquo;mention its average annual rainfall,\u0026rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.\nThrough this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.\nThe authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.\nCreating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.\nNow for the final section, \u0026ldquo;Experiments,\u0026rdquo; where we\u0026rsquo;ll look at the results.\nFor a fair comparison, they ran an \u0026ldquo;Apples-to-Apples\u0026rdquo; test against RepLLaMA, using the exact same data and hyperparameters.\nThey used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions.\nThe results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.\nThis table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability.\nOn the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nThis is where it gets interesting. When given a helpful prompt, Promptriever\u0026rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely \u0026ldquo;promptable.\u0026rdquo;\nThis table looks at the standard deviation of scores across different prompts. Promptriever\u0026rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nThe ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nThe authors also proved their training \u0026ldquo;recipe\u0026rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, \u0026ldquo;A golden recipe doesn\u0026rsquo;t discriminate against ingredients!\u0026rdquo;\nFinally, let\u0026rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper\u0026rsquo;s scope.\nIn the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","permalink":"https://mookjsi.github.io/posts/paper-review-promptriever/","summary":"A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","title":"Promptriever - Instruction-Trained Retrievers"},{"content":"This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the geometry of the loss landscape and a model\u0026rsquo;s ability to generalize.\nThe core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, flat regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.\nThis is the title slide for my presentation on \u0026ldquo;Simplifying neural nets by discovering flat minima,\u0026rdquo; which I prepared for our lab meeting.\nFirst, let\u0026rsquo;s credit the authors: Sepp Hochreiter and Jürgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?\nHere are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).\nThis quote from the authors perfectly captures the paper\u0026rsquo;s central thesis: \u0026ldquo;Find wide, not sharp, minima - and your network generalizes for free.\u0026rdquo; It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.\nMy presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.\nSo, let\u0026rsquo;s start with the motivation. Why was this research necessary?\nTo understand the paper\u0026rsquo;s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network\u0026rsquo;s weights (so-called \u0026ldquo;priors\u0026rdquo;). This paper marked a shift, arguing that we should focus on the geometry of the error surface rather than imposing assumptions about the weights themselves.\nThis 3D plot illustrates the core concept perfectly. On the right, we see a \u0026ldquo;sharp\u0026rdquo; minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a \u0026ldquo;flat\u0026rdquo; minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.\nHere\u0026rsquo;s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It\u0026rsquo;s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It\u0026rsquo;s much easier to find a good spot, and moving around a little doesn\u0026rsquo;t drastically change your altitude (or your model\u0026rsquo;s error). This robustness is linked to lower model complexity.\nSo, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a \u0026ldquo;good\u0026rdquo; prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.\nThe \u0026ldquo;Flat Minima\u0026rdquo; approach offers solutions to these problems. First, it doesn\u0026rsquo;t require any pre-chosen priors; the geometry of the solution space itself defines what a \u0026ldquo;simple\u0026rdquo; model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.\nNow, let\u0026rsquo;s formalize the problem by defining the tasks and architectures this method applies to.\nThe basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.\nThe model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).\nBuilding on that, we can define the set of \u0026ldquo;acceptable\u0026rdquo; solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors w for which the training MSE is less than or equal to this tolerance.\nNow we get to the core of the algorithm\u0026rsquo;s construction. For a given weight vector w, we define a \u0026ldquo;box\u0026rdquo; around it. For each individual weight $w_{ij}$, we find the maximum amount $\\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\\Delta w_{ij}$ for each weight.\nThese intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the \u0026ldquo;Flat Minima\u0026rdquo; as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.\nSo, how do we actually find these large-volume minima? That brings us to the algorithm itself.\nThe main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\\Delta w$. A larger volume signifies a flatter, more desirable minimum.\nThis slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.\nMaximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.\nThis new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.\nTo build the algorithm, we first need to mathematically define \u0026ldquo;flatness\u0026rdquo;. We start by defining the change in the network\u0026rsquo;s output, $ED(w, \\delta w)$, that results from a small change in weights, $\\delta w$.\nTo make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \\delta w)$, in terms of the original output and the first derivatives (gradients).\nSubstituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.\nThis leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don\u0026rsquo;t lead to large output changes.\nThe second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as \u0026ldquo;spherical\u0026rdquo; as possible by ensuring that perturbations to each weight contribute equally to the total output change.\nHere is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.\nBy rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).\nThis slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.\nBy solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\\Delta w_{ij}|$. This formula depends on the network\u0026rsquo;s output gradients.\nLet\u0026rsquo;s quickly recap the algorithm\u0026rsquo;s goal. We aim to maximize the box volume $\\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.\nNow that we have a formula for the size of the box edges, $\\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network\u0026rsquo;s derivatives. This slide shows that connection, approximating the log of $\\Delta w_{ij}$ with the log of the derived formula.\nThis slide restates the formula for the size of the perturbation $|\\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.\nPlugging the expression for $\\Delta w_{ij}$ into our cost function $B(w, D_0) = -\\sum \\log \\Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the \u0026ldquo;flatness\u0026rdquo; of the minimum.\nThe complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\\lambda$ balances the two.\nTo minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network\u0026rsquo;s output, which would typically be very expensive to compute.\nHowever, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$—the same as standard backpropagation—using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.\nTo recap the key advantages: the \u0026ldquo;Flat Minima\u0026rdquo; method is appealing because it uses geometry instead of priors to define simplicity, it\u0026rsquo;s computationally efficient, and it naturally performs network pruning for better generalization.\nThe paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.\nIn the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.\nThe results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.\nThe second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.\nThe final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.\nThe results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.\n","permalink":"https://mookjsi.github.io/posts/paper-review-flatminima/","summary":"A detailed review of the classic 1994 paper by Hochreiter \u0026amp; Schmidhuber, \u0026lsquo;Simplifying Neural Nets by Discovering Flat Minima\u0026rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization.","title":"Simplifying Neural Nets by Discovering Flat Minima"},{"content":"Hello everyone, I\u0026rsquo;m Jungmook Kang at Yonsei University. Today, I\u0026rsquo;m excited to share a review of a fascinating paper that was recently accepted to NeurIPS, which delves into the theoretical underpinnings of the attention mechanism. Let\u0026rsquo;s get started.\nThe title of the paper is \u0026ldquo;Max-Margin Token Selection in Attention Mechanism.\u0026rdquo; This work comes from our lab here at Yonsei University, and it explores the question of why attention works the way it does.\nSo, let\u0026rsquo;s begin with the motivation for this research. What prompted us to look into this?\nWe all know that the attention mechanism is incredibly effective. For a sentence like \u0026ldquo;The pizza came out of the oven and it tasted good!\u0026rdquo;, attention can correctly associate \u0026ldquo;it\u0026rdquo; with \u0026ldquo;pizza.\u0026rdquo; It works remarkably well\u0026hellip; but the story doesn\u0026rsquo;t end there.\nWhen we look at the standard Transformer architecture, which powers models like BERT and GPT, we see a complex system of encoders and decoders with multiple layers of multi-head attention. This raises some fundamental questions: Why is this architecture so successful? And more specifically, what is happening to the model\u0026rsquo;s weights during the optimization process that leads to this success?\nAnalyzing this architecture theoretically is very challenging. The optimization problem is both non-linear, due to functions like Softmax, and non-convex. This makes it extremely difficult to describe the training dynamics with traditional methods.\nTo tackle this, we turn to the concept of \u0026ldquo;Implicit Bias.\u0026rdquo; This is a phenomenon where the optimization algorithm itself—like Gradient Descent—has a preference for a certain type of solution, even when there\u0026rsquo;s no explicit regularization term in the loss function telling it to do so.\nThis leads us to the central question of our research: What is the implicit bias of the attention model? What kind of solution does it naturally favor?\nWe can get a clue from simpler models. It\u0026rsquo;s a known result from Soudry et al. (2018) that when you train a linear model with logistic loss using gradient descent, the solution tends to converge towards the one you\u0026rsquo;d get from a hard-margin Support Vector Machine (SVM). The goal of a hard-margin SVM is to find the hyperplane that maximizes the distance, or margin, to the nearest data points.\nSo, we formed a hypothesis: Could it be that the implicit bias of the much more complex attention model is also related to a hard-margin SVM solution?\nHere\u0026rsquo;s a core idea. We observe that attention has a tendency to focus on a small number of important tokens. We believe the principle guiding this selection is related to maximizing a margin. For instance, to distinguish between a cat and a dog, you might focus on the shape of the \u0026rsquo;ears\u0026rsquo; because that feature provides the clearest separation, the largest \u0026lsquo;margin,\u0026rsquo; between the two classes.\nThis brings us to the first main part of the analysis, where we explore the idea of global and local margin maximization within the attention mechanism.\nTo make the analysis tractable, we start with a simplified Softmax Attention model. The function f(X) takes an input sequence X and computes a final output. The key learnable parameters are p, the query embedding, and W, the key-query weights. In Transformers, p can be thought of as the embedding for the [CLS] token.\nOur training objective is to minimize the empirical risk for a binary classification task. We use a standard setup with input sequences X and labels Y, and a decreasing loss function like logistic loss. The model\u0026rsquo;s prediction f(Xi) is a function of the learnable parameters v, p, and W.\nHere, I\u0026rsquo;ll just quickly go over some of the mathematical notation we use throughout the paper. We use standard conventions for vectors and matrices, and we\u0026rsquo;ll use simplified notations like L(p) to denote the risk when v and W are fixed.\nOur approach involves studying the regularization path, which is the path of the solution p as we increase its allowed norm, R. This path mirrors the trajectory of gradient descent. We analyze a final attention model where the trainable parameters W and p jointly influence the softmax, leading to similar optimization dynamics.\nThis brings us to Lemma 1, which is a crucial simplification. It states that the weight matrix W is effectively a rank-1 matrix and its learning dynamics are completely determined by the vector p. This is very important because it means we can fix W and focus our entire analysis on the optimization of p.\nSo, we can formally define our problem. We are exploring the training risk L(v, p) where the key embeddings Ki are derived from the input Xi. For the analysis, we can even treat Xi and Ki as separate entities.\nTo ensure our proofs hold, we need Assumption A. This is a standard assumption that requires our loss function l to be well-behaved—specifically, it must be strictly decreasing and have a Lipschitz-continuous derivative. Common functions like logistic loss and exponential loss satisfy this condition.\nNow, we introduce the central piece of our theory: a hard-margin SVM problem, which we call ATT-SVM. The goal here is to find a direction p that, for each input sequence, separates one chosen token (k_iαi) from all other tokens (k_it) by a margin of at least 1. We will show that the optimization of softmax attention is effectively trying to solve this problem.\nTo formalize this, we define the score of a token (γ_it) as its contribution to the final correct prediction. The optimal tokens are naturally the ones with the highest scores for each input. The Globally-Optimal Max-Margin (GMM) direction, denoted p^mm*, is then the solution to our ATT-SVM problem when we choose these optimal tokens.\nTheorem 1 is our first main result. It states that the regularization path—the sequence of solutions as we increase the norm R—converges in direction to this GMM solution, p^mm*. In simple terms, as the optimization progresses and the norm of p grows, its direction aligns with the direction that best separates the highest-scoring tokens from the rest.\nTheorem 2 looks at the convergence of gradient descent directly. It says that under our assumptions, the norm of the weight vector p will grow towards infinity during training. Critically, for the simple case of a single training example (n=1), the direction of p converges to the GMM direction. However, for n \u0026gt; 1, it\u0026rsquo;s possible for the optimization to get trapped in a local minimum.\nTo test these theorems, we designed a simple experiment. We created synthetic data with three tokens, where we can visualize their key embeddings in 2D and independently control their scores using a third dimension. This lets us clearly see what\u0026rsquo;s happening.\nThese plots show the results. In Figure (a), where the conditions for global convergence are met, we see that all gradient descent trajectories (the grey arrows) correctly converge to the GMM direction (the red dashed line). In (b), we\u0026rsquo;ve changed the scores so that a locally optimal solution exists, and we see some trajectories get stuck there instead. Figure (c) shows the more complex scenario with multiple inputs, where finding the joint GMM solution is harder.\nThis brings us to the idea of locally-optimal tokens. These are tokens that might not be the highest-scoring ones globally, but they form a stable solution for the ATT-SVM problem. The direction associated with them is a locally-optimal max-margin (LMM) direction.\nTo talk about local convergence, we introduce the geometric concept of a cone. A cone around a direction q is simply the set of all vectors that are \u0026ldquo;close\u0026rdquo; in angle to q. The diagram illustrates an initialization p(0) that lies within the cone of an LMM direction p^mm.\nTheorem 3 formalizes local convergence. It states that if you initialize the gradient descent p(0) within the cone of an LMM direction, the optimization will stay within that cone and ultimately converge to that specific LMM direction.\nLooking back at our experiment in Figure (b), we can now understand it better through Theorem 3. The trajectories that don\u0026rsquo;t find the global optimum are the ones that were initialized inside the cone of the locally-optimal solution (the blue square), and as the theorem predicts, they converge to it.\nTheorem 4 provides a tightness guarantee. It essentially says that the LMM directions are the only stable convergence points. If you start in any direction q that is not an LMM direction, the optimization path will eventually move away from q.\nThis diagram illustrates Theorem 4. If our initial parameter p(0) is in a cone (the grey one) that does not contain any LMM or GMM direction, the optimization process will not converge in that direction. It is implicitly forced to seek out one of the valid max-margin solutions.\nTo summarize the first half: we\u0026rsquo;ve shown that when training only the attention parameter p, the implicit bias of gradient descent drives the solution towards a hard-margin SVM that separates tokens. Now, what happens if we also learn the prediction head v at the same time?\nThis leads to the second part of our analysis: the joint convergence of the prediction head v and the attention weights p.\nThe high-level intuition is that the model is linear in v, so the optimization with respect to v also has an implicit bias towards a standard max-margin classifier solution. The features for this classifier, x_i^p, are the outputs of the attention layer.\nThe challenge here is that the features r that v sees are determined by p, which itself is changing during training. This creates a coupled dynamic. We analyze this by separating the problem into cases based on whether the selected features are support vectors for the v-classifier or not.\nTheorem 5 addresses the case where we assume the selected tokens are all support vectors. We introduce Assumption C, which formalizes that if the attention doesn\u0026rsquo;t perfectly select the optimal token, the classification margin for v shrinks. Under this assumption, we find that v converges to the direction of a standard SVM solution, and p converges to the direction of our ATT-SVM solution.\nThis raises a natural question: Does every selected token really need to be a support vector, sitting right on the margin? That seems like a very strict condition. Intuitively, for the data points that are not support vectors, we don\u0026rsquo;t need to maximize their margin. We just need to make sure they are on the correct side of the decision boundary. This allows us to define a \u0026ldquo;relaxed\u0026rdquo; version of our ATT-SVM problem.\nTheorem 6 presents this more general result. The ATT-SVM problem is relaxed: for tokens corresponding to support vectors (i ∈ S), we require a margin of 1, but for non-support vectors (i ∈ S-bar), we only require a margin of 0 (correct classification). We only need Assumption C to hold for the support vectors. The result is that v still converges to the max-margin v^mm, and p converges to the solution of this new, relaxed SVM problem, p^relax.\nWe ran experiments for this joint convergence case as well. The plots show the trajectories for the attention weights p and the classifier head v. Plot (a) shows a case aligning with Theorem 5, where all inputs are support vectors. Plot (b) shows a case for Theorem 6, where one input is not a support vector. Plot (c) shows that the softmax probability for the optimal token quickly goes to 1, confirming that the attention is learning to select specific tokens as predicted.\nSo, to summarize the entire theoretical part: Whether we train the attention weights p alone or jointly with the classifier head v, the implicit bias of the optimization consistently pushes the model towards a hard-margin SVM solution. This provides a strong theoretical explanation for why attention learns to become sparse and focus on the most discriminative tokens.\nFinally, let\u0026rsquo;s look at a few more experiments to see these principles in action on more complex tasks.\nThis slide provides an experimental comparison between Normalized Gradient Descent (GD) and Vanilla GD, validating the paper\u0026rsquo;s theory. Plot (a) shows that while both methods learn to focus on a single token (softmax probability approaches 1.0), Normalized GD achieves this sparse attention much faster. Plot (b) demonstrates a key theoretical prediction: the norm of the attention weights ||p|| diverges (grows linearly without bound) when using Normalized GD. This behavior is characteristic of an optimization process seeking a max-margin solution.\nHere, we trained a vision transformer on an image classification task. Figure 7 shows that as training progresses over epochs, the sparsity of the attention map increases (red curve goes down), meaning the model learns to focus on fewer, more important image patches. At the same time, the norm of the attention weights ||W|| steadily increases (blue curve), which is exactly what our theory predicts will happen as the solution converges towards an infinite-norm, max-margin boundary. The images in Figure 6 visually show this focusing effect over time.\nThis final experiment shows how the choice of loss function affects the dynamics. We compare a correlation loss (l(x) = -x) with the logistic loss. The gradient\u0026rsquo;s magnitude depends on the token\u0026rsquo;s score γ differently for each loss. For correlation loss, larger scores get larger gradients, while for logistic loss, the gradient is largest for scores near zero. This results in different trajectories, but as you can see, both are ultimately guided by the same underlying max-margin principle, pushing towards a separating hyperplane.\n","permalink":"https://mookjsi.github.io/posts/paper-review-maxtoken/","summary":"This post reviews the NeurIPS 2025 paper \u0026lsquo;Max-Margin Token Selection in Attention Mechanism.\u0026rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs.","title":"Max-Margin Token Selection in Attention Mechanism"},{"content":"This is my detailed slide-by-slide analysis of the paper Stochastic Approximation to Contrastive Learning, which was submitted to the ICLR 2025 conference.\nThe paper tackles a critical challenge in contrastive learning: its heavy reliance on large batch sizes and the associated computational cost. The authors introduce SACLR, a novel framework inspired by Stochastic Cluster Embedding (SCE) that reformulates the objective using I-divergence. The goal was to enable efficient training with as little as one negative sample, a significant departure from methods like SimCLR.\nWhile the premise is compelling, the paper faced substantial criticism during the open review process regarding its experimental comparisons, novelty, and the substantiation of its claims. Despite rebuttals and additional experiments, it was ultimately rejected. In this review, I\u0026rsquo;ll walk through the method as presented and layer in the context from the public reviews to provide a complete picture of its strengths and weaknesses.\nThis is the title slide for my review of the paper \u0026ldquo;Stochastic Approximation to Contrastive Learning.\u0026rdquo; This presentation was prepared for the Information Theory and Machine Learning Lab at Yonsei University.\nTo begin, I\u0026rsquo;ll recap the core concept of Representation Learning. This field is all about how we represent data, which leads to the fundamental question: what exactly is a \u0026ldquo;representation\u0026rdquo; in the context of machine learning?\nI\u0026rsquo;ll illustrate this with a simple task. Imagine you need to solve the division problem CCV / VI. For most people, this is not immediately obvious.\nNow, consider this problem: 210 / 6. This is likely much easier, and you can quickly determine the answer is 35. The interesting part is that both problems represent the exact same calculation.\nThe key difference lies in the representation of the same numerical information. The first task used Roman numerals, while the second used Arabic numerals. The choice of representation dramatically changes the difficulty of the task.\nThis analogy illustrates a critical point that is central to representation learning: the right representation can make a complex task much easier to solve.\nTo summarize this introductory point: the difficulty of many information processing tasks is highly dependent on how that information is represented.\nSo, the crucial question for us is: how can we obtain a \u0026ldquo;good representation\u0026rdquo; for various machine learning tasks? Today, I\u0026rsquo;ll focus on one prominent approach for achieving this: Self-Supervised Learning.\nSelf-Supervised Learning is a subset of the broader field of Representation Learning.\nA primary motivation for the development of self-supervised methods is the immense cost and effort required to obtain large-scale labeled datasets for traditional supervised learning.\nThe solution that Self-Supervised Learning proposes is to find a way to learn a \u0026ldquo;good representation\u0026rdquo; that captures the essential features of the data using only an unlabeled dataset.\nThis slide illustrates the typical self-supervised learning pipeline. First, a model is trained on a \u0026ldquo;pretext task\u0026rdquo; using a large amount of unlabeled data. The learned model is then transferred and fine-tuned on a \u0026ldquo;downstream task\u0026rdquo; using task-specific (and often limited) labeled data.\nThe goal of the pre-training stage is to transform raw data, which can be a poor representation for a computer (like a raw image of a dog), into a feature vector that is a much better representation for downstream tasks.\nThe central question this paper investigates is the pre-training step: how exactly does the model learn a good representation from unlabeled data?\nWithin the realm of Self-Supervised Learning, I will now narrow the focus to Contrastive Learning. This approach is based on making \u0026ldquo;inter-sample\u0026rdquo; predictions.\nHere is the core mechanic of contrastive learning. We start with an \u0026ldquo;anchor\u0026rdquo; image. We create two different augmented versions, or \u0026ldquo;views,\u0026rdquo; of this anchor, which form a \u0026ldquo;positive pair.\u0026rdquo; We then contrast this with a \u0026ldquo;negative pair,\u0026rdquo; which is formed by the anchor and a view from a completely different image.\nIn summary, the objective of contrastive learning is to learn effective representations by mapping similar data points close to each other in the representation space, while simultaneously pushing dissimilar data points far apart.\nThis slide outlines the flow of the main arguments in this review. I\u0026rsquo;ll begin by discussing the high computational costs that arise from the conventional definition of positive and negative pairs. Then, I will introduce the paper\u0026rsquo;s proposed method, which uses matrix approximation with I-divergence to create a decomposable and stochastic loss, ultimately achieving competitive results with a low batch size and fewer negative pairs.\nNow, we move from the recap to the main introduction of the paper\u0026rsquo;s contribution.\nThe core problem is that while supervised learning is effective, it depends on having extensive labeled data. Self-Supervised Learning is the alternative we are exploring.\nHowever, popular contrastive learning methods like SimCLR require very large batch sizes to ensure a sufficient balance of positive and negative examples. This expends a large amount of computational resources, particularly on the negative pairs. A method called Sog-CLR attempted to address this by mixing an EMA of image similarities into the denominator of the InfoNCE loss.\nTo be more specific, SimCLR\u0026rsquo;s InfoNCE loss operates in a full-batch mode, which is not decomposable in a mini-batch setting. Sog-CLR showed improvement by incorporating a running average for the negative pair estimations.\nThis paper poses the question: even with Sog-CLR\u0026rsquo;s improvements, is there still room for further optimization? It introduces SACLR, a method that uses I-divergence to reformulate the objective into a matrix approximation problem that is decomposable across instance pairs.\nThe key idea behind SACLR is this reformulation using I-divergence, which allows the objective to be decomposed and approximated stochastically. This is presented as an advancement over Sog-CLR\u0026rsquo;s EMA mixing approach.\nThe paper\u0026rsquo;s central claim is that its method, SACLR, can learn high-quality representations with a small batch size and very few negative pairs. While reviewers found the core idea of tackling the large batch size problem to be an interesting and valuable contribution, and saw the novel formulation inspired by Stochastic Cluster Embedding as a strength, they heavily scrutinized the experimental evidence supporting these claims.\nTo understand SACLR, we first need to look at its theoretical foundation: Stochastic Cluster Embedding (SCE). I will now detail the matrix approximation with I-divergence that underpins the method.\nIn SCE, we have embedded data points, like $y_i$ and $y_j$. We define a similarity kernel $q_{ij}$ between these points in the embedded space, typically using a Gaussian or a Student\u0026rsquo;s t-distribution kernel. This value should be close to 1 for similar points.\nWe also define a target similarity matrix, P, which represents the desired similarities in the embedded space. For example, perfectly clustered data would have a block-diagonal P matrix.\nThe goal is to make the learned similarity matrix Q as close as possible to our desirable target matrix P. The question is, how do we measure this closeness?\nThis is where the choice of divergence metric is crucial. t-SNE uses the KL-divergence. In contrast, SCE uses the I-divergence, which includes an additional scaling factor \u0026rsquo;s\u0026rsquo; and linear terms.\nThis slide highlights the formulas for both KL-divergence, used in t-SNE, and the I-divergence with a scaling factor, used in SCE.\nAn important property is that the I-divergence can reduce to the KL-divergence. This happens if the scaling factor \u0026rsquo;s\u0026rsquo; is set to be the inverse of the sum of all kernel similarities, effectively normalizing the Q matrix.\nSCE defines the scaling factor \u0026rsquo;s\u0026rsquo; using a weighted sum controlled by the parameter α, which introduces additional repulsion to improve cluster quality. A key methodological point, raised by Reviewer C43T, was why \u0026rsquo;s\u0026rsquo; is treated as a constant during optimization when it is a function of the embeddings. The authors clarified that they use an interleaving optimization strategy: the model parameters are optimized while \u0026rsquo;s\u0026rsquo; is fixed, and then \u0026rsquo;s\u0026rsquo; is periodically updated based on the new embeddings.\nBy plugging this definition of the weights $w_{ij}$ into the formula for \u0026rsquo;s\u0026rsquo;, the denominator term can be rewritten as a sum of two expectations.\nThese two expectations have clear interpretations: $E_1$ is the expected similarity for pairs drawn from the target distribution P, while $E_2$ is the expected similarity for pairs drawn uniformly at random from all possible pairs.\nThis formulation allows the I-divergence objective to be rewritten in a stochastic form, separating it into an attraction term based on the target distribution and a repulsion term based on the uniform distribution. The terms themselves are simple functions of the similarity $q_{ij}$.\nNow, let\u0026rsquo;s apply this SCE framework to Contrastive Learning. In our setting, for each input $x_i$, we generate two augmented views, $\\tilde{x}{i}^{(1)}$ and $\\tilde{x}{i}^{(2)}$. We denote their embeddings as $\\tilde{y}{i}^{(u)}$ and the similarity between any two embeddings as $q{ij}^{(u,v)}$.\nThe goal in contrastive learning is to make embeddings from the same instance ($i=j$) similar, and embeddings from different instances ($i \\ne j$) dissimilar. We can formally define this as a target tensor $p$, where the target similarity is 1 only for positive pairs ($p_{ii}^{(1,2)}$ and $p_{ii}^{(2,1)}$) and 0 for all other pairs.\nThis slide provides a concrete example of the target tensor $P$ for a case with N=3 instances. The matrices for same-view similarities ($P^{(1,1)}, P^{(2,2)}$) are all zeros, while the matrices for cross-view similarities ($P^{(1,2)}, P^{(2,1)}$) are identity matrices, capturing the positive pair targets.\nTo simplify the math, we can flatten this four-dimensional tensor structure into standard 2D matrices. The $2N \\times 2N$ target matrix $\\psi$ is formed by reorganizing the elements of the tensor $p$. This results in a sparse matrix where the identity matrices from the tensor become off-diagonal blocks.\nFor notational simplicity, we neglect the diagonal elements of the matrices $\\psi$ and $\\phi$ in the approximation, as they are constant for the kernels used and do not affect the optimization.\nNow we apply the SCE I-divergence formula, but to our new flattened matrices $\\psi$ and $\\phi$ which represent the contrastive learning problem.\nApplying the I-divergence $D_{I}(\\psi||s\\phi)$ and using the specific structure of our target matrix $\\psi$ (which is mostly zeros), the loss function simplifies significantly. The scaling factor $s$ is updated periodically, with its weights $w_{ij}^{u,v}$ also adapted for the contrastive case.\nThe full loss function $\\mathcal{L}{CLR}(\\theta)$ can be expressed as an expectation over the data indices. This form consists of a term for positive pairs ($log~q{ii}^{1,2}$) and a repulsion term involving a sum of similarities over all pairs.\nTo make this computationally feasible, we use a Monte Carlo approximation. This gives us the final SACLR objective, $\\mathcal{L}{SACLR}(\\theta)$, which is calculated over a mini-batch $\\mathcal{B}$ and a set of M negative samples $\\mathcal{M}{i}$. The paper studies two variants: SACLR-1 (M=1) and SACLR-all (M=B).\nThe scaling factor $s$ is also estimated stochastically. Its inverse, $s^{-1}$, is approximated using samples from the mini-batch, and this estimate is updated smoothly using an exponential moving average (EMA) after each batch.\nThe paper also explores a row-wise decomposition of the matrix approximation. Instead of one global scaling factor \u0026rsquo;s\u0026rsquo;, each row \u0026lsquo;a\u0026rsquo; of the similarity matrix gets its own scaling factor $s_a$. This results in the loss function $\\mathcal{L}_{CLR-row}(\\theta)$.\nThis slide provides proof for the simplified row-wise SACLR loss function. By substituting the sparse target matrix $\\psi$ into the general I-divergence formula and summing over all rows, we arrive at the expression shown.\nA key theoretical finding presented in the paper is that under specific conditions, the SACLR-row objective becomes equivalent to the SimCLR loss. This link, however, became a point of discussion during the review. Reviewer gz9D argued that this equivalence doesn\u0026rsquo;t explain why SACLR should be expected to outperform SimCLR. The authors countered that the method\u0026rsquo;s advantage comes not from this specific condition, but from using a different, more flexible weighting scheme for the scaling factor.\nHere is the proof of Theorem 3.1. By substituting the condition for the scaling factors into the loss function, the repulsion term simplifies to a constant, and the remaining terms can be rearranged to form precisely the SimCLR objective.\nJust as with the full matrix version, the row-wise loss can be approximated stochastically for mini-batch training. This gives us the $\\mathcal{L}_{SACLR-row}(\\theta)$ objective.\nSimilarly, the row-wise scaling factors $s_{2(i-1)+u}$ are estimated stochastically within each mini-batch and updated using an EMA rule.\nThis diagram provides a summary of the theoretical framework I have just presented. We started with the concept of I-divergence and a scaling factor, which we used to build a matrix approximation. This was then decomposed row-wise, and both versions were made practical via stochastic approximation. Now, it\u0026rsquo;s time to see the experimental results.\nThis slide presents the full pseudocode for the SACLR algorithm. A major point of contention in the initial review was that the paper claimed to be \u0026ldquo;more computationally efficient\u0026rdquo; without providing empirical data on runtime or memory. The detailed ablation studies on computational cost, shown later in the presentation, were added during the rebuttal period as a direct response to this criticism from the reviewers.\nNow we move to the experiments section. The standard evaluation protocol for self-supervised methods is used: first, a model is pre-trained without labels on a dataset like ImageNet or CIFAR. The learned weights from this backbone are then used to initialize a new network, a linear layer is added, and this new network is trained with labels to perform a classification task.\nTable 1 shows the Top-1 linear classification accuracies on ImageNet. While SACLR-ALL is shown to be competitive with some methods like MoCo v2, this comparison drew significant criticism during the review process. The Area Chair and multiple reviewers stated that the baseline methods used for comparison were \u0026ldquo;weak\u0026rdquo; and the performance gains \u0026ldquo;marginal,\u0026rdquo; noting that many stronger, more recent methods were omitted.\nThis table shows results for longer training, with SACLR-MIX surpassing SimCLR and SogCLR in this setup. However, reviewers found this comparison inadequate as well. Reviewer gz9D pointed out that results for top-performing methods like VICReg and Barlow Twins from other benchmark papers were significantly higher. The authors argued their goal was primarily efficiency without heavy tuning, but this did not overcome the concerns about the weak comparison set.\nThis experiment in Table 3 specifically investigates performance when using only a single negative sample (M=1) per image. SACLR-1 achieves a Top-1 accuracy of 65.3%, significantly outperforming classical contrastive losses like Triplet and Logistic loss under the same constraint.\nThis table evaluates semi-supervised learning performance, showing SACLR\u0026rsquo;s strength in data-limited regimes. It is worth noting that the initial submission focused almost exclusively on linear evaluation. Reviewers Tx4K and gz9D strongly recommended including more comprehensive evaluations like semi-supervised fine-tuning to provide a more complete picture, and the additional kNN and fine-tuning results were added to the paper in response.\nThe learned representations are also evaluated on transfer learning classification tasks. As shown in Table 5, the representations learned by SACLR-ALL and SACLR-MIX transfer very well to other datasets like VOC07 and especially iNaturalist18, where they significantly outperform SimCLR and MoCo v2.\nTable 6 shows transfer learning results on more complex downstream tasks: object detection and segmentation on VOC and COCO. Across all metrics, the SACLR variants are highly competitive and often outperform strong baselines like SimCLR, BYOL, and MoCo v2, with SACLR-MIX showing the strongest results overall.\nThis table provides a direct comparison with other stochastic estimation-based contrastive methods, using an architecture similar to the iSog-CLR paper for a fair comparison. The results on CIFAR10, CIFAR100, and ImageNet100 show that both the matrix and row versions of SACLR are highly competitive, often achieving the best or second-best performance in this specific class of methods.\nThis slide presents several ablation studies on computational complexity and robustness. These experiments were largely added in response to direct reviewer feedback. Reviewers requested empirical data on runtime and memory to substantiate the paper\u0026rsquo;s efficiency claims, as well as a study on the impact of batch size to support the claim that SACLR performs well in small-batch settings. These tables represent the authors\u0026rsquo; attempt to provide that missing evidence.\n","permalink":"https://mookjsi.github.io/posts/paper-review-stochastic/","summary":"This post reviews \u0026lsquo;Stochastic Approximation to Contrastive Learning,\u0026rsquo; a paper submitted to ICLR 2025. While ultimately rejected, the paper proposed an interesting approach to make contrastive learning more efficient. I\u0026rsquo;ll break down its core ideas, the community\u0026rsquo;s feedback, and why it fell short.","title":"Stochastic Approximation to Contrastive Learning"},{"content":"This is a detailed slide-by-slide review of the paper When to Retrieve?, which was submitted to ACL 2024 but was ultimately rejected.\nThe paper questions the efficiency of the standard Retrieval-Augmented Generation (RAG) framework, which naively performs retrieval for every query. The authors propose an adaptive retrieval model (RET indicator) that dynamically decides, for each query, whether external knowledge retrieval is necessary. If the LLM\u0026rsquo;s parametric memory is sufficient, retrieval is skipped; otherwise, external information is fetched.\nThis is the title slide for my presentation on the paper \u0026ldquo;When to Retrieve?\u0026rdquo;, which I delivered at the Yonsei University Machine Learning Lab on February 28, 2025.\nTo set the stage, I\u0026rsquo;m introducing a real-world application I developed, \u0026ldquo;momugo,\u0026rdquo; a restaurant recommendation app. This slide shows the user input screen, where a user is asking for a recommendation for a quiet place to talk with a friend while enjoying soju and hot fish cake soup.\nThis slide demonstrates the detailed output of the \u0026ldquo;momugo\u0026rdquo; app. When a user clicks on a recommended restaurant, it displays more information, including relevant review snippets that match the user\u0026rsquo;s query, providing a justification for the recommendation.\nHere, I\u0026rsquo;m breaking down the initial data filtering process in my application. The system first performs a primary filtering based on structured data like category and business hours. Then, it moves to a more detailed secondary filtering based on the specifics of the user\u0026rsquo;s natural language query.\nThis slide illustrates the core retrieval and generation pipeline. After the initial filtering, the user\u0026rsquo;s query is used to find the top 10 most similar review embeddings from a vector database. The top 3 restaurants are then selected, and an LLM generates a descriptive recommendation reason for each.\nI\u0026rsquo;m detailing the \u0026ldquo;Detailed Filtering\u0026rdquo; step. An LLM is prompted with a system message to analyze the user\u0026rsquo;s query and extract specific requirements, such as \u0026ldquo;corkage available\u0026rdquo; or \u0026ldquo;pet-friendly,\u0026rdquo; into a structured JSON format.\nThis slide provides a high-level overview, separating the entire process into two main phases: Retrieval and Generation. The retrieval part finds the most relevant restaurants, and the generation part creates the user-facing explanation.\nI\u0026rsquo;m explaining the embedding process for the retrieval phase. The descriptions and latest reviews for each restaurant are converted into a 1536-dimensional vector using OpenAI\u0026rsquo;s text-embedding-3-small model and stored in a Pinecone vector database.\nThis slide details the retrieval mechanism. We embed all reviews and the user\u0026rsquo;s query. Using LangChain, we retrieve the top 10 review vectors based on cosine similarity. Finally, we determine the top 3 restaurants by calculating the average vector similarity for each.\nThis slide outlines the generation phase. The top 3 restaurants identified during retrieval, along with their metadata and the original user query, are used to create three separate prompts for the LLM.\nI\u0026rsquo;m showcasing the prompt engineering aspect for the generation phase. A structured prompt, including a system message and a human message with the query and restaurant context, is fed to the LLM to generate a tailored recommendation reason for each of the top 3 restaurants.\nThis slide concludes the overview of my application\u0026rsquo;s architecture. It shows how the generated recommendation reasons (from the LLM) and the filtered restaurant data are combined to produce the final, detailed output card shown to the user.\nShifting from my personal project to the main topic, this slide introduces the standard Retrieval-Augmented Generation (RAG) process, breaking it down into the \u0026lsquo;Retrieval\u0026rsquo; and \u0026lsquo;Generation\u0026rsquo; stages.\nHere, I pose a critical question about the standard RAG framework: Is the naive approach of always retrieving information for every query the most effective or efficient method?\nThis slide frames the core problem investigated in the paper. I\u0026rsquo;m questioning the fundamental assumption of RAG: \u0026ldquo;Should we always retrieve, regardless of the query?\u0026rdquo; This sets the stage for a more nuanced approach.\nI\u0026rsquo;m introducing the central concept of the paper: an adaptive retrieval model. The key idea is to use an indicator, which the authors call \u0026ldquo;RET,\u0026rdquo; to decide whether to retrieve external information or to answer directly from the LLM\u0026rsquo;s parametric memory.\nNow that we\u0026rsquo;ve established the need for a decision mechanism, the key research question becomes: \u0026ldquo;Where to place a RET?\u0026rdquo; In other words, how does the model learn when it\u0026rsquo;s appropriate to trigger the retrieval step?\nThis slide presents the core hypothesis of the paper. The authors believe that retrieval is unnecessary for \u0026ldquo;popular entities\u0026rdquo; (information likely stored in the LLM\u0026rsquo;s parameters) but crucial for \u0026ldquo;non-popular entities.\u0026rdquo;\nBuilding on the hypothesis, I\u0026rsquo;m highlighting the practical challenge: How can a model quantitatively identify if an entity is \u0026ldquo;popular\u0026rdquo;? The paper proposes a model that can learn this distinction to enhance the efficiency and accuracy of the RAG process.\nThis slide formally introduces the paper I\u0026rsquo;m reviewing: \u0026ldquo;When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\u0026rdquo; by Labruna, Campos, and Azkune. They name their proposed model \u0026ldquo;ADAPT-LLM.\u0026rdquo;\nI\u0026rsquo;m providing context with related works. This slide contrasts \u0026ldquo;Closed-book QA,\u0026rdquo; which relies solely on an LLM\u0026rsquo;s internal knowledge, with \u0026ldquo;Open-book QA,\u0026rdquo; which always uses an external information retrieval system.\nThis slide summarizes key issues in the field and their corresponding solutions. It covers the high cost of retraining models, the lexical gap in keyword-based retrieval, and the latency costs associated with retrieval, positioning an adaptive approach as a solution.\nContinuing with the related works, this slide discusses the problem of tool overuse in models like Toolformer and the unrealistic nature of previous popularity-based retrieval methods. It positions ADAPT-LLM as a solution that provides a robust baseline for comparison.\nHere, I\u0026rsquo;m outlining the four-step process of the ADAPT-LLM framework: 1) A query is sent to the model. 2) The model decides whether to retrieve or not. 3) If needed, it retrieves information. 4) It generates the final answer.\nThis slide focuses on the critical decision-making step. The central question is how the model determines whether to retrieve information. The paper\u0026rsquo;s answer is to fine-tune the LLM on a specially curated dataset to learn this behavior.\nI\u0026rsquo;m illustrating the data preparation process for fine-tuning ADAPT-LLM. The process starts with QA data, which is fed to a base LLM. The model\u0026rsquo;s answers are classified as correct or incorrect, which then informs the creation of different types of training prompts.\nThis slide provides a concrete example of the data preparation pipeline. Using a QA pair about the capital of South Korea, I show how the query is extracted from the source data.\nContinuing the example, this slide shows the base LLM generating both a correct answer (\u0026ldquo;Seoul\u0026rdquo;) and an incorrect answer (\u0026ldquo;Busan\u0026rdquo;) to the query, which is a crucial step for creating the fine-tuning dataset.\nThis slide details how the fine-tuning dataset, named DS_adapt, is constructed. Based on the LLM\u0026rsquo;s correctness, three types of prompts are generated: a direct answer for correct parametric knowledge, a \u0026lt;RET\u0026gt; token for incorrect answers, and a context-based prompt for incorrect answers.\nThis slide visualizes the final step of the model creation process. The DS_adapt dataset, with its varied prompt structures, is used to fine-tune a base LLM, resulting in the final ADAPT-LLM.\nI\u0026rsquo;m presenting the headline results from the paper. The table shows that ADAPT-LLM, trained on both NQ and SQuAD datasets, outperforms both the NEVER RETRIEVE (NR-LLM) and ALWAYS RETRIEVE (AR-LLM) baselines in terms of accuracy on the PopQA test set.\nThis slide outlines the structure of the experiments section of my review. I will cover three key areas: comparison to baselines, the model\u0026rsquo;s ability to determine when context is needed, and a comparison with the state-of-the-art approach for the PopQA dataset.\nI\u0026rsquo;m beginning the detailed experimental setup. This slide specifies the training datasets used for the fine-tuning process: Natural Questions (NQ) and Stanford Question Answering Dataset (SQuAD).\nThis slide specifies the base Large Language Model used in the experiments. The authors chose the Llama-2-7B model as the foundation for their fine-tuning.\nHere, I specify the dataset used for evaluating the models at test time. All configurations are evaluated on the PopQA dataset, which is designed to test knowledge of popular entities.\nI\u0026rsquo;m explaining how the baseline models are created for comparison. The NR-LLM (NEVER RETRIEVE) is fine-tuned only on prompts that require direct, parametric answers.\nSimilarly, this slide explains the creation of the AR-LLM (ALWAYS RETRIEVE) baseline. This model is fine-tuned exclusively on prompts that include retrieved context, forcing it to always rely on external information.\nThis slide recaps the training process for the main model, ADAPT-LLM. It is trained on the comprehensive DS_adapt dataset, which includes a mix of parametric, retrieval-triggering, and context-aware prompts.\nI\u0026rsquo;m presenting the main results table again, this time to emphasize the direct performance comparison. ADAPT-LLM consistently achieves the highest accuracy, demonstrating the effectiveness of its selective retrieval strategy.\nTo provide more context on the datasets, this slide presents a table comparing the statistics of NQ, SQuAD, and PopQA, including the number of questions and the average length of questions and answers.\nNow, I\u0026rsquo;m moving to the second part of the experimental analysis: evaluating ADAPT-LLM\u0026rsquo;s ability to correctly decide when to retrieve. This slide sets up the analysis of the model\u0026rsquo;s decision-making accuracy.\nThis slide presents a detailed breakdown of ADAPT-LLM\u0026rsquo;s performance. It analyzes the accuracy of the model in four scenarios: when it correctly decides to retrieve (and is given context), when it incorrectly decides not to retrieve (and isn\u0026rsquo;t given context), and so on.\nI\u0026rsquo;m highlighting a key observation from the results table. The accuracy for questions where the model chose to retrieve (Acc. w/ context for (RET)) seems quite low, around 33%. This prompts a deeper investigation.\nThis slide provides an explanation for the previously noted low accuracy. The authors point out that the performance of the underlying Information Retrieval (IR) system itself was a limiting factor.\nTo support the claim about poor IR performance, this slide presents results from Table 4 of the paper. It shows a massive accuracy gap when using the gold standard passages versus passages retrieved by the Contriever system, confirming the IR bottleneck.\nDespite the issues with the IR component, this slide shows that the model\u0026rsquo;s decision-making process is sound. The histograms show a clear inverse correlation between entity popularity and the usage of the \u0026lt;RET\u0026gt; token, confirming the model learned the core hypothesis.\nI\u0026rsquo;m now moving to the final experimental comparison: ADAPT-LLM versus the previous state-of-the-art method for the PopQA dataset. This slide visually contrasts the two approaches, highlighting differences in training data and thresholding methods.\nThis slide presents the results of the state-of-the-art comparison. While the accuracy is comparable, I\u0026rsquo;m highlighting the authors\u0026rsquo; claims that ADAPT-LLM is a more generalizable and efficient approach due to its lower reliance on the IR system and its independence from dataset-specific features like popularity scores.\nTo begin the conclusion, I\u0026rsquo;m bringing back the diagram illustrating the creation of the DS_adapt fine-tuning dataset. This serves as a reminder of the core technical contribution of the paper.\nThis slide recaps the fine-tuning process itself, showing how the diverse set of prompts from DS_adapt is used to train the base LLM into the final, adaptive ADAPT-LLM.\nIn my final slide, I\u0026rsquo;m summarizing the key takeaways from the paper. The experiments demonstrate that ADAPT-LLM is a robust model that successfully learns when to retrieve information, outperforming standard baselines and showing strong potential as a general, efficient approach to adaptive RAG.\n","permalink":"https://mookjsi.github.io/posts/paper-review-whentoretrieve/","summary":"This post reviews \u0026lsquo;When to Retrieve?\u0026rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community\u0026rsquo;s feedback, and the reasons it was not accepted.","title":"When to Retrieve?"},{"content":"Education Yonsei University, Seoul, Korea Senior @ Dept. of Applied Statistics GPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics) Research Interests Statistical Machine Learning Large Language Model Retrieval Method Research Experience Undergraduate Researcher, ITML @ Yonsei (Jan. 2025 – Jun. 2025) Conducting undergraduate research under the supervision of Prof. Jy-yong Sohn. Engaged in ongoing projects related to RAG. Skills Programming: R, Python, Frontend, Git, MATLAB Languages: Korean (Native), English (Intermediate), Chinese (Beginner) ","permalink":"https://mookjsi.github.io/about/","summary":"\u003ch2 id=\"education\"\u003eEducation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eYonsei University\u003c/strong\u003e, Seoul, Korea\n\u003cul\u003e\n\u003cli\u003eSenior @ Dept. of Applied Statistics\u003c/li\u003e\n\u003cli\u003eGPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-interests\"\u003eResearch Interests\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical Machine Learning\u003c/li\u003e\n\u003cli\u003eLarge Language Model\u003c/li\u003e\n\u003cli\u003eRetrieval Method\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-experience\"\u003eResearch Experience\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUndergraduate Researcher\u003c/strong\u003e, ITML @ Yonsei (Jan. 2025 – Jun. 2025)\n\u003cul\u003e\n\u003cli\u003eConducting undergraduate research under the supervision of Prof. Jy-yong Sohn.\u003c/li\u003e\n\u003cli\u003eEngaged in ongoing projects related to RAG.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming:\u003c/strong\u003e R, Python, Frontend, Git, MATLAB\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLanguages:\u003c/strong\u003e Korean (Native), English (Intermediate), Chinese (Beginner)\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"}]