[{"content":"\nSlide 1: Preface Topic: Diffusion Models (In Process) Sources referenced: Deep Generative Modeling, Jacob M. Tomczak, 2022 Denoising Diffusion Probabilistic Models, Ho et al., 2020 Deep Unsupervised Learning using Nonequilibrium Thermodynamics, Sohl-Dickstein et al., 2015 Slide 2: Introduction - Data Representation An example of how we perceive data versus how a machine might process features. Human view: \u0026ldquo;How cute! This adorable little dog has a white soft coat and big eyes with excitement.\u0026rdquo; Visual Data: An image of a white dog. Slide 3: Computer Representation Computer view: The image is represented as a large tensor of pixel values (e.g., RGB arrays like [200, 189, 165]...). Slide 4: Latent Variable Space (1) Concept: Introduction to Latent Variable Space. We want to map the high-dimensional raw pixel data (Real Data, $x$) to a more meaningful, compressed representation. The descriptive features (\u0026ldquo;How cute\u0026rdquo;, \u0026ldquo;white soft coat\u0026rdquo;) represent the abstract information we want to capture. Slide 5: Latent Variable Space (2) $x$ (Real Data): High-dimensional pixel array. Latent Variable: A compressed vector (e.g., [1.23, -0.85, 0.44, ..., 0.03]) that encodes the semantic features of the data. Slide 6: Latent Variable Space (3) Visualizing the relationship: The raw data array corresponds to the latent vector. Slide 7: Mapping Semantics Latent Variable: Represents features like \u0026ldquo;Dog\u0026rdquo;, \u0026ldquo;Cute\u0026rdquo;, \u0026ldquo;White\u0026rdquo;, \u0026ldquo;soft\u0026rdquo;, \u0026ldquo;big eyes\u0026rdquo;, \u0026ldquo;excitement\u0026rdquo;. These abstract concepts are encoded into the numeric vector $z$. Slide 8: The Generative Goal Goal: To establish a two-way mapping. $x \\to z$: Encoding the image into latent features. $z \\to x$: Generating the image from latent features. Slide 9: The Process Make nice $z$: Generate a meaningful latent vector from semantic descriptions or distributions. Reconstruct image: Use $z$ to create $x$. Slide 10: Requirements for a Good Model Make nice $z$: The latent space should be well-structured (e.g., following a standard normal distribution). Reconstruct image well: The generated image should look like the real data. Slide 11: Example - Hierarchical VAE Hierarchical VAE: An extension of standard VAEs. Uses multiple layers of latent variables ($z_1, z_2, \u0026hellip;$) to capture features at different levels of abstraction. Slide 12: Hierarchical VAE Structure Structure: A (Generative/Top-down): $z_2 \\to z_1 \\to x$. B (Inference/Bottom-up): $x \\to z_1 \\to z_2$. Objective: Forces the model to use $z$ to create an Inductive Bias. Slide 13: The Problem - Posterior Collapse Even with hierarchy, Posterior Collapse remains a problem. Issue: The variational posterior has a bottom-up path, but the generative part has a top-down path. Sometimes the model ignores the latent code $z$ and relies solely on the powerful decoder (autoregressive part), making $z$ meaningless. Slide 14: Analyzing Posterior Collapse ELBO Equation: $$ELBO(x) = E_{q(z_1:z_2|x)}[\\ln p(x|z_1)] - KL[q(z_1|x)||p(z_1|z_2)] - KL[q(z_2|z_1)||p(z_2)]$$ Collapse condition: If $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$, then $q$ doesn\u0026rsquo;t encode any meaningful information about the data. Slide 15: Hierarchical VAE (Top-down Inference) Solution attempt: Make the variational posterior top-down as well. Mechanism: Use residual connections ($r_1, r_2$) and delta parameters ($\\Delta\\mu, \\Delta\\sigma^2$) to guide the generation. Benefit: Forces the model to use $z$. Creates stronger inductive bias. Relieves Posterior Collapse because both Variational Posterior \u0026amp; Generative parts have a top-down path. Note: $q(z_2|z_1) \\approx p(z_2)$ cannot happen easily here. Slide 16: Introduction to Diffusion Based Deep Generative Models Comparing the structures. Diffusion Models: B (Bottom-up): Add Gaussian noise repeatedly. A (Top-down): Denoise \u0026amp; generation. Slide 17: Diffusion Process Visualization Forward (B): Start with image $x$ -\u0026gt; add noise -\u0026gt; $z_1$ -\u0026gt; \u0026hellip; -\u0026gt; $z_5$ (pure noise). Reverse (A): Start with noise $z_5$ -\u0026gt; denoise -\u0026gt; \u0026hellip; -\u0026gt; generate $x$. Slide 18: VAEs vs. DDGMs (Structural Comparison) VAEs: A (Generative): Top-down ($z_2 \\to z_1 \\to x$). B (Inference): Bottom-up ($x \\to z_1 \\to z_2$). Structure has opposite paths. DDGMs (Diffusion): A (Generative): Top-down ($z_2 \\to z_1 \\to x$). B (Inference): Bottom-up ($x \\to z_1 \\to z_2$). Same structure visually, but distinct operational paths. Slide 19: Addressing Posterior Collapse in DDGMs Question: Does Posterior Collapse happen in DDGMs? VAE: Often suffers from $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$. DDGM: The structure is similar (opposite paths), so we need to check. Slide 20: Top-Down Fix in VAEs Recall slide 15: VAEs fixed collapse by making both paths top-down using ResNet-like connections. Does Diffusion do this? Slide 21: Diffusion\u0026rsquo;s Approach Diffusion models have the standard opposite path structure (Forward vs Reverse). Does Posterior Collapse remain? $q(z_2|z_1) \\approx p(z_2) \\sim \\mathcal{N}(0,1)$? Answer: No. Slide 22: Why No Posterior Collapse? In Diffusion, the forward process (Bottom-up B) is fixed to add noise. It forces the data to become Gaussian noise. It is not a learned parameter that can \u0026ldquo;collapse\u0026rdquo; to zero information; it is a fixed physical process. Slide 23: Paper Introduction Paper: Deep Unsupervised Learning using Nonequilibrium Thermodynamics (ICML 2015). Authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli. Slide 24: Motivation - Nonequilibrium Thermodynamics Physical Analogy: A drop of ink diffusing in water. Equation: $\\frac{\\partial p(x,t)}{\\partial t} = D\\nabla^2 p(x,t)$ Forward Process: $dx_t = -\\frac{1}{2}\\beta x_t dt + \\beta dW_t$ Organized structure (Ink drop) $\\to$ Disorganized structure (Uniform distribution). Slide 25: Our Objective Map the diffusion process to image generation. Sequence: $x \\to z_1 \\to z_2 \\to z_3 \\to z_4 \\to z_5$ (Noise). We want to reverse this. Slide 26: Defining the Target Distribution We want to learn the distribution $p_\\theta(x)$ that generates the cat image. Problem: How can we know the distribution of $x$ from $z$? Slide 27: Formulation Strategy We need to write down $p_\\theta(x)$ with respect to latent variables $z$. Slide 28: Formulation (Top-down) - Marginalization Express $p_\\theta(x)$ by marginalizing over all latent steps: $$p_{\\theta}(x) = \\int p_{\\theta}(x, z_{1:T}) dz_{1:T}$$ Slide 29: Joint Distribution Expansion Expand the joint probability using the chain rule (Markov property): $$p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ Slide 30: Detailed Expansion Since $p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_{1:T})p_{\\theta}(z_{1:T})$ By Markov property (dependency only on previous step): $$= p_{\\theta}(x|z_1) p_{\\theta}(z_1|z_2) p_{\\theta}(z_2|z_3) \\cdots p_{\\theta}(z_{T-1}|z_T) p_{\\theta}(z_T)$$ $$= p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ Slide 31: Top-down Path Formula Top-down Integral: $$p_{\\theta}(x) = \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ This represents the path from Noise ($z_T$) back to Image ($x$). Slide 32: Formulation (Bottom-up) Now, how do we make $z$ from $x$ (Forward process)? Sequence: $x \\to z_1 \\to z_2 \\dots \\to z_5$. Slide 33: Forward Posterior Define the approximate posterior $Q$: $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 34: Forward Chain Expansion Breaking down the joint posterior: $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) q_{\\phi}(z_2|z_1, x) \\cdots q_{\\phi}(z_T|z_{1:T-1}, x)$$ Utilizing Markov property: $$= q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 35: Defining the Transition Kernel The forward step is defined as adding Gaussian noise: $$q_{\\phi}(z_i|z_{i-1}) = \\mathcal{N}(z_i | \\sqrt{1-\\beta_i}z_{i-1}, \\beta_i I)$$ Slide 36: Reparameterization Using the reparameterization trick: $$z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$ Slide 37: Learning $\\beta$? Question: Do we have to learn the schedule of $\\beta_i$? Slide 38: Fixed Variance Answer: It could be learned, but fixed $\\beta$ also works well! References: Sohl-Dickstein et al., Ho et al. Slide 39: Overall Formulation Summary Bottom-up (Encoder/Forward): $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ $$z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon$$ Top-down (Decoder/Reverse): $$p_{\\theta}(x) = \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ Slide 40: Where are the Learnable Parameters? Bottom-up: Fixed (Gaussian noise). Top-down: Parameters $\\theta$ exist here! We need to learn the reverse process. Slide 41: Learning Parameters We learn parameters $\\theta$ by maximizing likelihood. Slide 42: Intractability Objective: Maximize $\\ln p_{\\theta}(x)$. $$\\ln p_{\\theta}(x) = \\ln \\int p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T) dz_{1:T}$$ Problem: Can we calculate this integral? No, it\u0026rsquo;s too expensive (intractable). Slide 43: The Trick However, the forward process $Q_{\\phi}(z_{1:T}|x)$ is \u0026ldquo;easy peasy\u0026rdquo; (just adding noise). We can use this to approximate the objective. Slide 44: Importance Sampling Setup Multiply and divide by $Q$: $$= \\ln \\int Q_{\\phi}(z_{1:T}|x) \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} dz_{1:T}$$ Slide 45: Expectation Form Convert integral to expectation: $$= \\ln E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ This is related to Importance Sampling. Slide 46: Jensen\u0026rsquo;s Inequality We use Jensen\u0026rsquo;s Inequality: $f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]$ for convex $f$ (or $\\ge$ for concave like $\\ln$). $$\\ge E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\ln \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ Slide 47: The Lower Bound We now have a lower bound on the log-likelihood: $$\\ln p_{\\theta}(x) \\ge E_{Q_{\\phi}(z_{1:T}|x)} \\left[ \\ln \\frac{p_{\\theta}(x, z_{1:T})}{Q_{\\phi}(z_{1:T}|x)} \\right]$$ Slide 48: Expanding the Terms Substitute the definitions of $p_{\\theta}$ (reverse) and $Q_{\\phi}$ (forward): $$p_{\\theta}(x, z_{1:T}) = p_{\\theta}(x|z_1) \\prod_{i=1}^{T-1} p_{\\theta}(z_i|z_{i+1}) p_{\\theta}(z_T)$$ $$Q_{\\phi}(z_{1:T}|x) = q_{\\phi}(z_1|x) \\prod_{i=2}^{T} q_{\\phi}(z_i|z_{i-1})$$ Slide 49: Log Expansion (1) Expanding the log term inside the expectation: $$E_{Q_{\\phi}} [\\ln p_{\\theta}(x|z_1) + \\sum \\ln p_{\\theta} - \\sum \\ln q_{\\phi} - \\ln q_{\\phi}(z_1|x)]$$ Slide 50: Log Expansion (2) Grouping terms: $$= E_{Q_{\\phi}} [\\ln p_{\\theta}(x|z_1) + \\ln p_{\\theta}(z_1|z_2) + \\sum_{i=2}^{T-1} \\ln p_{\\theta}(z_i|z_{i+1}) + \\ln p_{\\theta}(z_T)$$ $$- \\sum_{i=2}^{T-1} \\ln q_{\\phi}(z_i|z_{i-1}) - \\ln q_{\\phi}(z_T|z_{T-1}) - \\ln q_{\\phi}(z_1|x)]$$ Slide 51: Regrouping for KL Divergence We want to pair matching $p$ and $q$ terms to form KL Divergences. Grouping $(p_{\\theta}(z_i|z_{i+1})$ and $q_{\\phi}(z_i|z_{i-1}))$. Slide 52: KL Divergence Recap Definition: $KL(q||p) = \\mathbb{E}_q [\\ln q - \\ln p]$ We will use this to simplify the expectation terms. Slide 53: Applying KL Definition $E_{Q_{\\phi}} [\\ln p - \\ln q] = - E_{Q_{\\phi}} [\\ln q - \\ln p] = - KL(q||p)$ Slide 54: Identifying Terms (1) Looking at the expanded equation again to match terms. Slide 55: Identifying Terms (2) The sum term becomes a sum of KL divergences: $$\\sum_{i=2}^{T-1} E [\\ln p_{\\theta}(z_i|z_{i+1}) - \\ln q_{\\phi}(z_i|z_{i-1})] = - \\sum_{i=2}^{T-1} KL(q_{\\phi}(z_i|z_{i-1}) || p_{\\theta}(z_i|z_{i+1}))$$ Slide 56: Identifying Terms (3) Placing the summation term back into the equation. Slide 57: Identifying Terms (4) Dealing with the $z_T$ term: $$E [\\ln p_{\\theta}(z_T) - \\ln q_{\\phi}(z_T|z_{T-1})] = - KL(q_{\\phi}(z_T|z_{T-1}) || p_{\\theta}(z_T))$$ Slide 58: Identifying Terms (5) Only the $z_1$ term and reconstruction term remain. Slide 59: Identifying Terms (6) The $z_1$ term: $$E [\\ln p_{\\theta}(z_1|z_2) - \\ln q_{\\phi}(z_1|x)] = - KL(q_{\\phi}(z_1|x) || p_{\\theta}(z_1|z_2))$$ Slide 60: Final Grouping All terms are now converted to Expectations or KL divergences. Slide 61: The ELBO Equation Evidence Lower Bound (ELBO): $$L_{ELBO} = \\mathbb{E}{Q{\\phi}} [\\ln p_{\\theta}(x|z_1)]$$ $$- \\sum_{i=2}^{T-1} KL(q_{\\phi}(z_i|z_{i-1}) || p_{\\theta}(z_i|z_{i+1}))$$ $$- KL(q_{\\phi}(z_T|z_{T-1}) || p_{\\theta}(z_T))$$ $$- KL(q_{\\phi}(z_1|x) || p_{\\theta}(z_1|z_2))$$ Slide 62: Usefulness of ELBO This gives us a computable lower bound on the true log-likelihood. We maximize this ELBO to learn the parameters. Slide 63: The Reconstruction Term Focus on the first term: $\\mathbb{E}{Q{\\phi}} [\\ln p_{\\theta}(x|z_1)]$ What distribution should we use for $p_{\\theta}(x|z_1)$? Slide 64: Gaussian Reconstruction Assume Gaussian distribution: $p(x|z_1) = \\mathcal{N}(x | \\tanh(NN(z_1)), I)$ The log-likelihood of a Gaussian is proportional to the negative Mean Squared Error (MSE). $\\ln p(x|z_1) = -MSE(x, \\tanh(NN(z_1))) + const$ Slide 65: Final ELBO for Implementation Result: $$L_{ELBO} = E[-MSE] - \\sum KL - KL - KL$$ Now we can implement this in code since it consists of MSE and KL divergence terms, which are differentiable. Slide 66: Revisiting the VAE Comparison Why did we go through all this math? To check stability and Posterior Collapse again. Slide 67: Posterior Collapse in DDGMs? VAE: Collapsed if $q(z|x) \\approx \\mathcal{N}(0,1)$. DDGM: The goal is to make $z$ become Gaussian noise! So \u0026ldquo;collapse\u0026rdquo; in the VAE sense is actually the objective of the forward diffusion process. Slide 68: DDPM Paper Paper: Denoising Diffusion Probabilistic Models (NeurIPS 2020). Authors: Jonathan Ho, Ajay Jain, Pieter Abbeel. This paper simplified the training of diffusion models significantly. Slide 69: Original Model Recap Bottom-up (Forward): Gaussian noise steps. Top-down (Reverse): Learned denoising. Slide 70: Efficiency Problem Do we have to sample iteratively $i=1\u0026hellip;T$ just to get the forward noise $z_T$? $z_i = \\sqrt{1-\\beta_i}z_{i-1} + \\sqrt{\\beta_i}\\epsilon$ Slide 71: Cumulation Trick (1) We can skip steps. $z_2 = \\sqrt{\\alpha_2}z_1 + \\sqrt{1-\\alpha_2}\\epsilon_2$ Substitute $z_1$: $z_2 = \\sqrt{\\alpha_2}(\\sqrt{\\alpha_1}x + \\sqrt{1-\\alpha_1}\\epsilon_1) + \\sqrt{1-\\alpha_2}\\epsilon_2$ (Where $\\alpha_i = 1 - \\beta_i$) Slide 72: Cumulation Trick (2) Combining Gaussians: $z_t = \\sqrt{\\alpha_t}z_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_t$ By induction, we can express $z_t$ directly from $x$. Slide 73: Cumulation Trick (3) Result: $$z_t = \\sqrt{\\bar{\\alpha}_t}x + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$$ Where $\\bar{\\alpha}t = \\prod{s=1}^t \\alpha_s$. Mean: $\\sqrt{\\bar{\\alpha}_t}x$ Variance: $(1-\\bar{\\alpha}_t)I$ Slide 74: Closed Form Forward Step We can sample $z_t$ at any timestep $t$ in one go: $$q(z_t|x) = \\mathcal{N}(z_t | \\sqrt{\\bar{\\alpha}_t}x, (1-\\bar{\\alpha}_t)I)$$ Slide 75: New Model Formulation Forward: Directly sample $z_t$ using the cumulation trick. Reverse: Same learned process. Slide 76: DDPM Learning Parameters We revisit the loss function derivation with this new trick in mind. Objective: Maximize $\\ln p_{\\theta}(x)$. Slide 77: Deriving DDPM Loss (1) Using the same variational bound logic: $$= E_Q [\\ln \\frac{p(z_T) \\prod p(z_{t-1}|z_t) p(x|z_1)}{Q(z_T|x) \\prod Q(z_{t-1}|z_t, x)}]$$ Slide 78: Deriving DDPM Loss (2) Grouping terms into KL divergences again. Terminology: $L_T$: KL between $Q(z_T|x)$ and $p(z_T)$ (Prior matching). $L_{t-1}$: KL between $Q(z_{t-1}|z_t, x)$ and $p_{\\theta}(z_{t-1}|z_t)$ (Denoising matching). $L_0$: Reconstruction log likelihood. Slide 79: Loss Terms Definition $L_T = D_{KL}(Q_{\\phi}(z_T|x) || p(z_T))$ $L_{t-1} = D_{KL}(Q_{\\phi}(z_{t-1}|z_t, x) || p_{\\theta}(z_{t-1}|z_t))$ $L_0 = -\\mathbb{E}{Q}[ \\ln p{\\theta}(x|z_1) ]$ Total Loss: Minimize $\\sum L$ terms. Slide 80: The Posterior $q(z_{t-1}|z_t, x_0)$ This term $Q_{\\phi}(z_{t-1}|z_t, x)$ is tractable! It is a Gaussian distribution $\\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)$. Mean $\\tilde{\\mu}_t$: A weighted combination of $x_0$ and $z_t$. Variance $\\tilde{\\beta}_t$: Function of $\\beta$ and $\\bar{\\alpha}$. Slide 81: Posterior Derivation Proof involves Gaussian conditioning formulas. Combining the forward equations allows us to solve for the distribution of $z_{t-1}$ given $z_t$ and $x_0$. Slide 82: Loss Summation We sum the expectations of the loss terms. Slide 83: Expectation Simplification $L_T$ relies on $z_T$. $L_{t-1}$ relies on $z_t, z_{t-1}$. By Markov chain property, we calculate expectations appropriately. Slide 84: Minimization Objective Minimize $\\sum_{t=1}^T \\mathbb{E}_{q}[L_t]$. Slide 85: Randomized Time Sampling Instead of summing all $T$ terms every step, we can sample $t \\sim Uniform(1, \\dots, T)$. Minimize $\\mathbb{E}{t} [\\mathbb{E}{q}[L_t]]$. Slide 86: Gradient Descent We can take the gradient of this expected loss. Slide 87: Efficient Training OOM: Calculating all $T$ steps is memory intensive. Solution: Only calculate one random $t$ per optimization step. Slide 88: Analyzing the Loss Let\u0026rsquo;s look at the full ELBO equation again. Slide 89: Positivity of KL KL divergences are non-negative. Since our models are not perfect approximations, these KL terms will be strictly positive. Slide 90: Accumulation of Error As $T$ grows (many timesteps), there are many KL terms. This makes the ELBO value very small (large negative). Slide 91: Unstable Learning Summing many KL terms can lead to variance issues and unstable learning. Slide 92: Changing the Goal We focus on the specific term: $L_{t-1} = D_{KL}(Q(z_{t-1}|z_t, x) || p_{\\theta}(z_{t-1}|z_t))$. Slide 93: Matching Distributions Our goal is to make the learned reverse process $p_{\\theta}(z_{t-1}|z_t)$ approximate the true posterior $q_{\\phi}(z_{t-1}|z_t, x)$. Slide 94: Gaussian Matching Both distributions are Gaussian. $q \\sim \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)$ $p_{\\theta} \\sim \\mathcal{N}(\\mu_{\\theta}, \\sigma_t^2 I)$ (Usually $\\sigma_t^2$ is fixed to $\\beta_t$ or $\\tilde{\\beta}_t$). Slide 95: KL between Gaussians The KL divergence between two Gaussians with diagonal covariance is proportional to the MSE of their means. Minimize $\\frac{1}{2\\sigma_t^2} || \\tilde{\\mu}t - \\mu{\\theta} ||^2$. Slide 96: Parametrization of $\\mu_{\\theta}$ We know $\\tilde{\\mu}_t(z_t, x)$ can be written in terms of $z_t$ and the noise $\\epsilon$ used to generate it. $\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon)$ So, we should parameterize $\\mu_{\\theta}$ to predict $\\epsilon$! $\\mu_{\\theta} = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}t}}\\epsilon{\\theta}(z_t, t))$ Slide 97: Derivation Step 1 Recall the definition of $\\tilde{\\mu}_t$ (weighted average of $x_0$ and $z_t$). Slide 98: Derivation Step 2 Substitute $x_0$ with its expression derived from $z_t$ and $\\epsilon$. $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(z_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)$ Slide 99: Derivation Step 3 Simplify the algebra to get the expression for $\\tilde{\\mu}_t$ solely in terms of $z_t$ and $\\epsilon$. Slide 100: Matching Means Since $\\tilde{\\mu}t$ depends on $\\epsilon$, our model $\\mu{\\theta}$ should approximate it by using a neural network $\\epsilon_{\\theta}(z_t, t)$ to predict that noise. Slide 101: Comparison Comparison of the target mean $\\tilde{\\mu}t$ and the model mean $\\mu{\\theta}$. They are identical in form, except one uses true noise $\\epsilon$ and the other uses predicted noise $\\epsilon_{\\theta}$. Slide 102: Difference of Means Calculating $\\tilde{\\mu}t - \\mu{\\theta}$. Everything cancels out except the noise terms. Difference $\\propto (\\epsilon - \\epsilon_{\\theta})$. Slide 103: Simplified Loss Function The loss becomes weighted MSE between true noise and predicted noise. Minimize $\\lambda_t || \\epsilon - \\epsilon_{\\theta}(z_t, t) ||^2$. $\\lambda_t$ is a weighting term derived from the variances. Slide 104: Final Simplification Ho et al. (DDPM) found that ignoring the weighting term $\\lambda_t$ (setting it to 1) works better in practice. It puts more weight on difficult aspects of the denoising task. Slide 105: The \u0026ldquo;Simple\u0026rdquo; Loss $L_{simple} = \\mathbb{E}{t, x_0, \\epsilon} [ || \\epsilon - \\epsilon{\\theta}(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t) ||^2 ]$ We simply train a network to predict the noise added to the image. Slide 106: Algorithms Algorithm 1 (Training): Sample $x_0$. Sample $t$. Sample noise $\\epsilon$. Gradient descent on $|| \\epsilon - \\epsilon_{\\theta}(\\text{noisy input}, t) ||^2$. Algorithm 2 (Sampling): Start from $x_T \\sim \\mathcal{N}(0, I)$. Iteratively denoise: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\dots \\epsilon_{\\theta}(x_t, t)) + \\sigma_t z$. Slide 107: Comparison of Training Methods (A) Full ELBO (Sohl-Dickstein): Requires $T$-step gradients, large memory, slow convergence. (B) Simplified Noise-Prediction (Ho et al.): One-step gradient (random $t$), no KL accumulation, efficient, stable training. Slide 108: Conclusion Thank You. ","permalink":"https://mookjsi.github.io/posts/diffusion/","summary":"A comprehensive breakdown of Diffusion Models, starting from the limitations of Hierarchical VAEs, covering the thermodynamics inspiration, and detailing the full mathematical derivation of the ELBO and the simplified noise-prediction loss used in DDPM. This post is based on a presentation I gave at a YBIGTA (Big data society in Yonsei) session.","title":"Diffusion Models: From VAEs to DDPM Derivation"},{"content":"ê°œìš” ì—„ì²­ë‚œ ë…ì„œê´‘ì´ë¼ ì„¸ìƒì˜ ëª¨ë“  ì±…ì„ ì½ì€ ì¹œêµ¬(Large Language Model)ê°€ ìˆë‹¤ê³  ìƒìƒí•´ ë³´ì„¸ìš”. ì´ ì¹œêµ¬ëŠ” ê¸€ì€ ê¸°ê°€ ë§‰íˆê²Œ ì˜ ì“°ì§€ë§Œ, íƒœì–´ë‚˜ì„œ í•œ ë²ˆë„ ê·¸ë¦¼ì´ë‚˜ ì‚¬ì§„ì„ ë³¸ ì ì´ ì—†ìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, ê·¸ë¦¼ë§Œ í•˜ë£¨ ì¢…ì¼ ë´ì„œ ì‹œê°ì ì¸ ì •ë³´ëŠ” ì™„ë²½í•˜ê²Œ ì•Œì§€ë§Œ ë§ì£¼ë³€ì´ ì—†ëŠ” ì¹œêµ¬(Vision Encoder)ë„ ìˆìŠµë‹ˆë‹¤.\nFlamingoëŠ” ì´ ë‘ ì¹œêµ¬ë¥¼ ì´ì–´ì£¼ëŠ” \u0026lsquo;ë§ˆë²•ì˜ í†µì—­ì‚¬\u0026rsquo; ê°™ì€ ëª¨ë¸ì…ë‹ˆë‹¤.\nê·¸ë¦¼ ì˜ ë³´ëŠ” ì¹œêµ¬ê°€ ì‚¬ì§„ì„ ë³´ê³  \u0026ldquo;ì´ê±´ í„¸ì´ ë³µìŠ¬ë³µìŠ¬í•œ ê³ ì–‘ì´ì•¼\u0026quot;ë¼ê³  ì •ë³´ë¥¼ ì••ì¶•í•´ì„œ ë„˜ê²¨ì£¼ë©´, Flamingoì˜ íŠ¹ë³„í•œ ì¥ì¹˜(Perceiver Resampler ë“±)ê°€ ì´ê²ƒì„ ê¸€ ì˜ ì“°ëŠ” ì¹œêµ¬ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ë¡œ ë²ˆì—­í•´ ì¤ë‹ˆë‹¤.\nê°€ì¥ ëŒ€ë‹¨í•œ ì ì€ **\u0026lsquo;ëˆˆì¹˜(Few-Shot Learning)\u0026rsquo;**ê°€ ì—„ì²­ë‚˜ê²Œ ë¹ ë¥´ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆì „ì—ëŠ” ì¸ê³µì§€ëŠ¥ì—ê²Œ \u0026ldquo;ì´ê±´ ê³ ì–‘ì´ê³ , ì´ê±´ ê°•ì•„ì§€ì•¼\u0026quot;ë¼ê³  ìˆ˜ë§Œ ë²ˆ ê°€ë¥´ì³ì•¼(Fine-tuning) ê²¨ìš° ì•Œì•„ë“¤ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ FlamingoëŠ” ë§ˆì¹˜ ë˜‘ë˜‘í•œ ê³ ë“±í•™ìƒì²˜ëŸ¼, ì‹œí—˜ ë¬¸ì œì§‘ì—ì„œ ì˜ˆì‹œ ë¬¸ì œ í•œë‘ ê°œë§Œ ì“± ë³´ì—¬ì£¼ë©´(Few-Shot), ë°”ë¡œ ì›ë¦¬ë¥¼ ê¹¨ë‹«ê³  ì²˜ìŒ ë³´ëŠ” ë¬¸ì œë„ ì²™ì²™ í’€ì–´ëƒ…ë‹ˆë‹¤. ì‚¬ì§„ê³¼ ê¸€ì´ ì„ì—¬ ìˆëŠ”(Interleaved) ë³µì¡í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ê°™ì€ ê²ƒë„ ìˆœì„œëŒ€ë¡œ ì´í•´í•˜ê³  ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì·„ìŠµë‹ˆë‹¤.\n1. ì„œë¡  (Introduction) ì´ ë…¼ë¬¸ì€ ë¹„ì „(Vision)ê³¼ ì–¸ì–´(Language)ë¥¼ ê²°í•©í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì¸ Flamingoë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ìˆ˜ì²œ ê°œì˜ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ FlamingoëŠ” ì‚¬ì „ í•™ìŠµëœ ê°•ë ¥í•œ Vision Encoderì™€ Large Language Model(LLM)ì„ ê²°í•©í•˜ì—¬, ë‹¨ ëª‡ ê°œì˜ ì˜ˆì‹œ(Few-Shot)ë§Œìœ¼ë¡œë„ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì—ì„œ ìµœê³  ì„±ëŠ¥(SOTA)ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.\nì´ ê·¸ë¦¼ì€ Flamingoê°€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì„ì¸ ì…ë ¥ì„ ë°›ì•„, ë™ë¬¼ ë¶„ë¥˜, ë„ì‹œ ì´ë¦„ ë§ì¶”ê¸°, ìˆ˜í•™ ë¬¸ì œ í’€ì´ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ ëª‡ ê°œì˜ ì˜ˆì‹œë§Œ ì£¼ì–´ì§€ë©´(Few-shot prompting) ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì™¼ìª½ì€ Flamingoê°€ ë³„ë„ì˜ Fine-tuning ì—†ì´ë„, ìˆ˜ì²œ ë°°ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œ ê¸°ì¡´ ëª¨ë¸ë“¤(SOTA)ë³´ë‹¤ 16ê°œ ì¤‘ 6ê°œ íƒœìŠ¤í¬ì—ì„œ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ì€ ëª¨ë¸ì˜ í¬ê¸°(Parameters)ì™€ ìƒ·(Shot) ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n2. ê¸°ìˆ ì  ì„¤ëª… (Technical Description) Flamingoì˜ í•µì‹¬ì€ ì´ë¯¸ ì˜ í•™ìŠµëœ Vision Encoderì™€ LLMì„ \u0026lsquo;ì–¼ë¦¬ì§€(Frozen)\u0026rsquo; ì•Šì€ ìƒíƒœë¡œ ë‘ê³ , ë‘˜ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ëŠ” ìƒˆë¡œìš´ ë ˆì´ì–´ë§Œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ê°€ì§„ ë°©ëŒ€í•œ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ì§€ ì•Šìœ¼ë©´ì„œ(Catastrophic Forgetting ë°©ì§€) ì‹œê° ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤.\n2.1 ì•„í‚¤í…ì²˜ (Architecture) ì…ë ¥ëœ ì‹œê° ë°ì´í„°(ì´ë¯¸ì§€/ë¹„ë””ì˜¤)ëŠ” Vision Encoderë¥¼ ê±°ì³ Perceiver Resamplerë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì—¬ê¸°ì„œ ê³ ì •ëœ ê°œìˆ˜ì˜ Visual Tokenìœ¼ë¡œ ë³€í™˜ëœ í›„, LLM ì¸µ ì‚¬ì´ì— ì‚½ì…ëœ GATED XATTN-DENSE ë ˆì´ì–´ë¥¼ í†µí•´ ì–¸ì–´ ëª¨ë¸ì— ì‹œê° ì •ë³´ê°€ ì£¼ì…ë©ë‹ˆë‹¤. íŒŒë€ìƒ‰ ë¶€ë¶„ì€ í•™ìŠµë˜ì§€ ì•ŠëŠ” Frozen ìƒíƒœ, ë³´ë¼ìƒ‰ ë¶€ë¶„ë§Œ í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\n2.2 Visual Processing \u0026amp; Perceiver Resampler Vision Encoder(NFNet)ëŠ” ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ì˜ í•´ìƒë„ë‚˜ ê¸¸ì´ì— ë”°ë¼ íŠ¹ì§•ë§µì˜ í¬ê¸°ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Perceiver Resamplerë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.\nì´ ëª¨ë“ˆì€ ê°€ë³€ì ì¸ í¬ê¸°ì˜ ì‹œê°ì  íŠ¹ì§•(Visual features)ì„ ì…ë ¥ë°›ì•„, ê³ ì •ëœ ê°œìˆ˜(ì˜ˆ: 64ê°œ)ì˜ Visual Tokenìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” Transformer ê¸°ë°˜ êµ¬ì¡°ë¡œ, í•™ìŠµ ê°€ëŠ¥í•œ Latent Queryë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê° ì •ë³´ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê³„ì‚° ë³µì¡ë„ë¥¼ ì¤„ì´ê³  LLMì´ ì‹œê° ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê²Œ í•©ë‹ˆë‹¤.\n2.3 GATED XATTN-DENSE \u0026amp; Conditioning ì¶”ì¶œëœ ì‹œê° ì •ë³´ëŠ” LLMì´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ë•Œ í™œìš©ë©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê¸°ì¡´ LLM ë ˆì´ì–´ ì‚¬ì´ì— ìƒˆë¡œìš´ Cross-Attention ë ˆì´ì–´ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.\nê¸°ì¡´ LLM ë¸”ë¡ ì•ì— Cross-Attention(ì‹œê° ì •ë³´ ì°¸ì¡°)ê³¼ Feed-Forward ë ˆì´ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ì ì€ Tanh Gating ê¸°ë²•ì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ˆê¸°ì— Tanh ê²Œì´íŠ¸ ê°’ì€ 0ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµ ì´ˆê¸°ì—ëŠ” ì‹œê° ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  ì˜¤ë¡œì§€ ì–¸ì–´ ëª¨ë¸ì˜ ì›ë˜ ì„±ëŠ¥ ê·¸ëŒ€ë¡œ ì‘ë™í•˜ê²Œ í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤. í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ê²Œì´íŠ¸ ê°’ì´ ì»¤ì§€ë©° ì‹œê° ì •ë³´ë¥¼ ë°›ì•„ë“¤ì…ë‹ˆë‹¤.\nìˆ˜ì‹ ì„¤ëª… (Likelihood Modeling):\nì´ ëª¨ë¸ì€ ì¸í„°ë¦¬ë¸Œ ëœ(ì„ì¸) ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í…ìŠ¤íŠ¸ $y$ì˜ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n$$ p(y|x) = \\prod_{l=1}^{L} p(y_l \\mid y_{\u0026lt;l}, x_{\\leq l}) $$\nì´ ìˆ˜ì‹ì€ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì—°ì‡„ ë²•ì¹™(Chain Rule)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n$y_l$: í˜„ì¬ ì˜ˆì¸¡í•˜ë ¤ëŠ” $l$ë²ˆì§¸ ì–¸ì–´ í† í°ì…ë‹ˆë‹¤. $y_{\u0026lt;l}$: ì´ì „ì— ìƒì„±ëœ ëª¨ë“  í…ìŠ¤íŠ¸ í† í°ë“¤ì…ë‹ˆë‹¤. $x_{\\le l}$: í˜„ì¬ í…ìŠ¤íŠ¸ í† í° $y_l$ë³´ë‹¤ ì‹œí€€ìŠ¤ìƒ ì´ì „ì— ë“±ì¥í•œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ë“¤ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ì˜ë¯¸: FlamingoëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ, í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥($y_{\u0026lt;l}$)ë¿ë§Œ ì•„ë‹ˆë¼, í˜„ì¬ ì‹œì  ì´ì „ì— ë‚˜ì™”ë˜ ëª¨ë“  ì‹œê° ì •ë³´($x_{\\le l}$)ë¥¼ ì¡°ê±´(Condition)ìœ¼ë¡œ í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ëŠ” ìê¸°íšŒê·€(Autoregressive) ìƒì„± ë°©ì‹ì…ë‹ˆë‹¤.\n2.4 Multi-visual Input \u0026amp; Data FlamingoëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì„ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ íŠ¹ë³„í•œ ë§ˆìŠ¤í‚¹ ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nëª¨ë¸ì€ í…ìŠ¤íŠ¸ í† í°ì„ ì˜ˆì¸¡í•  ë•Œ, í•´ë‹¹ í…ìŠ¤íŠ¸ ì§ì „ì— ë‚˜ì˜¨ ì´ë¯¸ì§€ì—ë§Œ Cross-Attentionì„ ìˆ˜í–‰í•˜ë„ë¡ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬(ì§„í•œ íŒŒë€ìƒ‰) ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ Self-Attention(í…ìŠ¤íŠ¸ë¼ë¦¬ì˜ ê´€ê³„)ì„ í†µí•´ ì „ì²´ ë¬¸ë§¥ì€ ìœ ì§€ë©ë‹ˆë‹¤.\nì§€ì› ì˜ˆì œ(Support examples) ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ì—´í•˜ê³ , ë§ˆì§€ë§‰ì— ì¿¼ë¦¬(Query) ì´ë¯¸ì§€ë¥¼ ë¶™ì—¬ ëª¨ë¸ì´ ë‹µì„ ìƒì„±í•˜ê²Œ ìœ ë„í•˜ëŠ” êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\nM3W: ì›¹í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ê°€ ì„ì¸ ë°ì´í„° (Few-shot ëŠ¥ë ¥ì˜ í•µì‹¬). Image-Text Pairs (ALIGN, LTIP): ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ìŒ. Video-Text Pairs (VTP): ë¹„ë””ì˜¤ì™€ ìº¡ì…˜ ìŒ. 3. ì‹¤í—˜ (Experiments) ì‹¤í—˜ì€ Flamingoê°€ ì–¼ë§ˆë‚˜ ì ì€ ë°ì´í„°ë¡œ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ì ì‘í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  SOTA ëª¨ë¸ë“¤ê³¼ ë¹„êµí•´ ì–´ë–¤ ì„±ëŠ¥ì„ ë‚´ëŠ”ì§€ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.\n3.1 Few-Shot Learning ì„±ëŠ¥ FlamingoëŠ” ë‹¨ 4ê°œì˜ Shotë§Œìœ¼ë¡œë„ ì´ì „ì˜ Zero/Few-shot SOTAë¥¼ ì••ë„í•©ë‹ˆë‹¤. íŠ¹íˆ 6ê°œì˜ íƒœìŠ¤í¬ì—ì„œëŠ” ìˆ˜ì²œ~ìˆ˜ì‹­ë§Œ ê°œì˜ ë°ì´í„°ë¡œ Fine-tuning ëœ ëª¨ë¸ë“¤ë³´ë‹¤ 32-shot Flamingoê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ í˜ì‹ ì ì¸ ê²°ê³¼ì…ë‹ˆë‹¤.\n3.2 Fine-tuning \u0026amp; Classification ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ì£¼ê³  í•™ìŠµì‹œí‚¤ë©´ VQAv2, VATEX ë“± 5ê°œ íƒœìŠ¤í¬ì—ì„œ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nContrastive ëª¨ë¸ë³´ë‹¤ëŠ” ë‹¤ì†Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ì§€ë§Œ, RICES(ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ê²€ìƒ‰í•´ì„œ í”„ë¡¬í”„íŠ¸ì— ë„£ëŠ” ë°©ë²•)ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n3.3 Ablation Studies (ì†Œê±° ì—°êµ¬) M3W(ì¸í„°ë¦¬ë¸Œ ë°ì´í„°)ê°€ ì—†ìœ¼ë©´ ì„±ëŠ¥ì´ 17% í•˜ë½í•©ë‹ˆë‹¤. Tanh gatingì´ ì—†ìœ¼ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³  ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. Perceiver Resamplerê°€ MLPë‚˜ ë°”ë‹ë¼ Transformerë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤. Resamplerì˜ í¬ê¸°, Multi-image attention ë°©ì‹, LM ì‚¬ì „ í•™ìŠµ ë°ì´í„°(C4 vs MassiveText) ë“±ì˜ ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.\në°ì´í„°ì˜ ì–‘ë³´ë‹¤ ì§ˆ(LTIP ë°ì´í„°ì…‹)ì´ ì¤‘ìš”í•˜ë©°, ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•©ì¹  ë•Œ ë‹¨ìˆœ ë³‘í•©ë³´ë‹¤ Gradient Accumulation ë°©ì‹ì´ ë” íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nì„±ë³„ì´ë‚˜ í”¼ë¶€ìƒ‰ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ(p-value \u0026gt; 0.05)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n4. ê²°ë¡  (Conclusion) FlamingoëŠ” ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì‹œê° ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬, ìµœì†Œí•œì˜ í•™ìŠµ ë°ì´í„°(Few-shot)ë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë²”ìš© ëª¨ë¸ì…ë‹ˆë‹¤.\ní•µì‹¬ ê¸°ì—¬: Perceiver Resamplerì™€ GATED XATTN-DENSEë¥¼ ë„ì…í•˜ì—¬ ì‹œê°-ì–¸ì–´ ê°„ì˜ ê°„ê·¹ì„ ì¤„ì˜€ê³ , M3W ë°ì´í„°ì…‹ì„ í†µí•´ ì¸í„°ë¦¬ë¸Œ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤. ì˜ì˜: ë³„ë„ì˜ íŒŒì¸ íŠœë‹ ì—†ì´ë„ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ ì‹œê°ì  ì§ˆë¬¸ ì‘ë‹µ, ìº¡ì…˜ ìƒì„±, ëŒ€í™” ë“±ì´ ê°€ëŠ¥í•¨ì„ ì…ì¦í•˜ì—¬, ë²”ìš© ì¸ê³µì§€ëŠ¥(AGI)ìœ¼ë¡œ ê°€ëŠ” ì¤‘ìš”í•œ ë°œíŒì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤. ","permalink":"https://mookjsi.github.io/posts/paper-review-flamingo/","summary":"DeepMindì˜ Flamingo ë…¼ë¬¸ ë¦¬ë·°.","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"content":"\n**\u0026lsquo;ìœ„ì¡°ì§€íë²”(G)\u0026rsquo;**ê³¼ **\u0026lsquo;ê²½ì°°(D)\u0026rsquo;**ì´ ìˆë‹¤ê³  ìƒìƒí•´ ë³´ì„¸ìš”.\nìœ„ì¡°ì§€íë²” (Generator, G): ì´ ì¹œêµ¬ì˜ ëª©í‘œëŠ” ì§„ì§œ ê°™ì€ ìœ„ì¡°ì§€íë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì•„ì£¼ ì„œíˆ´ëŸ¬ì„œ ì¢…ì´ì— ê·¸ë¦¼ì„ ê·¸ë¦° ìˆ˜ì¤€ì˜ ê°€ì§œ ëˆì„ ë§Œë“­ë‹ˆë‹¤. ê²½ì°° (Discriminator, D): ì´ ê²½ì°°ì˜ ì„ë¬´ëŠ” ì§„ì§œ ëˆê³¼ ìœ„ì¡°ì§€íë²”ì´ ë§Œë“  ê°€ì§œ ëˆì„ êµ¬ë³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ìœ„ì¡°ì§€íê°€ ë„ˆë¬´ ê°€ì§œ ê°™ì•„ì„œ êµ¬ë³„í•˜ê¸° ì•„ì£¼ ì‰½ìŠµë‹ˆë‹¤. ì´ì œ ì´ ë‘˜ì´ ì„œë¡œ **ê²½ìŸ(adversarial)**ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n**ê²½ì°°(D)**ì€ ì§„ì§œ ëˆ ìˆ˜ë°±ë§Œ ì¥ê³¼ ìœ„ì¡°ì§€íë²”(G)ì´ ë§Œë“  ê°€ì§œ ëˆ ìˆ˜ë°±ë§Œ ì¥ì„ ë³´ë©´ì„œ ì§„ì§œì™€ ê°€ì§œë¥¼ êµ¬ë³„í•˜ëŠ” ëŠ¥ë ¥ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì ì  ë˜‘ë˜‘í•´ì ¸ì„œ ì›¬ë§Œí•œ ê°€ì§œëŠ” ë‹¤ ì¡ì•„ëƒ…ë‹ˆë‹¤. **ìœ„ì¡°ì§€íë²”(G)**ì€ ìê¸°ê°€ ë§Œë“  ëˆì´ ê²½ì°°(D)ì—ê²Œ ê³„ì† ê±¸ë¦¬ì, ì–´ë–»ê²Œ í•˜ë©´ ê²½ì°°ì„ ì†ì¼ ìˆ˜ ìˆì„ì§€ ì—°êµ¬í•©ë‹ˆë‹¤. \u0026ldquo;ì•„, í™€ë¡œê·¸ë¨ì´ ì—†ì–´ì„œ ê±¸ë ¸êµ¬ë‚˜\u0026rdquo;, \u0026ldquo;ì¢…ì´ ì§ˆê°ì´ ë‹¬ë¼ì„œ ê±¸ë ¸êµ¬ë‚˜\u0026rdquo; ë“±ë“± ê²½ì°°(D)ì´ ë¬´ì—‡ì„ ë³´ê³  ê°€ì§œë¡œ íŒë‹¨í•˜ëŠ”ì§€, ê·¸ í”¼ë“œë°±ì„ ë°›ì•„ì„œ(ì´ê²ƒì´ backpropagationì…ë‹ˆë‹¤) ì ì  ë” ì •êµí•œ ìœ„ì¡°ì§€íë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ì œ ìœ„ì¡°ì§€íë²”(G)ì´ ë§Œë“  ê°€ì§œ ëˆì´ ë„ˆë¬´ ì •êµí•´ì ¸ì„œ ê²½ì°°(D)ì´ í—·ê°ˆë¦¬ê¸° ì‹œì‘í•©ë‹ˆë‹¤. ê·¸ëŸ¼ **ê²½ì°°(D)**ì€ ë” ì—´ì‹¬íˆ í›ˆë ¨í•´ì„œ, ì•„ì£¼ ë¯¸ì„¸í•œ ì°¨ì´ì (ì˜ˆ: ì‰í¬ ëƒ„ìƒˆ)ê¹Œì§€ êµ¬ë³„í•´ë‚´ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê²½ìŸì€ ê³„ì†ë©ë‹ˆë‹¤. ì´ ê²Œì„ì˜ ìµœì¢… ëª©í‘œëŠ” ìœ„ì¡°ì§€íë²”(G)ì´ ë„ˆë¬´ ì™„ë²½í•œ ìœ„ì¡°ì§€íë¥¼ ë§Œë“¤ì–´ì„œ, ê²½ì°°(D)ì´ ì´ê²Œ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ êµ¬ë³„í•  í™•ë¥ ì´ **ì •í™•íˆ 50% (ì¦‰, ì°ëŠ” ìˆ˜ì¤€)**ê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ìˆœê°„, ìœ„ì¡°ì§€íë²”(G)ì€ \u0026lsquo;ì§„ì§œ ëˆì˜ ë¶„í¬(distribution)\u0026lsquo;ë¥¼ ì™„ë²½í•˜ê²Œ í•™ìŠµí–ˆë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì€ ì´ ì•„ì´ë””ì–´ë¥¼ ì»´í“¨í„° ëª¨ë¸, íŠ¹íˆ multilayer perceptrons (ì‹ ê²½ë§)ì— ì ìš©í•œ ê²ƒì…ë‹ˆë‹¤. $G$ëŠ” ê°€ì§œ ì´ë¯¸ì§€(ë°ì´í„°)ë¥¼ ë§Œë“¤ê³ , $D$ëŠ” ì§„ì§œ ì´ë¯¸ì§€(ë°ì´í„°)ì™€ $G$ê°€ ë§Œë“  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë³„í•˜ë„ë¡ í›ˆë ¨í•©ë‹ˆë‹¤.\nI. ì„œë¡  (Introduction) ì´ ë…¼ë¬¸ì€ generative models (ìƒì„± ëª¨ë¸)ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ì¸ **Generative Adversarial Nets (GAN)**ì„ ì œì•ˆí•©ë‹ˆë‹¤.\nê¸°ì¡´ì˜ deep generative models (ì‹¬ì¸µ ìƒì„± ëª¨ë¸)ë“¤ì€ maximum likelihood estimation (ìµœëŒ€ìš°ë„ì¶”ì •) ê°™ì€ ì „ëµì„ ì‚¬ìš©í–ˆëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ë§¤ìš° ë³µì¡í•˜ê³  ë‹¤ë£¨ê¸° í˜ë“  í™•ë¥ ì  ê³„ì‚° (intractable probabilistic computations)ì´ ë§ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ì´ ë•Œë¬¸ì— Markov chains (MCMC) ê°™ì€ ê·¼ì‚¬ì ì¸ ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼ í–ˆê³ , ì´ëŠ” í•™ìŠµì„ ì–´ë µê³  ëŠë¦¬ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì€ ì´ëŸ° ì–´ë ¤ì›€ì„ í”¼í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ë°”ë¡œ **adversarial process (ì ëŒ€ì  ê³¼ì •)**ì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‘ ê°œì˜ ëª¨ë¸, ì¦‰ ë°ì´í„° ë¶„í¬ë¥¼ í¬ì°©í•˜ëŠ” **generative model ($G$)**ê³¼ ìƒ˜í”Œì´ $G$ê°€ ì•„ë‹Œ ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ ì™”ì„ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” **discriminative model ($D$)**ì„ ë™ì‹œì— í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n$G$ì˜ í•™ìŠµ ì ˆì°¨ëŠ” $D$ê°€ ì‹¤ìˆ˜í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” minimax two-player gameì— í•´ë‹¹í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ $G$ì™€ $D$ê°€ ì¶©ë¶„í•œ ìš©ëŸ‰(capacity)ì„ ê°€ì§ˆ ë•Œ, $G$ê°€ í•™ìŠµ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì™„ë²½íˆ ë³µêµ¬í•˜ê³  $D$ëŠ” ëª¨ë“  ê³³ì—ì„œ $\\frac{1}{2}$ë¥¼ ì¶œë ¥í•˜ëŠ” ìœ ì¼í•œ í•´ê°€ ì¡´ì¬í•¨ì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\nê°€ì¥ í° ì¥ì ì€ $G$ì™€ $D$ê°€ multilayer perceptrons (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )ìœ¼ë¡œ ì •ì˜ë  ë•Œ, ì „ì²´ ì‹œìŠ¤í…œì´ **backpropagation (ì—­ì „íŒŒ)**ë§Œìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. í•™ìŠµì´ë‚˜ ìƒ˜í”Œ ìƒì„± ê³¼ì •ì—ì„œ Markov chainsë‚˜ approximate inference (ê·¼ì‚¬ ì¶”ë¡ ) ë„¤íŠ¸ì›Œí¬ê°€ ì „í˜€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\nII. ê¸°ìˆ  ì„¤ëª… (Technology Description) Adversarial nets í”„ë ˆì„ì›Œí¬ëŠ” ë‘ ëª¨ë¸ì´ ëª¨ë‘ multilayer perceptronsì¼ ë•Œ ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ì ìš©ë©ë‹ˆë‹¤.\nGenerator ($G$): $G$ëŠ” ì…ë ¥ ë…¸ì´ì¦ˆ ë³€ìˆ˜ì— ëŒ€í•œ prior $p_z(z)$ (ì˜ˆ: ëœë¤ ë…¸ì´ì¦ˆ)ë¥¼ ì…ë ¥ë°›ì•„, ë°ì´í„° ê³µê°„ìœ¼ë¡œì˜ ë§¤í•‘ $G(z; \\theta_g)$ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. $G$ëŠ” íŒŒë¼ë¯¸í„° $\\theta_g$ë¥¼ ê°–ëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜(ì‹ ê²½ë§)ì…ë‹ˆë‹¤. Discriminator ($D$): $D$ëŠ” ë°ì´í„° $x$ë¥¼ ì…ë ¥ë°›ì•„ $D(x; \\theta_d)$ë¼ëŠ” ë‹¨ì¼ ìŠ¤ì¹¼ë¼ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ ê°’ì€ $x$ê°€ $G$ê°€ ìƒì„±í•œ $p_g$ê°€ ì•„ë‹Œ, ì‹¤ì œ ë°ì´í„° $p_{data}$ë¡œë¶€í„° ì™”ì„ í™•ë¥ ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. $D$ëŠ” íŒŒë¼ë¯¸í„° $\\theta_d$ë¥¼ ê°–ëŠ” ì‹ ê²½ë§ì…ë‹ˆë‹¤. $D$ëŠ” ì‹¤ì œ ë°ì´í„°($x$)ì™€ $G$ê°€ ë§Œë“  ê°€ì§œ ë°ì´í„°($G(z)$) ëª¨ë‘ì— ëŒ€í•´ ì •ë‹µ ë ˆì´ë¸”ì„ ë§í í™•ë¥ ì„ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. ë™ì‹œì— $G$ëŠ” $\\log(1 - D(G(z)))$ë¥¼ ìµœì†Œí™”í•˜ë„ë¡, ì¦‰ $D$ê°€ ì†ì•„ì„œ $D(G(z))$ë¥¼ 1ì— ê°€ê¹ê²Œ ë§Œë“¤ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.\nì´ê²ƒì´ ë°”ë¡œ minimax gameì˜ í•µì‹¬ì…ë‹ˆë‹¤.\nEquation (1): ì´ ê²Œì„ì˜ ê°€ì¹˜ í•¨ìˆ˜(Value function) $V(G, D)$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. $$ \\min_{G} \\max_{D} V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] $$ $\\max_{D} V(D, G)$: $D$ì˜ ê´€ì ì…ë‹ˆë‹¤. $D$ëŠ” $V$ë¥¼ ìµœëŒ€í™”í•˜ë ¤ í•©ë‹ˆë‹¤. $\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)]$ : ì‹¤ì œ ë°ì´í„° $x$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $D(x)$ (ì§„ì§œë¼ê³  íŒë‹¨í•  í™•ë¥ )ê°€ 1ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•©ë‹ˆë‹¤. $\\log(1)$ ì€ 0ìœ¼ë¡œ ìµœëŒ€ê°’ì…ë‹ˆë‹¤. $\\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$: ê°€ì§œ ë°ì´í„° $G(z)$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $D(G(z))$ (ì§„ì§œë¼ê³  íŒë‹¨í•  í™•ë¥ )ê°€ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•©ë‹ˆë‹¤. $\\log(1-0)$ì€ 0ìœ¼ë¡œ ìµœëŒ€ê°’ì…ë‹ˆë‹¤. ì¦‰, $D$ëŠ” ì§„ì§œëŠ” ì§„ì§œë¡œ(1), ê°€ì§œëŠ” ê°€ì§œë¡œ(0) ì™„ë²½í•˜ê²Œ ë¶„ë¥˜í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. $\\min_{G} V(D, G)$: $G$ì˜ ê´€ì ì…ë‹ˆë‹¤. $G$ëŠ” $V$ë¥¼ ìµœì†Œí™”í•˜ë ¤ í•©ë‹ˆë‹¤. $G$ ëŠ” ì²« ë²ˆì§¸ í•­( $\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)]$ )ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. $G$ëŠ” ë‘ ë²ˆì§¸ í•­($\\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$)ë§Œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $G$ëŠ” $D$ê°€ $G(z)$ë¥¼ ì§„ì§œë¼ê³  ì†ê²Œ(ì¦‰, $D(G(z))$ê°€ 1ì´ ë˜ê²Œ) ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. $D(G(z))$ê°€ 1ì— ê°€ê¹Œì›Œì§€ë©´ $\\log(1 - 1)$ì€ $-\\infty$ì— ê°€ê¹Œì›Œì§€ë©°, ì´ëŠ” $V$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ì…ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì€ ì´ ì ëŒ€ì  í•™ìŠµ ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. $p_{data}$ (ê²€ì€ ì ì„ , ì‹¤ì œ ë°ì´í„°)ì™€ $p_g$ (ë…¹ìƒ‰ ì‹¤ì„ , $G$ê°€ ìƒì„±)ê°€ ìˆê³ , $D$ (íŒŒë€ íŒŒì„ )ê°€ ì´ ë‘˜ì„ êµ¬ë³„í•˜ë ¤ í•©ë‹ˆë‹¤. (b)ì—ì„œ $D$ê°€ ë‘ ë¶„í¬ë¥¼ êµ¬ë³„í•˜ë„ë¡ ì—…ë°ì´íŠ¸ë˜ê³ , (c)ì—ì„œ $G$ê°€ $D$ë¥¼ ì†ì´ëŠ” ë°©í–¥(ì¦‰, $D$ê°€ \u0026lsquo;ì§„ì§œ\u0026rsquo;ë¼ê³  íŒë‹¨í•˜ëŠ” ë†’ì€ ì˜ì—­)ìœ¼ë¡œ $p_g$ë¥¼ ì´ë™ì‹œí‚¤ë„ë¡ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. (d)ëŠ” $p_g = p_{data}$ê°€ ë˜ì–´ $G$ì™€ $D$ê°€ ë” ì´ìƒ ê°œì„ ë  ìˆ˜ ì—†ëŠ” í‰í˜• ìƒíƒœ($D(x) = \\frac{1}{2}$)ì— ë„ë‹¬í•œ ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nAlgorithm 1ì€ ì´ ê³¼ì •ì„ í•™ìŠµí•˜ëŠ” êµ¬ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. minibatch stochastic gradient descent (ë¯¸ë‹ˆë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•™ìŠµ ë£¨í”„ ì•ˆì—ì„œ:\n$D$ë¥¼ $k$ ìŠ¤í… ë™ì•ˆ ìµœì í™”í•©ë‹ˆë‹¤. (ì‹¤í—˜ì—ì„œëŠ” $k=1$ì„ ì‚¬ìš©)\në…¸ì´ì¦ˆ ìƒ˜í”Œ $z$ì™€ ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ $x$ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë½‘ìŠµë‹ˆë‹¤. $D$ì˜ stochastic gradient (í™•ë¥ ì  ê²½ì‚¬)ë¥¼ ìƒìŠ¹ (ascending)ì‹œí‚µë‹ˆë‹¤. $D$ Gradient Update: $$ \\nabla_{\\theta_d}\\frac{1}{m}\\sum_{i=1}^{m}[\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)})))] $$ $G$ë¥¼ 1 ìŠ¤í… ìµœì í™”í•©ë‹ˆë‹¤.\nìƒˆë¡œìš´ ë…¸ì´ì¦ˆ ìƒ˜í”Œ $z$ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë½‘ìŠµë‹ˆë‹¤. $G$ì˜ stochastic gradient (í™•ë¥ ì  ê²½ì‚¬)ë¥¼ í•˜ê°• (descending)ì‹œí‚µë‹ˆë‹¤. $G$ Gradient Update: $$ \\nabla_{\\theta_g}\\frac{1}{m}\\sum_{i=1}^{m}\\log(1 - D(G(z^{(i)})))] $$ ì´ ë…¼ë¬¸ì€ $G$ë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ $\\log(1 - D(G(z)))$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒë³´ë‹¤ $\\log D(G(z))$ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ì´ˆê¸°ì— ë” ê°•ë ¥í•œ gradientë¥¼ ì œê³µí•œë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤.\nProposition 1: ì£¼ì–´ì§„ $G$ì— ëŒ€í•´, ìµœì ì˜ íŒë³„ì $D$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. $$ D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} $$ ì¦ëª…:\nëª©í‘œ: $G$ê°€ ê³ ì •ë˜ì–´ ìˆì„ ë•Œ, $D$ëŠ” $V(G, D)$ë¥¼ ìµœëŒ€í™”í•˜ë ¤ê³  í•©ë‹ˆë‹¤. $V(G, D)$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ : $$ V(G, D) = \\int_x p_{data}(x) \\log(D(x)) dx + \\int_z p_z(z) \\log(1 - D(G(z))) dz $$ $$ = \\int_x \\left[ p_{data}(x) \\log(D(x)) + p_g(x) \\log(1 - D(x)) \\right] dx $$ ($z$ì— ëŒ€í•œ ì ë¶„ì„ $x$ì— ëŒ€í•œ ì ë¶„ìœ¼ë¡œ ë³€í™˜. $p_g(x)$ëŠ” $G(z)$ ìƒ˜í”Œë“¤ì˜ ë¶„í¬)\n$V(G, D)$ëŠ” ëª¨ë“  $x$ì— ëŒ€í•œ ì ë¶„ì…ë‹ˆë‹¤. ì´ ì ë¶„ê°’ì„ ìµœëŒ€í™”í•˜ë ¤ë©´, ì ë¶„ ì•ˆì˜ í•¨ìˆ˜, ì¦‰ $f(y) = a \\log(y) + b \\log(1-y)$ í˜•íƒœì˜ ê°’ì„ ëª¨ë“  $x$ ì§€ì ì—ì„œ ê°œë³„ì ìœ¼ë¡œ ìµœëŒ€í™”í•˜ë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $y = D(x)$, $a = p_{data}(x)$, $b = p_g(x)$ì…ë‹ˆë‹¤.\nì´ í•¨ìˆ˜ì˜ ìµœëŒ“ê°’ì„ ì°¾ê¸° ìœ„í•´ $y$ì— ëŒ€í•´ ë¯¸ë¶„í•˜ì—¬ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤: $$ \\frac{d}{dy} f(y) = \\frac{a}{y} + \\frac{b(-1)}{1-y} = \\frac{a}{y} - \\frac{b}{1-y} $$ $$ \\frac{a}{y} - \\frac{b}{1-y} = 0 \\Rightarrow \\frac{a}{y} = \\frac{b}{1-y} \\Rightarrow a(1-y) = b(y) $$ $$ a - ay = by \\Rightarrow a = (a+b)y \\Rightarrow y = \\frac{a}{a+b} $$ ê²°ë¡ : $y = D(x)$, $a = p_{data}(x)$, $b = p_g(x)$ë¥¼ ë‹¤ì‹œ ëŒ€ì…í•˜ë©´, $V(G, D)$ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ $D(x)$ëŠ” ì •í™•íˆ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. $$ D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} $$ Theorem 1: $C(G)$ ì˜ ì „ì—­ ìµœì†Œê°’ì€ $p_g = p_{data}$ ì¼ ë•Œë§Œ ë‹¬ì„±ë˜ë©°, ê·¸ ê°’ì€ $-\\log 4$ ì´ë‹¤.\nì¦ëª…:\n$C(G)$ ì •ì˜: $C(G)$ ëŠ” $G$ ê°€ ì–¼ë§ˆë‚˜ ëª»í•˜ê³  ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” $G$ ì˜ \u0026lsquo;ë¹„ìš© í•¨ìˆ˜\u0026rsquo;ì…ë‹ˆë‹¤. $D$ ê°€ í•­ìƒ ìµœì ì˜ $D_G^*(x)$ ë¼ê³  ê°€ì •í•  ë•Œì˜ $V(G, D)$ ê°’ì…ë‹ˆë‹¤. $$ C(G) = \\max_D V(G, D) = V(G, D_G^*) $$ ë…¼ë¬¸ì€ $D_G^*(x)$ ë¥¼ $V(G, D)$ ì— ë‹¤ì‹œ ëŒ€ì…í•˜ë©´ : $$ C(G) = \\mathbb{E}_{x \\sim p_{data}}\\left[\\log\\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\\right] + \\mathbb{E}_{x \\sim p_{g}}\\left[\\log\\frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)}\\right] $$ ìµœì  ì§€ì  ( $p_g = p_{data}$ ): ë§Œì•½ $p_g = p_{data}$ ë¼ë©´, Proposition 1ì˜ ì‹ì— ëŒ€ì…í•˜ë©´ $D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{data}(x)} = \\frac{1}{2}$ ì´ ë©ë‹ˆë‹¤. $$ C(G) = \\mathbb{E}_{x \\sim p_{data}}[\\log(\\frac{1}{2})] + \\mathbb{E}_{x \\sim p_g}[\\log(\\frac{1}{2})] = \\log(\\frac{1}{2}) + \\log(\\frac{1}{2}) = -2\\log 2 = -\\log 4 $$ ì´ëŠ” $p_g = p_{data}$ ì¼ ë•Œ $C(G)$ ì˜ ê°’ì´ $-\\log 4$ ì„ì„ ë³´ì…ë‹ˆë‹¤.\nì´ê²ƒì´ ìœ ì¼í•œ ìµœì†Œê°’ì¸ê°€?: ë…¼ë¬¸ì€ $C(G)$ ì™€ $-\\log 4$ ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. $-\\log 4$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$ -\\log 4 = -2\\log 2 = \\mathbb{E}_{x \\sim p_{data}}[-\\log 2] + \\mathbb{E}_{x \\sim p_g}[-\\log 2] $$ $C(G) - (-\\log 4)$ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤: $$ C(G) - (-\\log 4) = \\left( \\mathbb{E}_{x \\sim p_{data}}\\left[\\log\\frac{p_{data}}{p_{data}+p_{g}}\\right] + \\mathbb{E}_{x \\sim p_{g}}\\left[\\log\\frac{p_{g}}{p_{data}+p_{g}}\\right] \\right) - \\left( \\mathbb{E}_{x \\sim p_{data}}[-\\log 2] + \\mathbb{E}_{x \\sim p_g}[-\\log 2] \\right) $$ ê¸°ëŒ“ê°’( $\\mathbb{E}$ )ì„ í•˜ë‚˜ë¡œ ë¬¶ê³  ë¡œê·¸ì˜ ì„±ì§ˆ($\\log A - \\log B = \\log(A/B)$)ì„ ì´ìš©í•©ë‹ˆë‹¤: $$ = \\mathbb{E}_{x \\sim p_{data}}\\left[\\log\\frac{p_{data}}{p_{data}+p_{g}} - (-\\log 2)\\right] + \\mathbb{E}_{x \\sim p_{g}}\\left[\\log\\frac{p_{g}}{p_{data}+p_{g}} - (-\\log 2)\\right] $$ $$ = \\mathbb{E}_{x \\sim p_{data}}\\left[\\log\\frac{p_{data}}{(p_{data}+p_{g})/2}\\right] + \\mathbb{E}_{x \\sim p_{g}}\\left[\\log\\frac{p_{g}}{(p_{data}+p_{g})/2}\\right] $$ ì´ê²ƒì´ ë°”ë¡œ Kullback-Leibler (KL) ë°œì‚°ì˜ ì •ì˜ì…ë‹ˆë‹¤. $KL(P || Q) = \\mathbb{E}_{x \\sim P}[\\log \\frac{P(x)}{Q(x)}]$. $$ = KL\\left(p_{data} \\left|\\right| \\frac{p_{data}+p_{g}}{2}\\right) + KL\\left(p_{g} \\left|\\right| \\frac{p_{data}+p_{g}}{2}\\right) $$ ë”°ë¼ì„œ, $C(G) = -\\log 4 + KL(\\dots) + KL(\\dots)$ì…ë‹ˆë‹¤. ì´ ë‘ KL í•­ì˜ í•©ì€ **Jensen-Shannon Divergence (JSD)**ì˜ $2$ë°°ì™€ ê°™ìŠµë‹ˆë‹¤. $$ C(G) = -\\log(4) + 2 \\cdot JSD(p_{data} || p_g) $$ ìµœì¢… ê²°ë¡ : JSDëŠ” ë‘ ë¶„í¬ ê°„ì˜ \u0026lsquo;ê±°ë¦¬\u0026rsquo;ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë©°, í•­ìƒ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤ ($JSD \\ge 0$). JSDê°€ 0ì´ ë˜ëŠ” ìœ ì¼í•œ ê²½ìš°ëŠ” ë‘ ë¶„í¬ê°€ ì •í™•íˆ ê°™ì„ ë•Œì…ë‹ˆë‹¤ ($p_{data} = p_g$). ë”°ë¼ì„œ $C(G)$ëŠ” $C(G) = -\\log 4 + (\\text{í•­ìƒ 0 ì´ìƒì¸ ê°’})$ ì…ë‹ˆë‹¤. $C(G)$ì˜ ì „ì—­ ìµœì†Œê°’($C^*$)ì€ $JSD=0$ì¼ ë•Œ ë‹¬ì„±ë˜ë©°, ê·¸ ê°’ì€ $-\\log 4$ì…ë‹ˆë‹¤.\n$JSD=0$ì€ $p_g = p_{data}$ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ, $G$ì˜ í•™ìŠµ ëª©í‘œ($C(G)$ ìµœì†Œí™”)ê°€ $p_g$ë¥¼ $p_{data}$ì™€ ê°™ê²Œ ë§Œë“œëŠ” ê²ƒê³¼ ë™ì¼í•¨ì´ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ë˜ì—ˆìŠµë‹ˆë‹¤.\nAlgorithm 1ì˜ ìˆ˜ë ´ì„± (Convergence) Proposition 2: $G$ ì™€ $D$ ê°€ ì¶©ë¶„í•œ ìš©ëŸ‰ì„ ê°–ê³ , $D$ ê°€ $G$ ì— ëŒ€í•´ í•­ìƒ ìµœì ì— ë„ë‹¬í•  ìˆ˜ ìˆìœ¼ë©°, $p_g$ ê°€ ê¸°ì¤€ì„ ê°œì„ í•˜ë„ë¡ ì—…ë°ì´íŠ¸ëœë‹¤ë©´, $p_g$ ëŠ” $p_{data}$ ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.\nëª©í‘œ: $G$ ì— ëŒ€í•œ ë¹„ìš© í•¨ìˆ˜ $C(G) = \\sup_D V(G, D) = \\sup_D U(p_g, D)$ ë¥¼ ìµœì†Œí™”í•˜ëŠ” gradient (ê¸°ìš¸ê¸°) ì—…ë°ì´íŠ¸ê°€ $p_g \\rightarrow p_{data}$ ë¡œ ì´ë„ëŠ”ê°€? (ì°¸ê³ : $V(G,D)$ ë¥¼ $p_g$ ì˜ í•¨ìˆ˜ë¡œ ë³´ê¸° ìœ„í•´ $U(p_g, D)$ ë¡œ í‘œê¸°í–ˆìŠµë‹ˆë‹¤.)\ní•µì‹¬ 1 (Convexity): $U(p_g, D)$ ëŠ” $p_g$ ì— ëŒ€í•´ convex (ë³¼ë¡) í•¨ìˆ˜ì…ë‹ˆë‹¤.\nì´ìœ : $U(p_g, D) = \\int [ p_{data} \\log D + p_g \\log(1-D) ] dx$ ì…ë‹ˆë‹¤. ì´ëŠ” $p_g$ ì— ëŒ€í•œ $A + \\int p_g \\cdot B(x) dx$ í˜•íƒœì˜ ì„ í˜•(linear) í•¨ìˆ˜ì´ë©°, ì„ í˜• í•¨ìˆ˜ëŠ” convexì…ë‹ˆë‹¤. í•µì‹¬ 2 (Supremum): $C(G)$ ëŠ” convex í•¨ìˆ˜( $U$ )ë“¤ì˜ supremum (ìƒí•œ, ì—¬ê¸°ì„œëŠ” $\\max$ )ì…ë‹ˆë‹¤. convex í•¨ìˆ˜ë“¤ì˜ supremum ì—­ì‹œ convex í•¨ìˆ˜ì…ë‹ˆë‹¤.\në”°ë¼ì„œ $G$ ì˜ ë¹„ìš© í•¨ìˆ˜ $C(G)$ ëŠ” $p_g$ ì— ëŒ€í•´ convexí•©ë‹ˆë‹¤. í•µì‹¬ 3 (Gradient): $C(G) = \\sup_D U(p_g, D)$ ì™€ ê°™ì€ í•¨ìˆ˜ì˜ subderivativeë¥¼ ì–´ë–»ê²Œ ê³„ì‚°í•˜ëŠ”ê°€?\nDanskin\u0026rsquo;s Theoremì— ë”°ë¥´ë©´, $\\sup_D U(p_g, D)$ ì˜ $p_g$ ì— ëŒ€í•œ gradientëŠ”, $U(p_g, D)$ ë¥¼ ìµœëŒ€í™”í•˜ëŠ” $D$ (ì¦‰, $D_G^*$ ) ë¥¼ ì°¾ì€ ë‹¤ìŒ, ê·¸ $D_G^*$ ë¥¼ ê³ ì •í•œ ì±„ $U(p_g, D_G^*)$ ë¥¼ $p_g$ ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ê³¼ ê°™ìŠµë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ 1ì€ ì •í™•íˆ ì´ ì‘ì—…ì„ ê·¼ì‚¬ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. (1) $D$ ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤ ( $D_G^*$ ë¥¼ ê·¼ì‚¬). (2) $G$ ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ ( $U(p_g, D_G^*)$ ì˜ gradient descentë¥¼ ìˆ˜í–‰). ê²°ë¡ : $C(G)$ ëŠ” $p_g$ ì— ëŒ€í•´ convex í•¨ìˆ˜ì´ë©°, Theorem 1ì—ì„œ $p_g = p_{data}$ ë¼ëŠ” ìœ ì¼í•œ ì „ì—­ ìµœì í•´ë¥¼ ê°€ì§ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\nConvex í•¨ìˆ˜ë¥¼ gradient descent (ê²½ì‚¬ í•˜ê°•ë²•)ë¡œ ìµœì í™”í•˜ë©´ (ì¶©ë¶„íˆ ì‘ì€ ì—…ë°ì´íŠ¸ ìŠ¤í…ìœ¼ë¡œ), ê·¸ ìœ ì¼í•œ ì „ì—­ ìµœì í•´( $p_g = p_{data}$ )ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì´ ë³´ì¥ë©ë‹ˆë‹¤.\ní•œê³„ì  ë…¼ë¬¸ì€ ë§ˆì§€ë§‰ì— ì¤‘ìš”í•œ í˜„ì‹¤ì  ë¬¸ì œë¥¼ ì–¸ê¸‰í•©ë‹ˆë‹¤. ìœ„ì˜ ëª¨ë“  ì¦ëª…ì€ ìš°ë¦¬ê°€ $p_g$ë¼ëŠ” í™•ë¥  ë¶„í¬ ìì²´ë¥¼ ì§ì ‘ ìµœì í™”í•  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œ ë¹„ëª¨ìˆ˜ì (non-parametric) í™˜ê²½ì—ì„œ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤.\nì‹¤ì œ: ìš°ë¦¬ëŠ” $p_g$ ìì²´ë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, $G(z; \\theta_g)$ë¼ëŠ” multilayer perceptron (ì‹ ê²½ë§)ì˜ **íŒŒë¼ë¯¸í„° $\\theta_g$**ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.\në¬¸ì œ: $C(G)$ê°€ $p_g$ì— ëŒ€í•´ì„œëŠ” convexí• ì§€ë¼ë„, $G$ì˜ íŒŒë¼ë¯¸í„° $\\theta_g$ì— ëŒ€í•´ì„œëŠ” ì „í˜€ convexí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. $\\theta_g$ ê³µê°„ì€ critical points (ì„ê³„ì , ì¦‰ local minima ë‚˜ saddle point)ê°€ ë§ìŠµë‹ˆë‹¤.\nê²°ë¡ : ë”°ë¼ì„œ ì‹¤ì œ í•™ìŠµ(Algorithm 1)ì´ ì´ë¡ ì ì¸ ì „ì—­ ìµœì í•´($p_g = p_{data}$)ì— ë„ë‹¬í•œë‹¤ëŠ” ì´ë¡ ì  ë³´ì¥ì€ ì—†ìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë…¼ë¬¸ì€ multilayer perceptronì´ ì‹¤í—˜ì—ì„œ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê¸° ë•Œë¬¸ì—, ì´ ì´ë¡ ì  ë³´ì¥ì˜ ë¶€ì¬ì—ë„ ë¶ˆêµ¬í•˜ê³  í•©ë¦¬ì ì¸ ëª¨ë¸ì´ë¼ê³  ì£¼ì¥í•©ë‹ˆë‹¤. (ğŸ¤”??)\nIII. ì‹¤í—˜ (Experiments) Adversarial netsëŠ” MNIST, Toronto Face Database (TFD), CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.\nìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ëŠ” ìƒì„±ëœ ìƒ˜í”Œì˜ log-likelihoodë¥¼ ì§ì ‘ ê³„ì‚°í•˜ê¸° ì–´ë ¤ì›Œ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ Gaussian Parzen window (ê°€ìš°ì‹œì•ˆ íŒŒì   ì°½) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ì…‹ ë°ì´í„°ì˜ í™•ë¥ ì„ ì¶”ì •í–ˆìŠµë‹ˆë‹¤.\nTable 1ì€ ì´ Parzen window ê¸°ë°˜ log-likelihood ì¶”ì • ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. MNISTì™€ TFD ë°ì´í„°ì…‹ì— ëŒ€í•´ Adversarial netsì˜ ê²°ê³¼ë¥¼ DBN, Stacked CAE (Stacked Contractive Auto-encoders), Deep GSN ë“± ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•©ë‹ˆë‹¤. ì´ í‘œëŠ” Adversarial netsê°€ ê¸°ì¡´ì˜ ìš°ìˆ˜í•œ ìƒì„± ëª¨ë¸ë“¤ê³¼ \u0026ldquo;ìµœì†Œí•œ ê²½ìŸë ¥ ìˆëŠ”(at least competitive)\u0026rdquo; ì„±ëŠ¥ì„ ë³´ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nFigure 2ëŠ” í›ˆë ¨ëœ generator ë„·ì—ì„œ ì¶”ì¶œí•œ ìƒ˜í”Œë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. MNIST (ìˆ«ì), TFD (ì–¼êµ´), CIFAR-10 (ë¬¼ì²´)ì— ëŒ€í•´ ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ê·¸ë¦¼ì—ì„œ ì¤‘ìš”í•œ ì ì€ ê° ì´ë¯¸ì§€ ê·¸ë¦¬ë“œì˜ ê°€ì¥ ì˜¤ë¥¸ìª½ ì—´ì…ë‹ˆë‹¤. ì´ ì—´ì€ ë°”ë¡œ ì™¼ìª½ì˜ ìƒì„±ëœ ìƒ˜í”Œê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì‹¤ì œ í›ˆë ¨ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ë‹¨ìˆœíˆ **\u0026ldquo;ì•”ê¸°(memorized)\u0026rdquo;**í•œ ê²ƒì´ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±í•˜ê³  ìˆìŒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ìƒ˜í”Œë“¤ì€ Markov chainì— ì˜ì¡´í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.\nFigure 3ì€ $z$ ê³µê°„(ë…¸ì´ì¦ˆ ì…ë ¥ ê³µê°„)ì˜ ì¢Œí‘œ ì‚¬ì´ë¥¼ linearly interpolating (ì„ í˜• ë³´ê°„)í•˜ì—¬ ì–»ì€ ìˆ«ì ì´ë¯¸ì§€ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \u0026lsquo;1\u0026rsquo;ì„ ìƒì„±í•˜ëŠ” $z$ ë²¡í„°ì™€ \u0026lsquo;5\u0026rsquo;ë¥¼ ìƒì„±í•˜ëŠ” $z$ ë²¡í„° ì‚¬ì´ì˜ ì¤‘ê°„ ì§€ì ë“¤ì„ $G$ì— ì…ë ¥í•˜ë©´, \u0026lsquo;1\u0026rsquo;ì´ \u0026lsquo;5\u0026rsquo;ë¡œ ë¶€ë“œëŸ½ê²Œ ë³€í•´ê°€ëŠ” ì¤‘ê°„ í˜•íƒœì˜ ìˆ«ì ì´ë¯¸ì§€ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ëŠ” $G$ê°€ ì˜ë¯¸ ìˆëŠ” representation (í‘œí˜„)ì„ í•™ìŠµí–ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\nIV. ê²°ë¡  (Advantages and disadvantages) ì´ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ëŠ” ì¥ì ê³¼ ë‹¨ì ì„ ëª¨ë‘ ê°€ì§‘ë‹ˆë‹¤.\nTable 2ëŠ” generative modeling (ìƒì„± ëª¨ë¸ë§)ì˜ ì—¬ëŸ¬ ì ‘ê·¼ë²•ë“¤ì´ ê²ªëŠ” ì–´ë ¤ì›€ë“¤ì„ ìš”ì•½í•©ë‹ˆë‹¤. Adversarial modelsë¥¼ Deep directed graphical models, Deep undirected graphical models, Generative autoencodersì™€ ë¹„êµí•©ë‹ˆë‹¤.\nì¥ì :\nMarkov chainsê°€ ì „í˜€ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œë§ì€ $G$ë¥¼ í†µí•œ ë‹¨ í•œ ë²ˆì˜ forward propagation (ìˆœì „íŒŒ)ì…ë‹ˆë‹¤. í•™ìŠµ ì¤‘ inference (ì¶”ë¡ )ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤. gradient (ê¸°ìš¸ê¸°)ë¥¼ ì–»ê¸° ìœ„í•´ backpropagation (ì—­ì „íŒŒ)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§¤ìš° ë‚ ì¹´ë¡­ê±°ë‚˜ degenerate (í‡´í™”ëœ) ë¶„í¬(ì¦‰, í•œ ì ì— ëª¨ì¸ ë¶„í¬)ë„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë°˜ë©´ MCMC ê¸°ë°˜ ë°©ë²•ë“¤ì€ ë¶„í¬ê°€ ì–´ëŠ ì •ë„ \u0026ldquo;íë¦¿í•´ì•¼(blurry)\u0026rdquo; ë¯¹ì‹±ì´ ì˜ ë©ë‹ˆë‹¤). ë‹¨ì :\n$p_g(x)$ (ìƒì„±ëœ ë°ì´í„°ì˜ í™•ë¥ )ì— ëŒ€í•œ ëª…ì‹œì ì¸ í‘œí˜„ì´ ì—†ìŠµë‹ˆë‹¤. $D$ì™€ $G$ê°€ í•™ìŠµ ì¤‘ì— ì˜ **\u0026ldquo;ë™ê¸°í™”(synchronized)\u0026rdquo;**ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ $G$ê°€ $D$ì˜ ì—…ë°ì´íŠ¸ ì—†ì´ ë„ˆë¬´ ë§ì´ í•™ìŠµë˜ë©´, $G$ê°€ $p_{data}$ë¥¼ ëª¨ë¸ë§í•˜ê¸°ì— ì¶©ë¶„í•œ ë‹¤ì–‘ì„±ì„ ê°–ì§€ ëª»í•˜ê³  íŠ¹ì • $x$ ê°’ìœ¼ë¡œ **collapse (ë¶•ê´´)**í•˜ëŠ” \u0026ldquo;Helvetica ì‹œë‚˜ë¦¬ì˜¤\u0026rdquo; (ì—¬ëŸ¬ $z$ê°€ ë˜‘ê°™ì€ $x$ë¡œ ë§¤í•‘ë˜ëŠ” í˜„ìƒ)ì— ë¹ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ","permalink":"https://mookjsi.github.io/posts/paper-review-gan/","summary":"NIPS 2014ì—ì„œ ë°œí‘œëœ \u0026lsquo;Generative Adversarial Nets\u0026rsquo; (GAN) ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ì…ë‹ˆë‹¤.","title":"Generative Adversarial Nets (GAN) ë…¼ë¬¸ ë¦¬ë·°"},{"content":"ì´ ë…¼ë¬¸ì€ ì‚¬ëŒì´ ì¼ì¼ì´ ì‚¬ì§„ì— \u0026ldquo;ì´ê±´ ê³ ì–‘ì´ì•¼\u0026rdquo;, \u0026ldquo;ì €ê±´ ê°•ì•„ì§€ì•¼\u0026quot;ë¼ê³  ì•Œë ¤ì£¼ì§€ ì•Šì•„ë„, ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. íŠ¹íˆ ì•„ì£¼ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ì—¬ í° ì£¼ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\nì»´í“¨í„°ì—ê²Œ ì‚¬ì§„ì„ ì£¼ê³  ì´ê²Œ ë­”ì§€ ë§í˜€ë³´ë¼ê³  ì‹œí‚¤ëŠ” ê±¸ **ì§€ë„ í•™ìŠµ(Supervised Learning)**ì´ë¼ í•©ë‹ˆë‹¤. ì •ë‹µì„ ì•Œë ¤ì£¼ë©´ì„œ ê³µë¶€ì‹œí‚¤ëŠ” ê±°ì£ . í•˜ì§€ë§Œ ì„¸ìƒì—ëŠ” ì •ë‹µì´ ì—†ëŠ” ì‚¬ì§„ì´ í›¨ì”¬ ë§ìŠµë‹ˆë‹¤. ì´ ì •ë‹µ ì—†ëŠ” ì‚¬ì§„ë“¤ë¡œ ì»´í“¨í„°ë¥¼ ë˜‘ë˜‘í•˜ê²Œ ë§Œë“¤ ìˆœ ì—†ì„ê¹Œìš”? ì´ê²Œ ë°”ë¡œ **\u0026lsquo;ìê¸° ì§€ë„ í•™ìŠµ(Self-Supervised Learning)\u0026rsquo;**ì˜ ëª©í‘œì…ë‹ˆë‹¤.\nSimCLRì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ **\u0026lsquo;ëŒ€ì¡° í•™ìŠµ(Contrastive Learning)\u0026rsquo;**ì´ë¼ëŠ” ê±´ë°ìš”, ì•„ì£¼ ê°„ë‹¨í•œ ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí•©ë‹ˆë‹¤.\nì˜ˆì‹œ: ê°•ì•„ì§€ ì‚¬ì§„ì´ í•œ ì¥ ìˆë‹¤ê³  ìƒìƒí•´ ë´…ì‹œë‹¤.\nì´ ì‚¬ì§„ì„ ê°€ì§€ê³  ì•½ê°„ ë‹¤ë¥¸ ë²„ì „ì˜ ì‚¬ì§„ ë‘ ì¥ì„ ë§Œë“­ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ ì¥ì€ ê°•ì•„ì§€ ì–¼êµ´ë§Œ í™•ëŒ€(crop)í•˜ê³ , ë‹¤ë¥¸ í•œ ì¥ì€ ìƒ‰ê°ì„ ì‚´ì§ ë°”ê¾¸ëŠ”(color distortion) ê±°ì£ . ì»´í“¨í„°ì—ê²Œ ì´ ë‘ ì‚¬ì§„ì€ \u0026lsquo;ê°™ì€ ê°•ì•„ì§€\u0026rsquo;ì—ì„œ ë‚˜ì˜¨ ê¸ì •ì  ìŒ(positive pair) ì´ë¼ê³  ì•Œë ¤ì¤ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë™ì‹œì—, ì „í˜€ ë‹¤ë¥¸ ì‚¬ì§„ë“¤(ì˜ˆ: ê³ ì–‘ì´, ìë™ì°¨, ë‚˜ë¬´ ì‚¬ì§„)ì„ ë³´ì—¬ì£¼ë©´ì„œ, ì´ê²ƒë“¤ì€ ë°©ê¸ˆ ë³¸ ê°•ì•„ì§€ ì‚¬ì§„ê³¼ ë‹¤ë¥¸ ë¶€ì •ì  ìŒ(negative pair) ì´ë¼ê³  ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ìˆ˜ë§ì€ ì‚¬ì§„ìœ¼ë¡œ ë°˜ë³µí•˜ë©´, ì»´í“¨í„°ëŠ” ì ì°¨ \u0026lsquo;ì–´ë–¤ íŠ¹ì§•\u0026rsquo;ì´ ê°™ì€ ëŒ€ìƒì„ ë‚˜íƒ€ë‚´ê³ , \u0026lsquo;ì–´ë–¤ íŠ¹ì§•\u0026rsquo;ì´ ë‹¤ë¥¸ ëŒ€ìƒì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤. ì¦‰, \u0026lsquo;ê°•ì•„ì§€ë‹¤ì›€\u0026rsquo;ì´ ë¬´ì—‡ì¸ì§€ ê·¸ ë³¸ì§ˆì ì¸ **í‘œí˜„(representation)**ì„ í•™ìŠµí•˜ê²Œ ë˜ëŠ” ê±°ì£ . SimCLRì€ ì´ ê°„ë‹¨í•œ ì›ë¦¬ë¥¼ ë§¤ìš° íš¨ê³¼ì ìœ¼ë¡œ êµ¬í˜„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\nì°¸ê³ : Stochastic Optimization Review (í‘œí˜„ í•™ìŠµ ê´€ë ¨ ë°œí‘œ ìë£Œ) 1. ì„œë¡  (Introduction) ì—°êµ¬ì˜ ì‹œì‘ì€ \u0026ldquo;ì‚¬ëŒì˜ ì •ë‹µ ì—†ì´ ì–´ë–»ê²Œ ì»´í“¨í„°ê°€ **ì‹œê°ì  í‘œí˜„(visual representation)**ì„ ë°°ìš¸ ìˆ˜ ìˆì„ê¹Œ?\u0026ldquo;ë¼ëŠ” ì˜¤ëœ ì§ˆë¬¸ì—ì„œ ì¶œë°œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ë³µì¡í•œ êµ¬ì¡°ë¥¼ í•„ìš”ë¡œ í•˜ê±°ë‚˜, ëŒ€ëŸ‰ì˜ ì´ì „ ë°ì´í„°ë¥¼ ì €ì¥í•´ë‘ëŠ” ë©”ëª¨ë¦¬ ë±…í¬(memory bank) ê°™ì€ ë¶€ê°€ì ì¸ ì¥ì¹˜ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤.\nFigure 1ì€ SimCLRì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ì—¬ëŸ¬ ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„ì…ë‹ˆë‹¤. ê°€ë¡œì¶•ì€ ëª¨ë¸ì˜ í¬ê¸°(Number of Parameters), ì„¸ë¡œì¶•ì€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì •í™•ë„(ImageNet Top-1 Accuracy)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ì—ì„œ **SimCLR(ë³„ ëª¨ì–‘ â˜…)**ì€ ë‹¤ë¥¸ ëª¨ë¸ë“¤(ì  ëª¨ì–‘ â€¢)ì— ë¹„í•´ í›¨ì”¬ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ë©°, ì‹¬ì§€ì–´ ì‚¬ëŒì´ ì •ë‹µì„ ì•Œë ¤ì£¼ê³  í•™ìŠµì‹œí‚¨ **ì§€ë„ í•™ìŠµ ëª¨ë¸(Supervised, íšŒìƒ‰ ì‹­ìê°€)**ì˜ ì„±ëŠ¥ê³¼ ë§ë¨¹ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° ê°„ë‹¨í•œ í”„ë ˆì„ì›Œí¬ê°€ ê¸°ì¡´ì˜ ë³µì¡í•œ ë°©ë²•ë“¤ì„ ë›°ì–´ë„˜ì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì€ SimCLRì˜ ì„±ê³µ ë¹„ê²°ì´ ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ ìš”ì†Œì˜ ì¡°í•©ì— ìˆìŒì„ ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•´ ë³´ì—¬ì¤ë‹ˆë‹¤.\në°ì´í„° ì¦ê°• ê¸°ë²•ì˜ ì¡°í•©ì´ íš¨ê³¼ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ëª¨ë¸ì˜ ìµœì¢… í‘œí˜„(representation)ê³¼ ì†ì‹¤ í•¨ìˆ˜(loss function) ì‚¬ì´ì— **ë¹„ì„ í˜• ë³€í™˜(nonlinear transformation)**ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤. ëŒ€ì¡° í•™ìŠµì€ ì§€ë„ í•™ìŠµë³´ë‹¤ ë” í° **ë°°ì¹˜ í¬ê¸°(batch size)**ì™€ ë” ê¸´ í•™ìŠµ ì‹œê°„ìœ¼ë¡œë¶€í„° ë” í° ì´ë“ì„ ì–»ëŠ”ë‹¤. 2. ê¸°ìˆ  ì„¤ëª… (Technical Description) SimCLR í”„ë ˆì„ì›Œí¬ëŠ” ë„¤ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\nFigure 2ëŠ” ì´ ì „ì²´ ê³¼ì •ì„ ì•„ì£¼ ì˜ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ì´ë¯¸ì§€ $x$ì—ì„œ ì‹œì‘í•´ì„œ, ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì¦ê°•(augmentation) $t$ì™€ $t\u0026rsquo;$ë¥¼ ì ìš©í•˜ì—¬ ë‘ ê°œì˜ ìƒˆë¡œìš´ ì´ë¯¸ì§€ $\\tilde{x}_i$ì™€ $\\tilde{x}_j$ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ ë‘˜ì€ ì„œë¡œ ì—°ê´€ëœ **\u0026lsquo;ê¸ì •ì  ìŒ\u0026rsquo;**ì´ ë©ë‹ˆë‹¤. ì´ ë‘ ì´ë¯¸ì§€ëŠ” ë™ì¼í•œ ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬(encoder network) $f(\\cdot)$ë¥¼ í†µê³¼í•˜ì—¬ ê°ê°ì˜ í‘œí˜„ ë²¡í„°(representation vector) $h_i$ì™€ $h_j$ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ê·¸ í›„, ì´ í‘œí˜„ ë²¡í„°ë“¤ì€ ë˜ ë‹¤ë¥¸ ì‘ì€ ì‹ ê²½ë§ì¸ íˆ¬ì˜ í—¤ë“œ(projection head) $g(\\cdot)$ë¥¼ ê±°ì³ ìµœì¢…ì ìœ¼ë¡œ $z_i$ì™€ $z_j$ë¼ëŠ” ë²¡í„°ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. í•™ìŠµì˜ ëª©í‘œëŠ” ì´ $z_i$ì™€ $z_j$ ì‚¬ì´ì˜ **ìœ ì‚¬ë„(agreement)**ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•™ìŠµì´ ëë‚˜ë©´ íˆ¬ì˜ í—¤ë“œ $g(\\cdot)$ëŠ” ë²„ë¦¬ê³ , í‘œí˜„ ë²¡í„° $h$ë¥¼ ë‹¤ë¥¸ ì‘ì—…(ì˜ˆ: ì´ë¯¸ì§€ ë¶„ë¥˜)ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\në°ì´í„° ì¦ê°• (Data Augmentation): í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œë¶€í„° ë‘ ê°œì˜ ì—°ê´€ëœ ë·°(view)ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë¬´ì‘ìœ„ ìë¥´ê¸° í›„ ì›ë˜ í¬ê¸°ë¡œ ë³µì›(random cropping and resize), ìƒ‰ìƒ ì™œê³¡(color distortion), **ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬(Gaussian blur)**ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤. ì–´ë–¤ ì¦ê°• ê¸°ë²•ì„ ì–´ë–»ê²Œ ì¡°í•©í•˜ëŠ”ì§€ê°€ ëª¨ë¸ ì„±ëŠ¥ì˜ í•µì‹¬ì…ë‹ˆë‹¤.\nê¸°ë³¸ ì¸ì½”ë” (Base Encoder): ì¦ê°•ëœ ì´ë¯¸ì§€ë¡œë¶€í„° í‘œí˜„ ë²¡í„° $h_i$ë¥¼ ì¶”ì¶œí•˜ëŠ” ì‹ ê²½ë§ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì£¼ë¡œ ResNet êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©°, íŠ¹ë³„í•œ êµ¬ì¡°ì  ì œì•½ì´ ì—†ì–´ ì–´ë–¤ ì‹ ê²½ë§ì´ë“  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $h_i = f(\\tilde{x}_i)$ ì…ë‹ˆë‹¤.\níˆ¬ì˜ í—¤ë“œ (Projection Head): ì¸ì½”ë”ì—ì„œ ë‚˜ì˜¨ í‘œí˜„ ë²¡í„° $h$ë¥¼ **ëŒ€ì¡° ì†ì‹¤(contrastive loss)**ì„ ê³„ì‚°í•˜ëŠ” ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ì‘ì€ ì‹ ê²½ë§ $g(\\cdot)$ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” í•˜ë‚˜ì˜ **ì€ë‹‰ì¸µ(hidden layer)**ì„ ê°€ì§„ **MLP(Multi-Layer Perceptron)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. $z_i = g(h_i)$ ì´ë©°, $h_i$ê°€ ì•„ë‹Œ $z_i$ì— ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì— í° ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\nëŒ€ì¡° ì†ì‹¤ í•¨ìˆ˜ (Contrastive Loss Function): ì£¼ì–´ì§„ \u0026lsquo;ê¸ì •ì  ìŒ\u0026rsquo;($z_i$, $z_j$)ì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , ë‚˜ë¨¸ì§€ ëª¨ë“  \u0026lsquo;ë¶€ì •ì  ìŒ\u0026rsquo;ê³¼ì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶”ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” NT-Xent (Normalized Temperature-scaled Cross Entropy) ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nAlgorithm 1ì€ ì´ SimCLR í•™ìŠµ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤. Nê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ê°€ì ¸ì™€ ê° ì´ë¯¸ì§€ë§ˆë‹¤ ë‘ ê°œì˜ ì¦ê°•ëœ ë·°ë¥¼ ë§Œë“¤ê³ , ì¸ì½”ë”ì™€ íˆ¬ì˜ í—¤ë“œë¥¼ í†µê³¼ì‹œì¼œ 2Nê°œì˜ $z$ ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  ë²¡í„° ìŒ ê°„ì˜ ìœ ì‚¬ë„($s_{i,j}$)ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì†ì‹¤($l(i,j)$)ì„ ê³„ì‚°í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ $f$ì™€ $g$ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\nìˆ˜ì‹ì— ëŒ€í•´ ì¢€ ë” ê¹Šì´ ë“¤ì–´ê°€ ë³´ê² ìŠµë‹ˆë‹¤. NT-Xent ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤. $$ l_{ij} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{I}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)} $$ $z_i$ì™€ $z_j$ëŠ” ê°ê° ì¦ê°•ëœ ì´ë¯¸ì§€ $\\tilde{x}_i$ì™€ $\\tilde{x}_j$ë¡œë¶€í„° ì¶”ì¶œëœ $l_2$ ì •ê·œí™”ëœ(normalized) í‘œí˜„ ë²¡í„°ì…ë‹ˆë‹¤. ì´ ë‘˜ì€ \u0026lsquo;ê¸ì •ì  ìŒ\u0026rsquo;ì…ë‹ˆë‹¤. $\\text{sim}(u, v) = u^T v / (||u|| ||v||)$ëŠ” ë‘ ë²¡í„° ê°„ì˜ **ì½”ì‚¬ì¸ ìœ ì‚¬ë„(cosine similarity)**ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. $\\tau$ (íƒ€ìš°)ëŠ” ì˜¨ë„(temperature) í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ìœ ì‚¬ë„ ë¶„í¬ë¥¼ ì–¼ë§ˆë‚˜ ë¾°ì¡±í•˜ê²Œ ë§Œë“¤ì§€ ì¡°ì ˆí•©ë‹ˆë‹¤. $\\tau$ê°€ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì€ ì–´ë ¤ìš´ ë¶€ì •ì  ì˜ˆì‹œ(hard negatives, ì¦‰ ê¸ì •ì  ì˜ˆì‹œì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¶€ì •ì  ì˜ˆì‹œ)ì— ë” ì§‘ì¤‘í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(gradient)ì—ì„œ ë¶€ì •ì  ì˜ˆì‹œë“¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ëŠ” íš¨ê³¼ë¥¼ ë‚³ìŠµë‹ˆë‹¤. ë¶„ì $\\exp(\\text{sim}(z_i, z_j)/\\tau)$ëŠ” ê¸ì •ì  ìŒ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦í­ì‹œí‚¨ ê°’ì…ë‹ˆë‹¤. ë¶„ëª¨ $\\sum_{k=1}^{2N} \\mathbb{I}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)$ëŠ” $z_i$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ìê¸° ìì‹ ì„ ì œì™¸í•œ ë°°ì¹˜ ë‚´ì˜ ëª¨ë“  $2N-1$ê°œì˜ ë‹¤ë¥¸ ë²¡í„°(ê¸ì •ì  ìŒì¸ $z_j$ 1ê°œì™€ ë¶€ì •ì  ìŒ $2N-2$ê°œ í¬í•¨)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦í­ì‹œì¼œ í•©í•œ ê°’ì…ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ì´ ìˆ˜ì‹ì€ Softmax í•¨ìˆ˜ì˜ í˜•íƒœë¥¼ ë ë©°, 2Nê°œì˜ ë²¡í„° ì¤‘ì—ì„œ $z_i$ì˜ ì§ì¸ $z_j$ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ì·¨í•˜ê³  ìŒìˆ˜ë¥¼ ë¶™ì—¬, ì •ë‹µ(ê¸ì •ì  ìŒ)ì˜ í™•ë¥ ì„ ìµœëŒ€í™”(ì†ì‹¤ì„ ìµœì†Œí™”)í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ Cross-Entropy ì†ì‹¤ì˜ ê¸°ë³¸ ì›ë¦¬ì…ë‹ˆë‹¤.\n3. ì‹¤í—˜ (Experiments) SimCLRì˜ ê° ì„¤ê³„ ìš”ì†Œê°€ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ê²€ì¦í•˜ê¸° ìœ„í•´ ë°©ëŒ€í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\në°ì´í„° ì¦ê°•ì˜ íš¨ê³¼ Figure 3ì€ **ë¬´ì‘ìœ„ ìë¥´ê¸°(random cropping)**ë§Œìœ¼ë¡œë„ ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ ëª¨ìŠµì„ ë³´ëŠ” \u0026lsquo;global view\u0026rsquo;ì™€ íŠ¹ì • ë¶€ë¶„ì„ ë³´ëŠ” \u0026rsquo;local view\u0026rsquo; (B â†’ A), ë˜ëŠ” ì¸ì ‘í•œ ë¶€ë¶„ì„ ë³´ëŠ” \u0026lsquo;adjacent view\u0026rsquo; (D â†’ C)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¤ì–‘í•œ ëŒ€ì¡° í•™ìŠµ ê³¼ì œë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nFigure 4ëŠ” ì‹¤í—˜ì— ì‚¬ìš©ëœ ë‹¤ì–‘í•œ ë°ì´í„° ì¦ê°• ê¸°ë²•ë“¤ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. (a) ì›ë³¸ ì´ë¯¸ì§€ë¶€í„° ì‹œì‘í•´ ìë¥´ê¸°, ìƒ‰ìƒ ì™œê³¡, íšŒì „, ë…¸ì´ì¦ˆ, ë¸”ëŸ¬ ë“± ë‹¤ì–‘í•œ ë³€í™˜ì„ ì ìš©í•œ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nFigure 5ëŠ” ì´ ì¦ê°• ê¸°ë²•ë“¤ì˜ ì¡°í•© íš¨ê³¼ë¥¼ ë¶„ì„í•œ íˆíŠ¸ë§µì…ë‹ˆë‹¤. ëŒ€ê°ì„ ì€ ë‹¨ì¼ ë³€í™˜ì˜ ì„±ëŠ¥, ë¹„ëŒ€ê°ì„ ì€ ë‘ ë³€í™˜ ì¡°í•©ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ í‘œì—ì„œ ì–´ë–¤ ë‹¨ì¼ ë³€í™˜ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í•˜ì§€ë§Œ, ë‘ ê°€ì§€ ë³€í™˜ì„ ì¡°í•©í–ˆì„ ë•Œ(íŠ¹íˆ \u0026lsquo;Crop\u0026rsquo;ê³¼ \u0026lsquo;Color\u0026rsquo;ì˜ ì¡°í•©) ì„±ëŠ¥ì´ ê·¹ì ìœ¼ë¡œ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nFigure 6ì€ ì™œ ìƒ‰ìƒ ì™œê³¡ì´ ì¤‘ìš”í•œì§€ ì„¤ëª…í•©ë‹ˆë‹¤. ìƒ‰ìƒ ì™œê³¡ì´ ì—†ìœ¼ë©´(a), ê°™ì€ ì´ë¯¸ì§€ì—ì„œ ì˜ë¼ë‚¸ ì¡°ê°ë“¤ì€ ë¹„ìŠ·í•œ ìƒ‰ìƒ ë¶„í¬(íˆìŠ¤í† ê·¸ë¨)ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ëª¨ë¸ì´ ì´ \u0026lsquo;ì‰¬ìš´ ê¸¸\u0026rsquo;ì„ íƒí•´ ìƒ‰ìƒë§Œìœ¼ë¡œ ì •ë‹µì„ ë§íˆëŠ” ê¼¼ìˆ˜ë¥¼ ë¶€ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒ‰ìƒ ì™œê³¡ì„ ì¶”ê°€í•˜ë©´(b), ì´ëŸ° ê¼¼ìˆ˜ê°€ ë¶ˆê°€ëŠ¥í•´ì ¸ ëª¨ë¸ì´ ë” ì¼ë°˜í™” ê°€ëŠ¥í•œ íŠ¹ì§•ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.\nTable 1ì€ ìƒ‰ìƒ ì¦ê°•ì˜ ê°•ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ìê¸° ì§€ë„ í•™ìŠµ(SimCLR)ì—ì„œëŠ” ìƒ‰ìƒ ì¦ê°•ì„ ê°•í•˜ê²Œ í• ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, ì§€ë„ í•™ìŠµì—ì„œëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ì €í•˜ë˜ê¸°ë„ í•©ë‹ˆë‹¤. ì´ëŠ” ìê¸° ì§€ë„ í•™ìŠµì´ ì§€ë„ í•™ìŠµë³´ë‹¤ ë” ê°•í•œ ë°ì´í„° ì¦ê°•ì„ í•„ìš”ë¡œ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\nì•„í‚¤í…ì²˜ì˜ ì˜í–¥ Figure 7ì€ ëª¨ë¸ì˜ ê¹Šì´ì™€ ë„ˆë¹„ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í¥ë¯¸ë¡œìš´ ì ì€ ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡, ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸(íŒŒë€ ì , ë¹¨ê°„ ë³„)ê³¼ ì§€ë„ í•™ìŠµ ëª¨ë¸(ë…¹ìƒ‰ ì‹­ìê°€) ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“ ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ìê¸° ì§€ë„ í•™ìŠµì´ í° ëª¨ë¸ë¡œë¶€í„° ë” ë§ì€ í˜œíƒì„ ë³¸ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\nFigure 8ì€ **íˆ¬ì˜ í—¤ë“œ $g(\\cdot)$**ì˜ êµ¬ì¡°ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ë¹„ì„ í˜• MLP í—¤ë“œ(\u0026lsquo;Non-linear\u0026rsquo;)ê°€ ì„ í˜• í—¤ë“œ(\u0026lsquo;Linear\u0026rsquo;)ë‚˜ í—¤ë“œê°€ ì—†ëŠ” ê²½ìš°(\u0026lsquo;None\u0026rsquo;)ë³´ë‹¤ ì›”ë“±íˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n$h$ì™€ $g(h)$ê°€ ì–´ë–¤ ì¦ê°•(ìƒ‰ìƒ, íšŒì „ ë“±)ì´ ì ìš©ë˜ì—ˆëŠ”ì§€ ì˜ˆì¸¡í•˜ëŠ” ì‹¤í—˜ì„ í–ˆì„ ë•Œ, $h$ê°€ í›¨ì”¬ ë” ë§ì€ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì¦‰, $g(h)$ëŠ” ëŒ€ì¡° í•™ìŠµ ê³¼ì œì— ë¶ˆí•„ìš”í•œ ì •ë³´(ì˜ˆ: ìƒ‰ìƒ, ë°©í–¥)ë¥¼ ì œê±°í•˜ëŠ” ì—­í• ì„ í•˜ë©°, ì´ë¡œ ì¸í•´ ê·¸ ì´ì „ ë‹¨ê³„ì¸ $h$ì—ëŠ” ë” í’ë¶€í•œ ì •ë³´ê°€ ë³´ì¡´ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì†ì‹¤ í•¨ìˆ˜ì™€ ë°°ì¹˜ í¬ê¸° Table 2ëŠ” NT-Xent ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë‹¤ë¥¸ ëŒ€ì¡° ì†ì‹¤ í•¨ìˆ˜ë“¤(NT-Logistic, Margin Triplet)ê³¼ ìˆ˜ì‹ ë° ê¸°ìš¸ê¸° ì¸¡ë©´ì—ì„œ ë¹„êµí•©ë‹ˆë‹¤. NT-XentëŠ” ì˜¨ë„ $\\tau$ì™€ $l_2$ ì •ê·œí™”ë¥¼ í†µí•´ ì–´ë ¤ìš´ ë¶€ì •ì  ì˜ˆì‹œì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ëŠ” ë°˜ë©´, ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë“¤ì€ ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.\nTable 4ëŠ” ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ë¡œ, NT-Xentê°€ ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë“¤ë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸ì‹œì¼œ ì¤ë‹ˆë‹¤.\nTable 5ëŠ” $l_2$ ì •ê·œí™”ì™€ ì˜¨ë„ $\\tau$ì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜, ì ì ˆí•œ ì˜¨ë„ë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë©ë‹ˆë‹¤.\nFigure 9ëŠ” ë°°ì¹˜ í¬ê¸°ì™€ í•™ìŠµ ì—í¬í¬(epoch) ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í•™ìŠµ ì´ˆê¸°ì—ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì§€ë§Œ, í•™ìŠµì„ ì˜¤ë˜ ì§„í–‰í• ìˆ˜ë¡ ê·¸ ì°¨ì´ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤. ëŒ€ì¡° í•™ìŠµì—ì„œëŠ” í° ë°°ì¹˜ê°€ ë” ë§ì€ ë¶€ì •ì  ì˜ˆì‹œë¥¼ ì œê³µí•˜ë¯€ë¡œ ìˆ˜ë ´ì„ ë•ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.\nìµœì‹  ê¸°ìˆ ê³¼ì˜ ë¹„êµ (SOTA) Table 6ì€ ImageNet ë°ì´í„°ì…‹ì—ì„œì˜ ì„ í˜• í‰ê°€(linear evaluation) ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ìê¸° ì§€ë„ í•™ìŠµ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. SimCLRì€ ResNet-50 (4x) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 76.5%ì˜ Top-1 ì •í™•ë„ë¥¼ ë‹¬ì„±, ì´ì „ ìµœê³  ì„±ëŠ¥ì„ í¬ê²Œ ë›°ì–´ë„˜ì—ˆìœ¼ë©° ì§€ë„ í•™ìŠµ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ë™ë“±í•œ ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\nTable 7ì€ ë ˆì´ë¸”ì´ ì ì€ ë°ì´í„°(1% ë˜ëŠ” 10%)ë¡œ ëª¨ë¸ì„ **ë¯¸ì„¸ ì¡°ì •(fine-tuning)**í•˜ëŠ” ì¤€ì§€ë„ í•™ìŠµ(semi-supervised learning) ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œë„ SimCLRì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ í° ì°¨ì´ë¡œ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.\nTable 8ì€ ë‹¤ë¥¸ 12ê°œì˜ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œì˜ ì „ì´ í•™ìŠµ(transfer learning) ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SimCLRë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ë§ì€ ë°ì´í„°ì…‹ì—ì„œ ì§€ë„ í•™ìŠµìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ ëŒ€ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n4. ê²°ë¡  (Conclusion) ì´ ë…¼ë¬¸ì€ SimCLRì´ë¼ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ ëŒ€ì¡° í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. SimCLRì˜ ì„±ê³µì€ ì–´ëŠ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ë°œê²¬ì´ ì•„ë‹Œ, ê¸°ì¡´ì— ì•Œë ¤ì§„ ìš”ì†Œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬í•˜ê³  ìµœì ìœ¼ë¡œ ì¡°í•©í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\ní•µì‹¬ì ì¸ ë°œê²¬ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\në°ì´í„° ì¦ê°• ê¸°ë²•ì˜ ê°•ë ¥í•œ ì¡°í•©(íŠ¹íˆ ë¬´ì‘ìœ„ ìë¥´ê¸°ì™€ ìƒ‰ìƒ ì™œê³¡)ì€ íš¨ê³¼ì ì¸ í‘œí˜„ í•™ìŠµì— í•„ìˆ˜ì ì´ë‹¤. ì¸ì½”ë” ë’¤ì— ë¹„ì„ í˜• íˆ¬ì˜ í—¤ë“œë¥¼ ì¶”ê°€í•˜ê³ , í—¤ë“œë¥¼ í†µê³¼í•˜ê¸° ì „ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤. **ì •ê·œí™”ëœ Cross Entropy ì†ì‹¤ í•¨ìˆ˜(NT-Xent)**ì™€ ì ì ˆí•œ ì˜¨ë„ íŒŒë¼ë¯¸í„°, ê·¸ë¦¬ê³  ë§¤ìš° í° ë°°ì¹˜ í¬ê¸°ì—ì„œì˜ í•™ìŠµì´ ë§¤ìš° íš¨ê³¼ì ì´ë‹¤. SimCLRì€ ë³µì¡í•œ êµ¬ì¡°ë‚˜ ë©”ëª¨ë¦¬ ë±…í¬ ì—†ì´ë„ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ìœ¼ë¡œì¨, ìê¸° ì§€ë„ í•™ìŠµì˜ ì ì¬ë ¥ì´ ì—¬ì „íˆ ê³¼ì†Œí‰ê°€ë˜ê³  ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” ì´í›„ì˜ ìˆ˜ë§ì€ ìê¸° ì§€ë„ í•™ìŠµ ì—°êµ¬ì— í° ì˜ê°ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.\n","permalink":"https://mookjsi.github.io/posts/paper-review-simclr/","summary":"ICML 2020ì—ì„œ ë°œí‘œëœ \u0026lsquo;A Simple Framework for Contrastive Learning of Visual Representations\u0026rsquo; ë…¼ë¬¸ì— ëŒ€í•œ ì‹¬ì¸µ ë¦¬ë·°ì…ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„°ë¡œë¶€í„° ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ìê¸° ì§€ë„ í•™ìŠµ(Self-Supervised Learning) ë°©ë²•ë¡ ì¸ SimCLRì˜ í•µì‹¬ ì•„ì´ë””ì–´, í”„ë ˆì„ì›Œí¬ êµ¬ì„± ìš”ì†Œ, ê·¸ë¦¬ê³  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•©ë‹ˆë‹¤.","title":"SimCLR ë…¼ë¬¸ ë¦¬ë·°"},{"content":"ì´ ë…¼ë¬¸ì€ ì´ë¯¸ì§€ ë¶„í• (Image Segmentation) ë¶„ì•¼ì— \u0026lsquo;íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(Foundation Model)\u0026rsquo; ì´ë¼ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ëŠ” \u0026ldquo;Segment Anything (SA)\u0026ldquo;ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„í• ì´ë€, ì´ë¯¸ì§€ ì† ê° í”½ì…€ì´ ì–´ë–¤ ê°ì²´ì— ì†í•˜ëŠ”ì§€ êµ¬ë¶„í•˜ì—¬ ì˜ì—­ì„ ë‚˜ëˆ„ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ ì‚¬ì§„ì—ì„œ í”½ì…€ ë‹¨ìœ„ë¡œ \u0026lsquo;ì—¬ê¸°ëŠ” ê³ ì–‘ì´\u0026rsquo;, \u0026lsquo;ì—¬ê¸°ëŠ” ë°°ê²½\u0026rsquo;ì´ë¼ê³  ì•Œë ¤ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.\nìµœê·¼ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œëŠ” GPTì™€ ê°™ì´ ì›¹ ê·œëª¨ì˜ ë°©ëŒ€í•œ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ **\u0026lsquo;íŒŒìš´ë°ì´ì…˜ ëª¨ë¸\u0026rsquo;**ì´ ë“±ì¥í•˜ì—¬, íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ì¶”ê°€ í›ˆë ¨(fine-tuning) ì—†ì´ë„ \u0026lsquo;í”„ë¡¬í”„íŠ¸(prompt)\u0026lsquo;ë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ì—…ì„ ë†€ëë„ë¡ ì˜ ìˆ˜í–‰í•´ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ì´ëŸ¬í•œ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„, ì´ë¯¸ì§€ ë¶„í•  ë¶„ì•¼ì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ë²”ìš© ëª¨ë¸ì„ ë§Œë“¤ê³ ì í–ˆìŠµë‹ˆë‹¤.\nì´ë¥¼ ìœ„í•´ ì €ìë“¤ì€ ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ ìš”ì†Œë¥¼ ìƒˆë¡­ê²Œ ì •ì˜í•˜ê³  ê°œë°œí–ˆìŠµë‹ˆë‹¤.\nìƒˆë¡œìš´ ê³¼ì—…(Task): í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í•  (Promptable Segmentation) ìƒˆë¡œìš´ ëª¨ë¸(Model): Segment Anything Model (SAM) ìƒˆë¡œìš´ ë°ì´í„°ì…‹(Data): Segment Anything 1-Billion (SA-1B) ì´ ì„¸ ê°€ì§€ëŠ” ì„œë¡œ ë§ë¬¼ë ¤ ëŒì•„ê°€ëŠ” í†±ë‹ˆë°”í€´ì²˜ëŸ¼ ì‘ìš©í•©ë‹ˆë‹¤. ì¢‹ì€ ëª¨ë¸(SAM)ì„ ë§Œë“¤ê¸° ìœ„í•´ ë°©ëŒ€í•œ ë°ì´í„°(SA-1B)ê°€ í•„ìš”í•˜ê³ , ì´ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ì¢‹ì€ ëª¨ë¸(SAM)ì„ \u0026lsquo;ë°ì´í„° ì—”ì§„\u0026rsquo;ìœ¼ë¡œ í™œìš©í–ˆìœ¼ë©°, ì´ ëª¨ë“  ê²ƒì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í• ì…ë‹ˆë‹¤.\n(a) ê³¼ì—…: ì´ ê·¸ë¦¼ì€ \u0026lsquo;í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í• \u0026rsquo;ì´ë¼ëŠ” ìƒˆë¡œìš´ taskë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì´ë¯¸ì§€ ìœ„ì— ì ì„ ì°ê±°ë‚˜(points), ë„¤ëª¨ ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ê±°ë‚˜(box), ì‹¬ì§€ì–´ \u0026ldquo;ê²€ì€ ê·€ë¥¼ ê°€ì§„ ê³ ì–‘ì´\u0026quot;ì²˜ëŸ¼ í…ìŠ¤íŠ¸(text)ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì£¼ë©´, ëª¨ë¸ì´ ê·¸ì— í•´ë‹¹í•˜ëŠ” ê°ì²´ì˜ ë§ˆìŠ¤í¬(mask)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n(b) ëª¨ë¸: ì´ê²ƒì´ ë°”ë¡œ SAM(Segment Anything Model)ì˜ êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ë¬´ê±°ìš´ \u0026lsquo;ì´ë¯¸ì§€ ì¸ì½”ë”\u0026rsquo;, ì‚¬ìš©ìì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì´í•´í•˜ëŠ” \u0026lsquo;í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”\u0026rsquo;, ê·¸ë¦¬ê³  ì´ ë‘˜ì„ í•©ì³ ìµœì¢… ë§ˆìŠ¤í¬ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“¤ì–´ë‚´ëŠ” ê°€ë²¼ìš´ \u0026lsquo;ë§ˆìŠ¤í¬ ë””ì½”ë”\u0026rsquo;ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n(c) ë°ì´í„°: ê±°ëŒ€í•œ SA-1B ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ \u0026lsquo;ë°ì´í„° ì—”ì§„\u0026rsquo;ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë¸ì„ ì´ìš©í•´ ë°ì´í„° ì£¼ì„ ì‘ì—…ì„ ë•ê³ (annotate), ê·¸ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ”(train) ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ 11ì–µ ê°œê°€ ë„˜ëŠ” ë§ˆìŠ¤í¬ ë°ì´í„°ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.\nê¸°ìˆ  ì„¤ëª… 1. ê³¼ì—…: í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í•  (Promptable Segmentation) ê¸°ì¡´ì˜ ë¶„í•  ëª¨ë¸ë“¤ì€ \u0026lsquo;ê³ ì–‘ì´ë§Œ ì°¾ì•„ë‚´ë¼\u0026rsquo; ë˜ëŠ” \u0026lsquo;ì‚¬ëŒê³¼ ìë™ì°¨ë§Œ êµ¬ë¶„í•´ë¼\u0026rsquo;ì™€ ê°™ì´ ì •í•´ì§„ ì¢…ë¥˜ì˜ ê°ì²´ë§Œ ë¶„í• í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í• ì€ \u0026ldquo;ì–´ë–¤ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì§€ë“  ê·¸ì— í•´ë‹¹í•˜ëŠ” ìœ íš¨í•œ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ë°˜í™˜\u0026quot;í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\nì—¬ê¸°ì„œ \u0026lsquo;ìœ íš¨í•œ(valid)\u0026rsquo; ë§ˆìŠ¤í¬ë¼ëŠ” ì ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŒŒë€ ì…”ì¸ ë¥¼ ì…ì€ ì‚¬ëŒì˜ ì‚¬ì§„ì—ì„œ ì…”ì¸  ë¶€ë¶„ì— ì  í•˜ë‚˜ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì°ì—ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤. ì´ ì ì€ \u0026lsquo;íŒŒë€ ì…”ì¸ \u0026rsquo;ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆê³ , \u0026lsquo;ì…”ì¸ ë¥¼ ì…ê³  ìˆëŠ” ì‚¬ëŒ ì „ì²´\u0026rsquo;ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•  ê²½ìš°, ëª¨ë¸ì€ ì…”ì¸  ë§ˆìŠ¤í¬ì™€ ì‚¬ëŒ ë§ˆìŠ¤í¬ ë‘˜ ì¤‘ í•˜ë‚˜ ì´ìƒì˜ í•©ë¦¬ì ì¸ ë§ˆìŠ¤í¬ë¥¼ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\nì´ ê·¸ë¦¼ì€ SAMì´ ì–´ë–»ê²Œ ëª¨í˜¸ì„±ì„ ì²˜ë¦¬í•˜ëŠ”ì§€ ì˜ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì—´ì€ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ˆë¡ìƒ‰ ì (green circle)ì´ë¼ëŠ” ë‹¨ í•˜ë‚˜ì˜ ëª¨í˜¸í•œ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, SAMì€ ì„¸ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ìœ íš¨í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì²« ë²ˆì§¸ ì—´ì˜ íƒ€ì¡° ì´ë¯¸ì§€ì—ì„œ ëª© ë¶€ë¶„ì— ì ì„ ì°ì, SAMì€ íƒ€ì¡°ì˜ ë¨¸ë¦¬, ëª©, ê·¸ë¦¬ê³  ëª¸ ì „ì²´ì— í•´ë‹¹í•˜ëŠ” ì„¸ ê°€ì§€ ë§ˆìŠ¤í¬ë¥¼ ëª¨ë‘ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ìì‹ ì´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n2. ëª¨ë¸: SAM (Segment Anything Model) SAMì€ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í•  í…ŒìŠ¤í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë§¤ìš° íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\nì´ë¯¸ì§€ ì¸ì½”ë” (Image Encoder): ë¬´ê±°ìš´ Vision Transformer(ViT) ê¸°ë°˜ì˜ ì¸ì½”ë”ë¡œ, ì…ë ¥ëœ ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•œ ë²ˆë§Œ ì²˜ë¦¬í•˜ì—¬ ê³ ì°¨ì›ì˜ \u0026lsquo;ì´ë¯¸ì§€ ì„ë² ë”©(image embedding)\u0026lsquo;ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê³„ì‚°ëŸ‰ì´ ë§ì§€ë§Œ, ì´ë¯¸ì§€ë‹¹ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë©´ ë˜ë¯€ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ ë¹„ìš©ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\ní”„ë¡¬í”„íŠ¸ ì¸ì½”ë” (Prompt Encoder): ì , ìƒì, í…ìŠ¤íŠ¸, ë§ˆìŠ¤í¬ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ ê²°í•©í•  ìˆ˜ ìˆëŠ” ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì ì´ë‚˜ ìƒìëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ì€ ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ê³ , í…ìŠ¤íŠ¸ëŠ” CLIPê³¼ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì˜ ì¸ì½”ë”ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\në§ˆìŠ¤í¬ ë””ì½”ë” (Mask Decoder): ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ì…ë ¥ë°›ì•„ ìµœì¢… ë§ˆìŠ¤í¬ë¥¼ ì¶œë ¥í•˜ëŠ” ê°€ë³ê³  ë¹ ë¥¸ ëª¨ë“ˆì…ë‹ˆë‹¤. ì´ ë””ì½”ë”ëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œë„ ì•½ 50ms ë§Œì— ì‘ë™í•  ì •ë„ë¡œ ë§¤ìš° íš¨ìœ¨ì ì´ì–´ì„œ, ì‚¬ìš©ìê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”ê¾¸ëŠ” ëŒ€ë¡œ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ìƒí˜¸ì‘ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨í˜¸í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ 3ê°œì˜ ë§ˆìŠ¤í¬ì™€ ê° ë§ˆìŠ¤í¬ì˜ ì‹ ë¢°ë„ ì ìˆ˜(ì˜ˆìƒ IoU)ë¥¼ í•¨ê»˜ ì¶œë ¥í•©ë‹ˆë‹¤.\nëª¨ë¸ í•™ìŠµì—ëŠ” Focal Lossì™€ Dice Lossë¥¼ ê²°í•©í•œ ì†ì‹¤ í•¨ìˆ˜ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ì†ì‹¤ í•¨ìˆ˜ëŠ” $L = \\alpha L_{focal} + \\beta L_{dice}$ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nFocal Loss ($L_{focal}$): ì£¼ë¡œ ê°ì²´ íƒì§€ì—ì„œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ Cross-Entropy ì†ì‹¤ í•¨ìˆ˜ì— $(1-p_t)^\\gamma$ í•­ì„ ì¶”ê°€í•˜ì—¬, ëª¨ë¸ì´ ì´ë¯¸ ì˜ ë§ì¶”ëŠ” ì‰¬ìš´ ìƒ˜í”Œ(ì˜ˆ: ë°°ê²½ í”½ì…€)ì—ëŠ” ì ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê³ , ë§ì¶”ê¸° ì–´ë ¤ìš´ ì–´ë ¤ìš´ ìƒ˜í”Œ(ì˜ˆ: ê°ì²´ì˜ ê²½ê³„ í”½ì…€)ì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê²½ê³„ê°€ ëª…í™•í•œ ë§ˆìŠ¤í¬ë¥¼ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\nDice Loss ($L_{dice}$): ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì™€ ì‹¤ì œ ë§ˆìŠ¤í¬ ê°„ì˜ ê²¹ì¹˜ëŠ” ì˜ì—­(Intersection over Union, IoU)ì„ ì§ì ‘ì ìœ¼ë¡œ ìµœëŒ€í™”í•˜ë„ë¡ ì„¤ê³„ëœ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. Dice ê³„ìˆ˜ $D = \\frac{2|A \\cap B|}{|A| + |B|}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì†ì‹¤ í•¨ìˆ˜ëŠ” $L_{dice} = 1 - D$ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” ë¶„í•  ì˜ì—­ì˜ í¬ê¸°ì— ëœ ë¯¼ê°í•˜ê³ , ì‹¬í•œ ë¶ˆê· í˜• ìƒí™©ì—ì„œë„ ì•ˆì •ì ì¸ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.\nì´ ë‘ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê²°í•©í•¨ìœ¼ë¡œì¨, í”½ì…€ ë‹¨ìœ„ì˜ ì •í™•ì„±(Focal Loss)ê³¼ ì˜ì—­ ë‹¨ìœ„ì˜ ìœ ì‚¬ì„±(Dice Loss)ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ê³ í’ˆì§ˆì˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ë„ë¡ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.\n3. ë°ì´í„° ì—”ì§„ ë° SA-1B ë°ì´í„°ì…‹ ê³ í’ˆì§ˆì˜ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ë°©ëŒ€í•˜ê³  ë‹¤ì–‘í•œ ë°ì´í„°ê°€ í•„ìˆ˜ì ì´ì§€ë§Œ, ì´ë¯¸ì§€ ë¶„í•  ë¶„ì•¼ì—ëŠ” ì›¹ì—ì„œ ë°”ë¡œ ìˆ˜ì§‘í•  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìë“¤ì€ \u0026lsquo;ë°ì´í„° ì—”ì§„(Data Engine)\u0026rsquo; ì´ë¼ëŠ” ë…ì°½ì ì¸ ë°©ë²•ì„ ê³ ì•ˆí–ˆìŠµë‹ˆë‹¤.\në°ì´í„° ì—”ì§„ì€ 3ë‹¨ê³„ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\në³´ì¡°-ìˆ˜ë™ ë‹¨ê³„ (Assisted-manual): ì´ˆê¸° SAM ëª¨ë¸ì„ ì´ìš©í•´ ì „ë¬¸ ì‘ì—…ìê°€ ëŒ€í™”í˜•ìœ¼ë¡œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ëŒ€ëµì ì¸ ë§ˆìŠ¤í¬ë¥¼ ì œì•ˆí•˜ë©´, ì‘ì—…ìëŠ” í´ë¦­ ëª‡ ë²ˆìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤. ë°˜ìë™ ë‹¨ê³„ (Semi-automatic): ëª¨ë¸ì´ ë°œì „í•¨ì— ë”°ë¼, ì´ì œ ëª¨ë¸ì´ ì´ë¯¸ì§€ì—ì„œ í™•ì‹¤í•œ ê°ì²´ë“¤ì„ ë¨¼ì € ìë™ìœ¼ë¡œ ì°¾ì•„ëƒ…ë‹ˆë‹¤. ì‘ì—…ìëŠ” ëª¨ë¸ì´ ë†“ì¹œ, ë” ì‘ê±°ë‚˜ ê¹Œë‹¤ë¡œìš´ ê°ì²´ë“¤ì— ì§‘ì¤‘í•˜ì—¬ ë§ˆìŠ¤í¬ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì™„ì „ ìë™ ë‹¨ê³„ (Fully automatic): ìµœì¢… ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì´ ì¶©ë¶„íˆ ë˜‘ë˜‘í•´ì ¸ì„œ ì‚¬ëŒì˜ ê°œì… ì—†ì´ë„ ê³ í’ˆì§ˆì˜ ë§ˆìŠ¤í¬ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì €ìë“¤ì€ ì´ë¯¸ì§€ì— $32 \\times 32$ ê²©ì í˜•íƒœì˜ ì  í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì…ë ¥í•˜ê³ , ëª¨í˜¸ì„± ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‹¹ í‰ê·  100ì—¬ ê°œì˜ ë§ˆìŠ¤í¬ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ì œë¶€í„° ê° ë°ì´í„° ì—”ì§„ ë‹¨ê³„ë³„ë¡œ ì‹¤ì œ ì˜ˆì‹œ ìƒí™©ê³¼ ê³¼ì •ì„ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n1. ë³´ì¡°-ìˆ˜ë™ ë‹¨ê³„ (Assisted-manual) ì´ ë‹¨ê³„ëŠ” ë˜‘ë˜‘í•œ ë³´ì¡° ë„êµ¬ë¥¼ í™œìš©í•´ ì‚¬ëŒì´ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\nì˜ˆì‹œ ìƒí™©: ì „ë¬¸ ì‘ì—…ìì—ê²Œ ê³µì›ì— ìˆëŠ” ê°•ì•„ì§€ ì‚¬ì§„ í•œ ì¥ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ì‘ì—… ë°©ì‹: ì‘ì—…ìëŠ” ìˆ˜ì‘ì—…ìœ¼ë¡œ ê°•ì•„ì§€ ì™¸ê³½ì„ ì„ ê·¸ë¦¬ëŠ” ëŒ€ì‹ , ê°•ì•„ì§€ ëª¸í†µ ì¤‘ì•™ì— ì (point) í•˜ë‚˜ë¥¼ í´ë¦­í•©ë‹ˆë‹¤. ì´ˆê¸° SAM ëª¨ë¸ì´ ì¦‰ì‹œ ê·¸ ì ì„ ê¸°ë°˜ìœ¼ë¡œ \u0026ldquo;ì´ê²ƒì´ ê°•ì•„ì§€ì¸ ê²ƒ ê°™ë‹¤\u0026quot;ê³  ì¶”ì¸¡í•˜ë©° ëŒ€ëµì ì¸ ê°•ì•„ì§€ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•´ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë§ˆìŠ¤í¬ëŠ” ê¼¬ë¦¬ê°€ ì˜ë ¤ìˆê³ , ë°°ê²½ì˜ í’€ ì¼ë¶€ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ì—…ìëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•´ ì˜ë¦° ê¼¬ë¦¬ ë¶€ë¶„ì— ì ì„ í•˜ë‚˜ ë” í´ë¦­í•˜ê³ (ì—¬ê¸°ë„ í¬í•¨í•´ì¤˜!), ì˜ëª» í¬í•¨ëœ í’€ ë¶€ë¶„ì— ì ì„ í´ë¦­í•©ë‹ˆë‹¤(ì—¬ê¸°ëŠ” ë¹¼ì¤˜!). í´ë¦­í•  ë•Œë§ˆë‹¤ SAMì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë§ˆìŠ¤í¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , ëª‡ ë²ˆì˜ í´ë¦­ë§Œìœ¼ë¡œ í”½ì…€ ìˆ˜ì¤€ê¹Œì§€ ì •í™•í•œ ê°•ì•„ì§€ ë§ˆìŠ¤í¬ê°€ ì™„ì„±ë©ë‹ˆë‹¤. ê²°ê³¼: ì´ ë°©ì‹ì„ í†µí•´ ì²˜ìŒë¶€í„° ì†ìœ¼ë¡œ ê·¸ë¦¬ëŠ” ê²ƒë³´ë‹¤ 6.5ë°° ë¹ ë¥´ê²Œ ê³ í’ˆì§ˆ ë§ˆìŠ¤í¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ SAM ëª¨ë¸ì„ ë°˜ë³µì ìœ¼ë¡œ ì¬í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ ì ì°¨ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. 2. ë°˜ìë™ ë‹¨ê³„ (Semi-automatic) 1ë‹¨ê³„ë¥¼ ê±°ì³ ë˜‘ë˜‘í•´ì§„ ëª¨ë¸ì´ ë¨¼ì € ì‰¬ìš´ ì‘ì—…ì„ ì²˜ë¦¬í•˜ê³ , ì‚¬ëŒì€ ë” ì–´ë ¤ìš´ ì‘ì—…ì— ì§‘ì¤‘í•˜ëŠ” í˜‘ì—… ë‹¨ê³„ì…ë‹ˆë‹¤.\nì˜ˆì‹œ ìƒí™©: ì‘ì—…ìì—ê²Œ ë‹¤ì–‘í•œ ë¬¼ê±´ì´ ìˆëŠ” ì±…ìƒ ì‚¬ì§„ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ì‘ì—… ë°©ì‹: ì‘ì—…ìê°€ ì´ë¯¸ì§€ë¥¼ ì—´ë©´, 1ë‹¨ê³„ ë°ì´í„°ë¡œ í•™ìŠµë˜ì–´ ë” ê°•ë ¥í•´ì§„ SAMì´ ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ë¶„ì„í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ìì‹ ì´ 90% ì´ìƒ í™•ì‹ í•˜ëŠ” ëª…í™•í•œ ê°ì²´(ì˜ˆ: ëª¨ë‹ˆí„°, í‚¤ë³´ë“œ, ë§ˆìš°ìŠ¤)ë“¤ì˜ ë§ˆìŠ¤í¬ë¥¼ ë¯¸ë¦¬ ìë™ìœ¼ë¡œ ìƒì„±í•´ ë†“ìŠµë‹ˆë‹¤. ğŸ¤– ì‘ì—…ìëŠ” ì´ë¯¸ ìƒì„±ëœ ë§ˆìŠ¤í¬ë“¤ì„ ê²€í† í•˜ê³ , ëª¨ë¸ì´ ë†“ì¹œ ë” ì‘ê±°ë‚˜ ë³µì¡í•˜ê³  ì• ë§¤í•œ ê°ì²´(ì˜ˆ: ì—‰ì¼œìˆëŠ” ì¼€ì´ë¸”, íœ, ì±…ìƒ ë’¤í¸ì˜ ì‘ì€ í™”ë¶„)ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ë§ˆìŠ¤í¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ê²°ê³¼: ì´ ë‹¨ê³„ì˜ ëª©í‘œëŠ” ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì‰¬ìš´ ê²ƒë§Œ í•™ìŠµí•˜ì§€ ì•Šë„ë¡, ì‚¬ëŒì´ ê¹Œë‹¤ë¡œìš´ ê°ì²´ ë°ì´í„°ë¥¼ ë³´ì¶©í•´ ì¤Œìœ¼ë¡œì¨ ëª¨ë¸ì´ \u0026ldquo;ì–´ë–¤ ê²ƒì´ë“ (Anything)\u0026rdquo; ë¶„í• í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ í‚¤ìš°ê²Œ ë©ë‹ˆë‹¤. 3. ì™„ì „ ìë™ ë‹¨ê³„ (Fully automatic) ìµœì¢… ë‹¨ê³„ì—ì„œëŠ” ì‚¬ëŒì˜ ê°œì… ì—†ì´, ê°€ì¥ ê°•ë ¥í•´ì§„ SAM ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nì˜ˆì‹œ ìƒí™©: ëª¨ë¸ì—ê²Œ ìˆ˜ë§ì€ ê³¼ì¼ì´ ì§„ì—´ëœ ì‹œì¥ ì‚¬ì§„ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ì‘ì—… ë°©ì‹: ì‹œìŠ¤í…œì´ ì´ë¯¸ì§€ ì „ì²´ì— 32x32 ê°„ê²©ì˜ ì´˜ì´˜í•œ ê²©ì(grid) ì  í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì…ë ¥í•©ë‹ˆë‹¤. SAMì€ ê° ì ì— ëŒ€í•´ ëª¨í˜¸ì„± ì²˜ë¦¬ ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í¬ë„ì†¡ì´ì˜ í¬ë„ì•Œ í•˜ë‚˜ì— ì ì´ ì°í˜”ë‹¤ë©´, ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ ê°œì˜ ìœ íš¨í•œ ë§ˆìŠ¤í¬ë¥¼ ë™ì‹œì— ìƒì„±í•©ë‹ˆë‹¤. ë§ˆìŠ¤í¬ 1: í¬ë„ì•Œ í•˜ë‚˜ ë§ˆìŠ¤í¬ 2: í¬ë„ì•Œì´ ì†í•œ í¬ë„ì†¡ì´ ì „ì²´ ë§ˆìŠ¤í¬ 3: í¬ë„ì†¡ì´ê°€ ë†“ì¸ ê³¼ì¼ ìƒì ì „ì²´ ì´ë ‡ê²Œ ìƒì„±ëœ ìˆ˜ë§ì€ ë§ˆìŠ¤í¬ë“¤ ì¤‘ì—ì„œ, ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ì˜ˆì¸¡í•œ ì‹ ë¢°ë„ ì ìˆ˜(IoU)ê°€ ë†’ê³  ì•ˆì •ì ì¸ ë§ˆìŠ¤í¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” í•„í„°ë§í•©ë‹ˆë‹¤. ê²°ê³¼: ì´ ìë™í™”ëœ ê³¼ì •ì„ í†µí•´ ì´ë¯¸ì§€ í•œ ì¥ë‹¹ í‰ê·  100ì—¬ ê°œì˜ ê³ í’ˆì§ˆ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ëŒì˜ ë„ì›€ ì—†ì´ ëŒ€ëŸ‰ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆì—ˆê³ , ìµœì¢…ì ìœ¼ë¡œ 11ì–µ ê°œ ì´ìƒì˜ ë§ˆìŠ¤í¬ë¡œ êµ¬ì„±ëœ SA-1B ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ íƒ„ìƒí•œ ê²ƒì´ ë°”ë¡œ SA-1B ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 1100ë§Œ ê°œì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ì´ 11ì–µ ê°œê°€ ë„˜ëŠ” ë¶„í•  ë§ˆìŠ¤í¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ê¸°ì¡´ì— ê°€ì¥ ì»¸ë˜ ë¶„í•  ë°ì´í„°ì…‹ë³´ë‹¤ ë§ˆìŠ¤í¬ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ 400ë°° ì´ìƒ ë§ì€ ê·œëª¨ì…ë‹ˆë‹¤.\nì´ ê·¸ë¦¼ì€ SA-1B ë°ì´í„°ì…‹ì— í¬í•¨ëœ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ì™€ ê·¸ ìœ„ì— ì˜¤ë²„ë ˆì´ëœ ë§ˆìŠ¤í¬ë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìœ„ìª½ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì´ë¯¸ì§€ ë‹¹ ë§ˆìŠ¤í¬ ìˆ˜ê°€ ì ê³ (\u0026lt;50ê°œ), ì•„ë˜ë¡œ ê°ˆìˆ˜ë¡ ë§ˆìŠ¤í¬ ìˆ˜ê°€ ë§ì€(\u0026gt;500ê°œ) ì´ë¯¸ì§€ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì´ ë‹¨ìˆœí•œ ì´ë¯¸ì§€ë¶€í„° ë§¤ìš° ë³µì¡í•˜ê³  ê°ì²´ê°€ ë§ì€ ì´ë¯¸ì§€ê¹Œì§€ í­ë„“ê²Œ í¬í•¨í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nFigure 6ì€ SA-1B ë°ì´í„°ì…‹ì˜ ë§ˆìŠ¤í¬ íŠ¹ì„±ì„ ë‹¤ë¥¸ ì£¼ìš” ë°ì´í„°ì…‹(LVIS, COCO ë“±)ê³¼ ë¹„êµí•©ë‹ˆë‹¤. SA-1BëŠ” ì´ë¯¸ì§€ë‹¹ ë§ˆìŠ¤í¬ ìˆ˜ê°€ ì••ë„ì ìœ¼ë¡œ ë§ê³ (ì™¼ìª½ ê·¸ë˜í”„), ì´ë¡œ ì¸í•´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ê±°ë‚˜ ì¤‘ê°„ í¬ê¸°ì˜ ë§ˆìŠ¤í¬ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤(ì¤‘ê°„ ê·¸ë˜í”„).\nFigure 7ì€ ë°ì´í„°ì…‹ì— í¬í•¨ëœ ì´ë¯¸ì§€ë“¤ì˜ ì¶”ì •ëœ ì§€ë¦¬ì  ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ì…‹ë“¤ì´ ë¶ë¯¸ë‚˜ íŠ¹ì • ì§€ì—­ì— í¸ì¤‘ëœ ê²½í–¥ì´ ìˆì—ˆë˜ ë°˜ë©´, SA-1BëŠ” ì•„ì‹œì•„, ìœ ëŸ½ ë“± í›¨ì”¬ ë” ë‹¤ì–‘í•œ êµ­ê°€ì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì—¬ ì§€ë¦¬ì  ë‹¤ì–‘ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.\nì‹¤í—˜ (Experiments) ì €ìë“¤ì€ SAMì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ì¢…ë¥˜ì˜ ì´ë¯¸ì§€ë‚˜ ì‘ì—…ì— ëŒ€í•´ì„œë„ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€, ì¦‰ ì œë¡œìƒ· ì „ì´(Zero-shot Transfer) ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n1. ë‹¨ì¼ ì  ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„± í‰ê°€ ê°€ì¥ í•µì‹¬ì ì¸ ì‹¤í—˜ìœ¼ë¡œ, ì´ë¯¸ì§€ì— ë‹¨ í•˜ë‚˜ì˜ ì ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì—ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ìœ íš¨í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ”ì§€ í‰ê°€í–ˆìŠµë‹ˆë‹¤.\n(a) ìë™ í‰ê°€(mIoU): 23ê°œì˜ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ìµœê°•ì˜ ëŒ€í™”í˜• ë¶„í•  ëª¨ë¸ì¸ RITMê³¼ ì„±ëŠ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤. SAMì€ 23ê°œ ì¤‘ 16ê°œ ë°ì´í„°ì…‹ì—ì„œ ë” ë†’ì€ ì„±ëŠ¥(IoU)ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ \u0026lsquo;SAM (oracle)\u0026lsquo;ì€ SAMì´ ì¶œë ¥í•œ 3ê°œì˜ ë§ˆìŠ¤í¬ ì¤‘ ì •ë‹µê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒì„ ì„ íƒí–ˆì„ ë•Œì˜ ì„±ëŠ¥ì¸ë°, ì´ ê²½ìš° ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ RITMì„ ì••ë„í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ì¼ ì  í”„ë¡¬í”„íŠ¸ê°€ ëª¨í˜¸í•˜ê¸° ë•Œë¬¸ì—, ìë™ í‰ê°€ ì§€í‘œë§Œìœ¼ë¡œëŠ” ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ ì¸¡ì •í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n(b) ì‚¬ëŒ í‰ê°€: ì „ë¬¸ ì‘ì—…ìê°€ ì§ì ‘ ë§ˆìŠ¤í¬ì˜ í’ˆì§ˆì„ 1~10ì ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì‚¬ëŒë“¤ì€ SAMì´ ìƒì„±í•œ ë§ˆìŠ¤í¬ì˜ í’ˆì§ˆì´ RITMë³´ë‹¤ ì›”ë“±íˆ ë†’ë‹¤ê³  í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” SAMì´ IoU ì ìˆ˜ë¡œëŠ” ì¸¡ì •ë˜ì§€ ì•ŠëŠ”, ì‹œê°ì ìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n2. ì œë¡œìƒ· ì—£ì§€ ê²€ì¶œ, ê°ì²´ ì œì•ˆ ë° ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  SAMì€ ë¶„í•  ëª¨ë¸ì´ì§€ë§Œ, ê·¸ ëŠ¥ë ¥ì„ ì‘ìš©í•˜ì—¬ ë‹¤ë¥¸ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì—ë„ ì œë¡œìƒ·ìœ¼ë¡œ ì ìš©í–ˆìŠµë‹ˆë‹¤.\nSAMì´ ìƒì„±í•œ ì—¬ëŸ¬ ë§ˆìŠ¤í¬ë“¤ì˜ ê²½ê³„ë¥¼ ì¢…í•©í•˜ì—¬ ì—£ì§€ ë§µì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì—£ì§€ ê²€ì¶œì„ ìœ„í•´ ì „í˜€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , Figure 10ì—ì„œ ë³´ë“¯ì´ ë§¤ìš° í•©ë¦¬ì ì¸ ì—£ì§€ ë§µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Table 3ì˜ ì •ëŸ‰ í‰ê°€ì—ì„œëŠ” ìµœì‹  ì „ë¬¸ ëª¨ë¸ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë‚®ì•˜ì§€ë§Œ, ì´ˆê¸°ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì—£ì§€ ê²€ì¶œ ëª¨ë¸(HED)ê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©° SAMì˜ ë²”ìš©ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.\nì´ë¯¸ì§€ ë‚´ì— ê°ì²´ê°€ ìˆì„ ë§Œí•œ ì˜ì—­ì„ ì œì•ˆí•˜ëŠ” ì‘ì—…ì—ì„œ, LVIS ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ëœ ê°•ë ¥í•œ íƒì§€ ëª¨ë¸(ViTDet-H)ê³¼ ì„±ëŠ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ë†€ëê²Œë„ SAMì€ ì¤‘ê°„ í¬ê¸°, í° í¬ê¸°, ê·¸ë¦¬ê³  í¬ê·€í•œ ê°ì²´ì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì¬í˜„ìœ¨(Average Recall)ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.\nê°ì²´ íƒì§€ê¸°ê°€ ì°¾ì•„ë‚¸ ë°•ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì–´ ê°ì²´ë¥¼ ë¶„í• í•˜ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤. Table 5ì˜ ìë™ í‰ê°€ ì ìˆ˜(AP)ëŠ” ì „ë¬¸ ëª¨ë¸ì¸ ViTDet-Hë³´ë‹¤ ë‚®ì•˜ì§€ë§Œ, Figure 11ì˜ ì‚¬ëŒ í‰ê°€ì—ì„œëŠ” SAMì˜ ë§ˆìŠ¤í¬ í’ˆì§ˆì´ ë” ë†’ê²Œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ViTDet-Hê°€ COCOë‚˜ LVIS ë°ì´í„°ì…‹ì˜ ê³ ìœ í•œ ì£¼ì„ í¸í–¥(ì˜ˆ: ë§ˆìŠ¤í¬ì— êµ¬ë©ì´ ì—†ëŠ” ë“±)ê¹Œì§€ í•™ìŠµí•˜ì—¬ ì ìˆ˜ë¥¼ ë†’ì¸ ë°˜ë©´, SAMì€ ê·¸ëŸ° í¸í–¥ ì—†ì´ ë” ë³´í¸ì ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.\n3. ì œë¡œìƒ· í…ìŠ¤íŠ¸-ë§ˆìŠ¤í¬ ë³€í™˜ ê°œë… ì¦ëª… ë‹¨ê³„ì˜ ì‹¤í—˜ìœ¼ë¡œ, í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ ê°ì²´ë¥¼ ë¶„í• í•˜ëŠ” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\nSAMì€ \u0026ldquo;a wheel(ë°”í€´)\u0026rdquo; ê°™ì€ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼, \u0026ldquo;beaver tooth grille(ë¹„ë²„ ì´ë¹¨ ê·¸ë¦´)\u0026ldquo;ê³¼ ê°™ì€ êµ¬ì²´ì ì´ê³  ë¯¸ë¬˜í•œ í‘œí˜„ê¹Œì§€ ì´í•´í•˜ê³  í•´ë‹¹ ê°ì²´ë¥¼ ì •í™•í•˜ê²Œ ë¶„í• í•´ëƒˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì‹¤íŒ¨í•  ê²½ìš°, ì  í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n","permalink":"https://mookjsi.github.io/posts/paper-review-sam/","summary":"Meta AIì˜ \u0026lsquo;Segment Anything\u0026rsquo; ë…¼ë¬¸ì— ëŒ€í•œ ì‹¬ì¸µ ë¦¬ë·°ì…ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ëŠ” ì´ë¯¸ì§€ ë¶„í• (Image Segmentation) ë¶„ì•¼ì— \u0026lsquo;íŒŒìš´ë°ì´ì…˜ ëª¨ë¸\u0026rsquo;ì´ë¼ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•œ SAM(Segment Anything Model)ì˜ í•µì‹¬ ê°œë…, ì¦‰ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„í•  ê³¼ì—…, íš¨ìœ¨ì ì¸ ëª¨ë¸ êµ¬ì¡°, ê·¸ë¦¬ê³  11ì–µ ê°œì˜ ë§ˆìŠ¤í¬ë¥¼ í¬í•¨í•˜ëŠ” SA-1B ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ ë°ì´í„° ì—”ì§„ì— ëŒ€í•´ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.","title":"Segment Anything ë…¼ë¬¸ ë¦¬ë·°"},{"content":"1. ì„œë¡  ê°ì²´ íƒì§€ë€ ì´ë¯¸ì§€ ì†ì—ì„œ ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆëŠ” ë¬¼ì²´(ê°ì²´)ê°€ ë¬´ì—‡ì¸ì§€(ë¶„ë¥˜, classification) ê·¸ë¦¬ê³  ì–´ë””ì— ìˆëŠ”ì§€(ìœ„ì¹˜ íŒŒì•…, localization)ë¥¼ ì•Œì•„ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ì£¼í–‰ ìë™ì°¨ê°€ ë„ë¡œ ìœ„ì˜ ì‚¬ëŒ, ë‹¤ë¥¸ ìë™ì°¨, ì‹ í˜¸ë“±ì„ ì •í™•íˆ ì¸ì‹í•˜ëŠ” ë° ì´ ê¸°ìˆ ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\nê¸°ì¡´ì˜ ê°ì²´ íƒì§€ ëª¨ë¸ë“¤ì€ ì´ ë¬¸ì œë¥¼ ì•½ê°„ ê°„ì ‘ì ì¸ ë°©ì‹ìœ¼ë¡œ í’€ì–´ì™”ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ìœ„ì— ìˆ˜ë§ì€ ê°€ìƒì˜ ì‚¬ê°í˜•(ì•µì»¤ ë°•ìŠ¤, anchor box)ì´ë‚˜ **í›„ë³´ ì˜ì—­(proposal)**ì„ ë¯¸ë¦¬ ë¿Œë ¤ë†“ê³ , ì´ ì¤‘ì—ì„œ ì–´ë–¤ ì‚¬ê°í˜•ì´ ì‹¤ì œ ë¬¼ì²´ë¥¼ ì˜ ê°ì‹¸ê³  ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ê·¸ ë¬¼ì²´ëŠ” ë¬´ì—‡ì¸ì§€ë¥¼ ë§ì¶”ëŠ” ë°©ì‹ì´ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê½¤ ì„±ê³µì ì´ì—ˆì§€ë§Œ ëª‡ ê°€ì§€ ë²ˆê±°ë¡œìš´ ê³¼ì •ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤.\nìˆ˜ì‘ì—… ì„¤ê³„ (Hand-designed components): ì•µì»¤ ë°•ìŠ¤ì˜ í¬ê¸°ë‚˜ ë¹„ìœ¨ì„ ë¯¸ë¦¬ ì •í•´ì•¼ í•˜ëŠ” ë“± ì‚¬ëŒì´ ì§ì ‘ ì„¤ì •í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ë§ì•˜ìŠµë‹ˆë‹¤. í›„ì²˜ë¦¬ (Post-processing): ëª¨ë¸ì´ í•˜ë‚˜ì˜ ë¬¼ì²´ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ, ì´ ì¤‘ ê°€ì¥ ì •í™•í•œ ë°•ìŠ¤ í•˜ë‚˜ë§Œ ë‚¨ê¸°ëŠ” **\u0026lsquo;ë¹„ìµœëŒ€ ì–µì œ(Non-Maximum Suppression, NMS)\u0026rsquo;**ë¼ëŠ” ë³µì¡í•œ í›„ì²˜ë¦¬ ê³¼ì •ì´ í•„ìˆ˜ì ì´ì—ˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” DETR (DEtection TRansformer) ì€ ì´ëŸ¬í•œ ë³µì¡í•œ ê³¼ì •ë“¤ì„ ëª¨ë‘ ì—†ì• ê³ , ê°ì²´ íƒì§€ë¥¼ \u0026ldquo;ì •ë‹µ ì§‘í•©ì„ í•œ ë²ˆì— ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ\u0026quot;ë¡œ ì¬ì •ì˜í–ˆìŠµë‹ˆë‹¤. ë§ˆì¹˜ ì‚¬ëŒì´ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆ ì“± ë³´ê³  \u0026ldquo;ì—¬ê¸°ì—” ê³ ì–‘ì´ í•œ ë§ˆë¦¬, ì €ê¸°ì—” ê°•ì•„ì§€ ë‘ ë§ˆë¦¬ê°€ ìˆë„¤\u0026quot;ë¼ê³  ë§í•˜ëŠ” ê²ƒì²˜ëŸ¼, ëª¨ë¸ì´ ì§ì ‘ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ì˜ **\u0026lsquo;ì§‘í•©(set)\u0026rsquo;**ì„ ì¶œë ¥í•˜ë„ë¡ ë§Œë“  ê²ƒì…ë‹ˆë‹¤.\nì´ëŸ¬í•œ í˜ì‹ ì€ ë‘ ê°€ì§€ í•µì‹¬ ìš”ì†Œ ë•ë¶„ì— ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\nì´ë¶„ ë§¤ì¹­ ì†ì‹¤ (Bipartite Matching Loss): ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µì„ ì¼ëŒ€ì¼ë¡œ ì§ì§€ì–´ì£¼ëŠ” ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¤‘ë³µëœ ì˜ˆì¸¡ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë°©ì§€í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer) êµ¬ì¡°: ì›ë˜ ë²ˆì—­ ë¶„ì•¼ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ ì•„í‚¤í…ì²˜ë¡œ, ì´ë¯¸ì§€ ì „ì²´ì˜ ë§¥ë½ì„ ì¢…í•©ì ìœ¼ë¡œ ì´í•´í•˜ëŠ” ë° íƒì›”í•œ ëŠ¥ë ¥ì„ ë³´ì…ë‹ˆë‹¤. ìœ„ ê·¸ë¦¼ì€ DETRì˜ ì „ì²´ì ì¸ íë¦„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\në¨¼ì €, ì¼ë°˜ì ì¸ CNNì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ì˜ íŠ¹ì§•(feature)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€ì˜ ì¤‘ìš”í•œ ì‹œê°ì  ì •ë³´ë¥¼ ì••ì¶•í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ íŠ¹ì§• ì •ë³´ë¥¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”-ë””ì½”ë”ì— ì…ë ¥í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì´ë¯¸ì§€ ì „ì²´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ê³ ì •ëœ ê°œìˆ˜ì˜ ì˜ˆì¸¡ ìƒì(box prediction) ì§‘í•©ì„ ì¶œë ¥í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œëŠ” **ì´ë¶„ ë§¤ì¹­ ì†ì‹¤(bipartite matching loss)**ì„ ì‚¬ìš©í•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìƒìë“¤ê³¼ ì‹¤ì œ ì •ë‹µ ìƒìë“¤ì„ ê°€ì¥ íš¨ìœ¨ì ìœ¼ë¡œ ì¼ëŒ€ì¼ ë§¤ì¹­ì‹œí‚µë‹ˆë‹¤. ë§¤ì¹­ë˜ì§€ ëª»í•œ ì˜ˆì¸¡ ìƒìëŠ” **\u0026ldquo;ë¬¼ì²´ ì—†ìŒ(no object)\u0026rdquo;**ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤. 2. ê¸°ìˆ  ì„¤ëª… (The DETR model) ìœ„ ê·¸ë¦¼ì€ DETRì˜ ìƒì„¸í•œ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\në°±ë³¸ (Backbone) ResNetê³¼ ê°™ì€ í‘œì¤€ CNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì €í•´ìƒë„ì˜ íŠ¹ì§• ë§µ(feature map)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 2048ê°œì˜ ì±„ë„ì„ ê°€ì§„ íŠ¹ì§• ë§µì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.\níŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” (Transformer Encoder) CNNì´ ë§Œë“  íŠ¹ì§• ë§µì€ 2D í˜•íƒœì¸ë°, íŠ¸ëœìŠ¤í¬ë¨¸ì— ë„£ê¸° ìœ„í•´ 1D ì‹œí€€ìŠ¤(ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°) í˜•íƒœë¡œ ê¸¸ê²Œ í¼ì¹©ë‹ˆë‹¤. ê° íŠ¹ì§•ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ê¸° ìœ„í•´ **ê³µê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©(spatial positional encoding)**ì„ ë”í•´ì¤ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë³¸ë˜ ìˆœì„œ ê°œë…ì´ ì—†ê¸° ë•Œë¬¸ì—, \u0026ldquo;ì´ íŠ¹ì§•ì€ ì´ë¯¸ì§€ì˜ ì™¼ìª½ ìœ„ì— ìˆë‹¤\u0026quot;ì™€ ê°™ì€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ì…€í”„ ì–´í…ì…˜(self-attention) ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ ì˜ì—­ë“¤ì´ ì„œë¡œ ì–´ë–»ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ì— ì–¼ë£©ë§ ë‘ ë§ˆë¦¬ê°€ ìˆë‹¤ë©´, ì¸ì½”ë”ëŠ” ì–¼ë£©ë§ ë¬´ëŠ¬ë¥¼ ê°€ì§„ í”½ì…€ë“¤ì´ ì„œë¡œ ê°•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆìŒì„ íŒŒì•…í•˜ê³ , ê° ì–¼ë£©ë§ì„ ë³„ê°œì˜ ê°œì²´ë¡œ ë¶„ë¦¬í•´ë‚´ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” (Transformer Decoder) ê°ì²´ ì¿¼ë¦¬ë€? - 100ëª…ì˜ ì „ë¬¸ íƒì •ë‹¨\nDETRì˜ **ê°ì²´ ì¿¼ë¦¬(object queries)**ë¥¼ ê°€ì¥ ì‰½ê²Œ ì´í•´í•˜ëŠ” ë°©ë²•ì€, ì´ë¯¸ì§€ë¥¼ \u0026lsquo;ì‚¬ê±´ í˜„ì¥\u0026rsquo;ì´ë¼ê³  ë³´ê³  100ëª…ì˜ ì „ë¬¸ íƒì •ë‹¨ì´ ê°ì ë¹ˆ ìˆ˜ì²©(=ê°ì²´ ì¿¼ë¦¬)ì„ ë“¤ê³  íŒŒê²¬ëœë‹¤ê³  ìƒìƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ íƒì •ë“¤ì€ ì²˜ìŒì—ëŠ” ì•„ë¬´ ì •ë³´ë„ ì—†ëŠ” ìƒíƒœ(ë¹ˆ ìˆ˜ì²©)ë¡œ ì‹œì‘í•˜ì§€ë§Œ, í•™ìŠµì„ ê±°ì¹˜ë©° ê°ì ìì‹ ë§Œì˜ ì „ë¬¸ ë¶„ì•¼(íŠ¹ì • ìœ„ì¹˜, í¬ê¸°, í˜•íƒœ ë“±)ë¥¼ ê°–ê²Œ ë©ë‹ˆë‹¤. ê° íƒì •(ê°ì²´ ì¿¼ë¦¬)ì€ ì´ë¯¸ì§€ ì „ì²´(ì¸ì½”ë”ì˜ ì¶œë ¥)ë¥¼ ìƒ…ìƒ…ì´ ì‚´í”¼ë©°, ìì‹ ì´ ë§¡ì„ ë‹¨ì„œ(ê°ì²´)ë¥¼ ì°¾ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. íƒì •ë“¤ë¼ë¦¬ë„ ê³„ì† ì •ë³´ë¥¼ ê³µìœ (ì…€í”„ ì–´í…ì…˜)í•˜ì—¬, í•œ ëª…ì´ í•˜ë‚˜ì˜ ë‹¨ì„œë§Œ ë§¡ë„ë¡ ì—­í• ì„ ë¶„ë‹´í•©ë‹ˆë‹¤. ì´ ë•ë¶„ì— í•˜ë‚˜ì˜ ë¬¼ì²´ì— ì—¬ëŸ¬ ë°•ìŠ¤ê°€ ì¤‘ë³µ ì˜ˆì¸¡ë˜ëŠ” ë¬¸ì œê°€ ìì—°ìŠ¤ëŸ½ê²Œ í•´ê²°ë©ë‹ˆë‹¤(NMS ë¶ˆí•„ìš”). ë””ì½”ë”ì—ì„œì˜ ê°ì²´ ì¿¼ë¦¬ ì—­í• :\në””ì½”ë”ëŠ” ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì§ì ‘ ë°›ëŠ” ëŒ€ì‹ , **ê°ì²´ ì¿¼ë¦¬(object queries)**ë¼ëŠ” ê³ ì •ëœ ê°œìˆ˜(N=100)ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ì´ ê°ì²´ ì¿¼ë¦¬ëŠ” \u0026ldquo;ì´ë¯¸ì§€ì—ì„œ ì°¾ì„ Nê°œì˜ ë¬¼ì²´ ìŠ¬ë¡¯\u0026quot;ì´ì, \u0026lsquo;ì‚¬ê±´ í˜„ì¥ì— íŒŒê²¬ëœ 100ëª…ì˜ íƒì •\u0026rsquo;ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë¹ˆ ìŠ¬ë¡¯(ë¹ˆ ìˆ˜ì²©)ì´ì§€ë§Œ, í•™ìŠµì„ í†µí•´ ê°ê° íŠ¹ì • ìœ„ì¹˜ë‚˜ í¬ê¸°ì˜ ë¬¼ì²´ë¥¼ ì°¾ëŠ” ë° íŠ¹í™”ë©ë‹ˆë‹¤. ë””ì½”ë”ëŠ” ì´ ê°ì²´ ì¿¼ë¦¬ë“¤ê³¼ ì¸ì½”ë”ì˜ ì¶œë ¥(ì´ë¯¸ì§€ ì „ì²´ì˜ ë¬¸ë§¥ ì •ë³´)ì„ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ \u0026ldquo;ë¬¼ì²´ê°€ ì–´ë””ì— ìˆëŠ”ì§€\u0026quot;ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤. ë””ì½”ë”ì˜ ì…€í”„ ì–´í…ì…˜ì€ Nê°œì˜ ì˜ˆì¸¡ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ì—¬, ì˜ˆë¥¼ ë“¤ì–´ í•˜ë‚˜ì˜ ë¬¼ì²´ì— ëŒ€í•´ ì—¬ëŸ¬ ìŠ¬ë¡¯ì´ ì¤‘ë³µìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ì–µì œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤(íƒì •ë“¤ë¼ë¦¬ ì—­í•  ë¶„ë‹´). í•˜ë‚˜ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ì´ Nê°œì˜ ê°ì²´ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ(in parallel), ì¦‰ í•œ ë²ˆì— ë””ì½”ë”©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ëŒ€ì¡°ì ì…ë‹ˆë‹¤. ì˜ˆì¸¡ í—¤ë“œ (Prediction Heads, FFNs) ë””ì½”ë”ì—ì„œ ë‚˜ì˜¨ Nê°œì˜ ì¶œë ¥ ì„ë² ë”©ì€ ê°ê° ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê³µìœ í•˜ëŠ” ì‘ì€ ì‹ ê²½ë§(FFN)ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì´ FFNì€ ìµœì¢…ì ìœ¼ë¡œ ê° ìŠ¬ë¡¯ì— ëŒ€í•´ ë¬¼ì²´ì˜ **í´ë˜ìŠ¤(class)**ì™€ ë°”ìš´ë”© ë°•ìŠ¤(bounding box) ì¢Œí‘œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ë§Œì•½ í•´ë‹¹ ìŠ¬ë¡¯ì´ ì°¾ì€ ë¬¼ì²´ê°€ ì—†ë‹¤ë©´, **\u0026ldquo;ë¬¼ì²´ ì—†ìŒ(no object)\u0026rdquo;**ì´ë¼ëŠ” íŠ¹ë³„í•œ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ë©ë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ (Loss Function): ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ ì§‘í•© ì˜ˆì¸¡ DETRì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì˜ˆì¸¡ê³¼ ì •ë‹µì„ \u0026lsquo;ì§‘í•© ëŒ€ ì§‘í•©\u0026rsquo;ìœ¼ë¡œ ë³´ê³ , ë‘˜ ì‚¬ì´ì˜ ìµœì ì˜ ì§ì„ ì°¾ì•„ ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ë¶„ ë§¤ì¹­ (Bipartite Matching) ëª¨ë¸ì´ 100ê°œì˜ ì˜ˆì¸¡(ì§‘í•© $\\hat{y}$)ì„ ë‚´ë†“ê³ , ì‹¤ì œ ì´ë¯¸ì§€ì—ëŠ” 5ê°œì˜ ë¬¼ì²´(ì •ë‹µ ì§‘í•© y)ê°€ ìˆë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. 100ê°œì˜ ì˜ˆì¸¡ ì¤‘ ì–´ë–¤ ê²ƒì´ 5ê°œì˜ ì •ë‹µ ê°ê°ì— í•´ë‹¹í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ **í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜(Hungarian Algorithm)**ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë§¤ì¹­ ë¹„ìš©ì´ ìµœì†Œê°€ ë˜ëŠ” ìµœì ì˜ ì¼ëŒ€ì¼ ì§(optimal assignment)ì„ ì°¾ìŠµë‹ˆë‹¤.\nì´ ë§¤ì¹­ ë¹„ìš© $\\mathcal{L}_{match}$ëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤: (1) ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ê°€ ì •ë‹µ í´ë˜ìŠ¤ì™€ ì¼ì¹˜í•  í™•ë¥ , (2) ì˜ˆì¸¡í•œ ë°•ìŠ¤ê°€ ì •ë‹µ ë°•ìŠ¤ì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€.\nìˆ˜ì‹ 1: ìµœì  í• ë‹¹ (Optimal Assignment) $$ \\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_N}{\\arg\\min} \\sum_{i}^{N} \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) $$ ìµœì ì˜ ìˆœì—´(permutation) $\\hat{\\sigma}$ ë¥¼ ì°¾ëŠ” ê³¼ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—¬ê¸°ì„œ $\\mathfrak{S}_{N}$ ì€ Nê°œ ì›ì†Œì˜ ëª¨ë“  ê°€ëŠ¥í•œ ìˆœì—´ ì§‘í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. $y_{i}$ ëŠ” ië²ˆì§¸ ì‹¤ì œ ì •ë‹µ ê°ì²´(í´ë˜ìŠ¤ $c_i$ , ë°•ìŠ¤ $b_i$ )ì´ê³ , $\\hat{y}_{\\sigma(i)}$ ëŠ” ìˆœì—´ $\\sigma$ ì— ì˜í•´ ì¬ë°°ì—´ëœ ì˜ˆì¸¡ ì¤‘ ië²ˆì§¸ ìœ„ì¹˜ì— ì˜¨ ì˜ˆì¸¡ì…ë‹ˆë‹¤. ì´ ìˆ˜ì‹ì˜ ëª©í‘œëŠ” ëª¨ë“  Nê°œì˜ ìŒì— ëŒ€í•œ ë§¤ì¹­ ë¹„ìš©( $\\mathcal{L}_{\\text{match}}$ )ì˜ ì´í•©ì„ ìµœì†Œí™”í•˜ëŠ” ìˆœì—´ $\\hat{\\sigma}$ ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì „í˜•ì ì¸ í• ë‹¹ ë¬¸ì œ(assignment problem)ì´ë©°, í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë‹¤í•­ ì‹œê°„ ë‚´ì— íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ ê° ì •ë‹µ ê°ì²´ì— ëŒ€í•´ ê°€ì¥ ì ì ˆí•œ ì˜ˆì¸¡ì´ ë‹¨ í•˜ë‚˜ë§Œ í• ë‹¹ë˜ë„ë¡ ë³´ì¥í•˜ì—¬, ì¤‘ë³µ íƒì§€ë¥¼ ì›ì²œì ìœ¼ë¡œ ë°©ì§€í•©ë‹ˆë‹¤.\nìˆ˜ì‹ 2: í—ê°€ë¦¬ì•ˆ ì†ì‹¤ (Hungarian Loss) $$ \\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^{N} \\left[ -\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbf{1}_{c_i \\ne \\emptyset} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)}) \\right] $$ ìœ„ì—ì„œ ì°¾ì€ ìµœì ì˜ ì§ $\\hat{\\sigma}$ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì†ì‹¤ì€ ëª¨ë“  Nê°œì˜ ë§¤ì¹­ëœ ìŒì— ëŒ€í•œ ì†ì‹¤ì˜ í•©ì…ë‹ˆë‹¤.\ní´ë˜ìŠ¤ ì˜ˆì¸¡ ì†ì‹¤: $-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_{i})$ ëŠ” í‘œì¤€ì ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼(cross-entropy) ì†ì‹¤ì…ë‹ˆë‹¤. ì¦‰, ìµœì ì˜ ì§ìœ¼ë¡œ ë§¤ì¹­ëœ ì˜ˆì¸¡ì´ ì •ë‹µ í´ë˜ìŠ¤ $c_i$ ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ë°•ìŠ¤ ì†ì‹¤: $\\mathbf{1}_{{c_{i}\\ne\\emptyset}}\\mathcal{L}_{\\text{box}}(...)$ ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ì— ëŒ€í•œ ì†ì‹¤ì…ë‹ˆë‹¤. $\\mathbf{1}_{{c_{i}\\ne\\emptyset}}$ ëŠ” ì •ë‹µì´ \u0026lsquo;ë¬¼ì²´ ì—†ìŒ\u0026rsquo;ì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ë°•ìŠ¤ ì†ì‹¤ì„ ê³„ì‚°í•˜ë¼ëŠ” ì˜ë¯¸ì˜ ì§€ì‹œ í•¨ìˆ˜(indicator function)ì…ë‹ˆë‹¤. $\\mathcal{L}_{\\text{box}}$ ëŠ” ì•„ë˜ì—ì„œ ì„¤ëª…í•  L1 ì†ì‹¤ê³¼ GIoU ì†ì‹¤ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ë°”ìš´ë”© ë°•ìŠ¤ ì†ì‹¤ ($\\mathcal{L}_{box}$) ê¸°ì¡´ì˜ L1 ì†ì‹¤(ì¢Œí‘œ ì°¨ì´ì˜ ì ˆëŒ“ê°’ í•©)ì€ ë°•ìŠ¤ê°€ í´ ë•Œ ì†ì‹¤ ê°’ë„ ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìˆì–´, ë°•ìŠ¤ í¬ê¸°ì— ë”°ë¼ ì†ì‹¤ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DETRì€ ë‘ ê°€ì§€ ì†ì‹¤ì„ ì¡°í•©í•©ë‹ˆë‹¤. $$ \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{L1}||b_i - \\hat{b}_{\\sigma(i)}||_1 $$ L1 ì†ì‹¤: ì˜ˆì¸¡ ë°•ìŠ¤ì™€ ì •ë‹µ ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì  ì¢Œí‘œ, ë„ˆë¹„, ë†’ì´ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì¼ë°˜í™”ëœ IoU (GIoU) ì†ì‹¤: ë‘ ë°•ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” IoU(Intersection over Union)ë¥¼ ì¼ë°˜í™”í•œ ê²ƒìœ¼ë¡œ, ë°•ìŠ¤ í¬ê¸°ì— ë¬´ê´€í•˜ê²Œ(scale-invariant) ì†ì‹¤ì„ ì¸¡ì •í•  ìˆ˜ ìˆì–´ ì‘ì€ ë¬¼ì²´ì™€ í° ë¬¼ì²´ë¥¼ ê³µí‰í•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì‹ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°: \u0026ldquo;ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ ì°¾ê¸°\u0026quot;ë¡œ í’€ì–´ë³´ëŠ” DETRì˜ ì§‘í•© ì˜ˆì¸¡ ì§€ê¸ˆê¹Œì§€ ì†Œê°œí•œ ìˆ˜ì‹ë“¤ì„ ì‹¤ì œ ê°ì²´ íƒì§€ ìƒí™©ì— ëŒ€ì…í•´, ì²˜ìŒë¶€í„° ëê¹Œì§€ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ë³´ê² ìŠµë‹ˆë‹¤.\nìƒí™© ì˜ˆì‹œ: ì´ë¯¸ì§€ ì† ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ ì…ë ¥ ì´ë¯¸ì§€: ì™¼ìª½ì—ëŠ” ê³ ì–‘ì´ í•œ ë§ˆë¦¬, ì˜¤ë¥¸ìª½ì—ëŠ” ê°•ì•„ì§€ í•œ ë§ˆë¦¬ê°€ ìˆëŠ” ì‚¬ì§„ DETR ëª¨ë¸: ìµœëŒ€ 4ê°œ( $N=4$ )ì˜ ê°ì²´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì„¤ì • 1. ì •ë‹µ(Ground Truth)ê³¼ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •ë‹µ ì§‘í•© (y):\nGT1: {í´ë˜ìŠ¤: \u0026lsquo;ê³ ì–‘ì´\u0026rsquo;, ë°•ìŠ¤: b_cat} GT2: {í´ë˜ìŠ¤: \u0026lsquo;ê°•ì•„ì§€\u0026rsquo;, ë°•ìŠ¤: b_dog} GT3: {í´ë˜ìŠ¤: \u0026lsquo;ì—†ìŒ âˆ…\u0026rsquo;} GT4: {í´ë˜ìŠ¤: \u0026lsquo;ì—†ìŒ âˆ…\u0026rsquo;} ëª¨ë¸ì˜ ì˜ˆì¸¡ ì§‘í•© (Å·):\nP1: {\u0026lsquo;ê³ ì–‘ì´\u0026rsquo; í™•ë¥  0.9, ë°•ìŠ¤: b_p1} (ê³ ì–‘ì´ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ê²Œ ì˜ˆì¸¡) P2: {\u0026lsquo;ê°•ì•„ì§€\u0026rsquo; í™•ë¥  0.8, ë°•ìŠ¤: b_p2} (ê°•ì•„ì§€ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ê²Œ ì˜ˆì¸¡) P3: {\u0026lsquo;ê³ ì–‘ì´\u0026rsquo; í™•ë¥  0.7, ë°•ìŠ¤: b_p3} (P1ê³¼ ì¤‘ë³µëœ, ëœ ì •í™•í•œ ê³ ì–‘ì´ ì˜ˆì¸¡) P4: {\u0026lsquo;ì—†ìŒ âˆ…\u0026rsquo; í™•ë¥  0.85, ë°•ìŠ¤: b_p4} (ìŠ¤ìŠ¤ë¡œ \u0026lsquo;ì—†ìŒ\u0026rsquo;ì´ë¼ê³  ì˜ˆì¸¡) 2. ìˆ˜ì‹ 1: ìµœì ì˜ ì§ ì°¾ê¸° (Optimal Assignment) $\\hat{\\sigma} = \\underset{\\sigma \\in \\mathfrak{S}_N}{\\arg\\min} \\sum_{i}^{N} \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})$ ì´ ë‹¨ê³„ì—ì„œëŠ” 4ê°œì˜ ì •ë‹µê³¼ 4ê°œì˜ ì˜ˆì¸¡ ì‚¬ì´ì—ì„œ ê°€ì¥ í•©ë¦¬ì ì¸ ì¼ëŒ€ì¼ ì§ì„ ì°¾ìŠµë‹ˆë‹¤.\në¹„ìš©( $\\mathcal{L}_{match}$ )ì€ (1) í´ë˜ìŠ¤ í™•ë¥ , (2) ë°•ìŠ¤ ìœ ì‚¬ë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•©ë‹ˆë‹¤.\nì»´í“¨í„°ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¹„ìš© í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤(ì‹¤ì œ ê°’ì€ ì˜ˆì‹œ):\nì˜ˆì¸¡ P1 (ê³ ì–‘ì´ 0.9) ì˜ˆì¸¡ P2 (ê°•ì•„ì§€ 0.8) ì˜ˆì¸¡ P3 (ê³ ì–‘ì´ 0.7) ì˜ˆì¸¡ P4 (ì—†ìŒ 0.85) GT1 (ê³ ì–‘ì´) ë¹„ìš© ë‚®ìŒ ë¹„ìš© ë†’ìŒ ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ë†’ìŒ GT2 (ê°•ì•„ì§€) ë¹„ìš© ë†’ìŒ ë¹„ìš© ë‚®ìŒ ë¹„ìš© ë†’ìŒ ë¹„ìš© ë†’ìŒ GT3 (ì—†ìŒ âˆ…) ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ GT4 (ì—†ìŒ âˆ…) ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ ë¹„ìš© ì¤‘ê°„ í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ì´ ì´ í‘œë¥¼ ë¶„ì„í•´ ì „ì²´ ë¹„ìš©ì´ ìµœì†Œê°€ ë˜ëŠ” ì¡°í•©ì„ ì°¾ìŠµë‹ˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì§( $\\hat{\\sigma}$ )ì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.\nGT1 (\u0026lsquo;ê³ ì–‘ì´\u0026rsquo;) â†”ï¸ P1 (ê°€ì¥ ì •í™•í•œ ê³ ì–‘ì´ ì˜ˆì¸¡) GT2 (\u0026lsquo;ê°•ì•„ì§€\u0026rsquo;) â†”ï¸ P2 (ê°€ì¥ ì •í™•í•œ ê°•ì•„ì§€ ì˜ˆì¸¡) GT3 (\u0026lsquo;ì—†ìŒ âˆ…\u0026rsquo;) â†”ï¸ P3 (ì¤‘ë³µëœ ê³ ì–‘ì´ ì˜ˆì¸¡ì€ \u0026lsquo;ì—†ìŒ\u0026rsquo; ì²˜ë¦¬) GT4 (\u0026lsquo;ì—†ìŒ âˆ…\u0026rsquo;) â†”ï¸ P4 (ì“¸ëª¨ì—†ëŠ” ì˜ˆì¸¡ì€ \u0026lsquo;ì—†ìŒ\u0026rsquo; ì²˜ë¦¬) 3. ìˆ˜ì‹ 2: ìµœì¢… ì†ì‹¤ ê³„ì‚° (Hungarian Loss) $\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^{N} \\left[ -\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbf{1}_{c_i \\ne \\emptyset} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)}) \\right]$ ìœ„ì—ì„œ ì°¾ì€ ì§ì„ ë°”íƒ•ìœ¼ë¡œ, ê° ìŒì— ëŒ€í•´ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n[GT1â†”P1] í´ë˜ìŠ¤ ì†ì‹¤: ê³ ì–‘ì´ í™•ë¥  0.9 â†’ $-\\log(0.9)$ (ë‚®ì€ ì†ì‹¤) ë°•ìŠ¤ ì†ì‹¤: ì‹¤ì œ ê³ ì–‘ì´ ë°•ìŠ¤ì™€ ì˜ˆì¸¡ ë°•ìŠ¤ì˜ ì°¨ì´( $\\mathcal{L}_{box}$ ) [GT2â†”P2] í´ë˜ìŠ¤ ì†ì‹¤: ê°•ì•„ì§€ í™•ë¥  0.8 â†’ $-\\log(0.8)$ (ë‚®ì€ ì†ì‹¤) ë°•ìŠ¤ ì†ì‹¤: ì‹¤ì œ ê°•ì•„ì§€ ë°•ìŠ¤ì™€ ì˜ˆì¸¡ ë°•ìŠ¤ì˜ ì°¨ì´( $\\mathcal{L}_{box}$ ) [GT3â†”P3] í´ë˜ìŠ¤ ì†ì‹¤: ì •ë‹µì€ \u0026lsquo;ì—†ìŒ\u0026rsquo;, ì˜ˆì¸¡ì´ \u0026lsquo;ì—†ìŒ\u0026rsquo;ì¼ í™•ë¥ ì´ 10%ë¼ë©´ $-\\log(0.1)$ (ë§¤ìš° í° ì†ì‹¤) ë°•ìŠ¤ ì†ì‹¤: ì •ë‹µì´ \u0026lsquo;ì—†ìŒ\u0026rsquo;ì´ë¯€ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ(0ì ) [GT4â†”P4] í´ë˜ìŠ¤ ì†ì‹¤: ì •ë‹µì€ \u0026lsquo;ì—†ìŒ\u0026rsquo;, ì˜ˆì¸¡ì´ \u0026lsquo;ì—†ìŒ\u0026rsquo;ì¼ í™•ë¥ ì´ 85%ë¼ë©´ $-\\log(0.85)$ (ë‚®ì€ ì†ì‹¤) ë°•ìŠ¤ ì†ì‹¤: ì •ë‹µì´ \u0026lsquo;ì—†ìŒ\u0026rsquo;ì´ë¯€ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ(0ì ) ì´ë ‡ê²Œ ë„¤ ìŒì˜ ì†ì‹¤ì„ ëª¨ë‘ ë”í•œ ê°’ì´ ìµœì¢… ì†ì‹¤ì´ ë˜ì–´, ëª¨ë¸ì´ ë” ë˜‘ë˜‘í•´ì§€ë„ë¡ í•™ìŠµì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n4. ë°”ìš´ë”© ë°•ìŠ¤ ì†ì‹¤( $\\mathcal{L}_{box}$ )ì˜ ì‹¤ì œ ì˜ë¯¸ $\\mathcal{L}_{\\text{box}} = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}} + \\lambda_{L1}||\\cdot||_1$ L1 ì†ì‹¤: ì˜ˆì¸¡ ë°•ìŠ¤ì™€ ì‹¤ì œ ë°•ìŠ¤ì˜ ì¢Œí‘œ(x, y, w, h) ì°¨ì´ì˜ ì ˆëŒ“ê°’ í•©\nì˜ˆ) ê³ ì–‘ì´ ë°•ìŠ¤ì™€ ì˜ˆì¸¡ ë°•ìŠ¤ê°€ ê°ê° (10,10,50,50), (12,11,48,52)ë¼ë©´, L1 ì†ì‹¤ì€ $|10-12|+|10-11|+|50-48|+|50-52|=2+1+2+2=7$ ë‹¨ì : í° ë¬¼ì²´ì™€ ì‘ì€ ë¬¼ì²´ì— ë™ì¼í•œ í”½ì…€ ì˜¤ì°¨ê°€ ë“¤ì–´ê°€ë©´ ë¶ˆê³µí‰í•¨ GIoU ì†ì‹¤: ë‘ ë°•ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€(Intersection over Union, IoU)ë¥¼ ì¼ë°˜í™”í•œ ê°’\nì˜ˆ) ê³ ì–‘ì´ ë°•ìŠ¤ì™€ ì˜ˆì¸¡ ë°•ìŠ¤ê°€ ê±°ì˜ ê²¹ì¹˜ë©´ GIoU ì†ì‹¤ì´ ë§¤ìš° ì‘ìŒ(ì •í™•) ì¥ì : ë°•ìŠ¤ í¬ê¸°ì— ìƒê´€ì—†ì´ ê³µí‰í•˜ê²Œ í‰ê°€ ìµœì¢… ì¡°í•©: GIoU ì†ì‹¤ë¡œ ì „ì²´ì ì¸ ìœ„ì¹˜ì™€ í¬ê¸° ì¼ì¹˜ë„ë¥¼, L1 ì†ì‹¤ë¡œ ë¯¸ì„¸í•œ ìœ„ì¹˜ ì¡°ì •ì„ ë™ì‹œì— í•™ìŠµ\nì´ì²˜ëŸ¼ DETRì˜ ì§‘í•© ì˜ˆì¸¡ ì†ì‹¤ì€\nì¤‘ë³µ ì˜ˆì¸¡ì„ ë°©ì§€í•˜ê³ , ë°•ìŠ¤ í¬ê¸°ì— ìƒê´€ì—†ì´ ê³µí‰í•˜ê²Œ í‰ê°€í•˜ë©°, í›„ì²˜ë¦¬(NMS) ì—†ì´ë„ ì •í™•í•œ ê°ì²´ íƒì§€ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 3. ì‹¤í—˜ (Experiments) DETRì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” COCO ë°ì´í„°ì…‹ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\nì´ í‘œëŠ” DETRê³¼ ê¸°ì¡´ì˜ ê°•ë ¥í•œ ëª¨ë¸ì¸ Faster R-CNNì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\nê²°ë¡ ì ìœ¼ë¡œ, DETRì€ ë§¤ìš° ì˜ ìµœì í™”ëœ Faster R-CNNê³¼ **ë¹„êµí•  ë§Œí•œ ì„±ëŠ¥(AP 42.0 vs 42.0)**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì£¼ëª©í•  ì ì€, DETRì´ **í° ë¬¼ì²´ì— ëŒ€í•´ì„œëŠ” í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥($AP_L$ 61.1 vs 53.4)**ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ê°€ ì´ë¯¸ì§€ ì „ì²´ì˜ ë„“ì€ ë¬¸ë§¥ì„ ë³´ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤. ë°˜ë©´, ì‘ì€ ë¬¼ì²´ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ë‹¤ì†Œ ë‚®ê²Œ($AP_S$ 20.5 vs 26.6) ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\nì£¼ìš” êµ¬ì„± ìš”ì†Œ ë¶„ì„ (Ablation Studies) DETRì˜ ì–´ë–¤ ë¶€ë¶„ì´ ì„±ëŠ¥ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ì—¬ëŸ¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\nì¸ì½”ë”ì˜ ì¤‘ìš”ì„± (Table 2, Figure 3): ì¸ì½”ë” ì¸µì„ ì œê±°í•˜ì ì „ì²´ ì„±ëŠ¥(AP)ì´ ì•½ 3.9ì  í•˜ë½í–ˆìœ¼ë©°, íŠ¹íˆ í° ë¬¼ì²´ì— ëŒ€í•œ ì„±ëŠ¥ì´ 6.0ì ì´ë‚˜ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€ ì „ì²´ì˜ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ì¸ì½”ë”ê°€ ê°ì²´ë“¤ì„ ì„œë¡œ ë¶„ë¦¬í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\nFigure 3ì€ ì¸ì½”ë”ì˜ ì…€í”„ ì–´í…ì…˜ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¼ ì¤‘ì•™ì˜ ì†Œ ì´ë¯¸ì§€ ìœ„ì— ì°íŒ ë¹¨ê°„ ì ì´ íŠ¹ì • ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ê³ , ì£¼ë³€ì˜ ì‘ì€ ì´ë¯¸ì§€ë“¤ì€ í•´ë‹¹ ìœ„ì¹˜ê°€ ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ì–´ë–¤ ë¶€ë¶„ì— ì£¼ëª©(attention)í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°ê¸° ë‹¤ë¥¸ ì†Œë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ì£¼ëª©ë°›ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ì¸ì½”ë”ê°€ ì´ë¯¸ ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ë¶„ë¦¬í•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\në””ì½”ë”ì˜ ì¤‘ìš”ì„± (Figure 4): ì´ ê·¸ë˜í”„ëŠ” ë””ì½”ë” ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥(AP)ì´ ê¾¸ì¤€íˆ í–¥ìƒë˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë””ì½”ë” ì¸µì˜ ê²°ê³¼ì™€ ë§ˆì§€ë§‰ ì¸µì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ë©´ APê°€ 8ì  ì´ìƒ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.\ní¥ë¯¸ë¡œìš´ ì ì€, ì²« ë²ˆì§¸ ë””ì½”ë” ì¸µì—ì„œëŠ” NMS í›„ì²˜ë¦¬ë¥¼ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì˜¤ë¥´ì§€ë§Œ, ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ NMSì˜ íš¨ê³¼ê°€ ì¤„ì–´ë“¤ê³  ë§ˆì§€ë§‰ì—ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ì•½ê°„ í•´ì¹œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ë””ì½”ë”ì˜ ì…€í”„ ì–´í…ì…˜ì´ ìŠ¤ìŠ¤ë¡œ ì¤‘ë³µ ì˜ˆì¸¡ì„ ì–µì œí•˜ëŠ” ë²•ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, DETRì€ ì„¤ê³„ì ìœ¼ë¡œ NMSê°€ í•„ìš” ì—†ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.\nê·¸ ì™¸ ìš”ì†Œë“¤ (Table 3, 4): ìœ„ì¹˜ ì¸ì½”ë”© (Table 3): ê³µê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©ì„ ì œê±°í•˜ì ì„±ëŠ¥ì´ 7.8 APë‚˜ í•˜ë½í•˜ì—¬, ì´ ìš”ì†Œê°€ ë§¤ìš° ì¤‘ìš”í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ (Table 4): L1 ì†ì‹¤ê³¼ GIoU ì†ì‹¤ ì¤‘ GIoU ì†ì‹¤ì´ ì„±ëŠ¥ì— ê±°ì˜ ì ˆëŒ€ì ì¸ ê¸°ì—¬ë¥¼ í•˜ë©°, L1 ì†ì‹¤ì€ ë³´ì¡°ì ì¸ ì—­í• ë§Œ í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. Figure 6ì€ ë””ì½”ë”ê°€ ê° ë¬¼ì²´ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì´ë¯¸ì§€ì˜ ì–´ëŠ ë¶€ë¶„ì„ ì£¼ëª©í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. ì½”ë¼ë¦¬ë‚˜ ì–¼ë£©ë§ì„ íƒì§€í•  ë•Œ, ì£¼ë¡œ ë¨¸ë¦¬, ë‹¤ë¦¬ ë“± ë¬¼ì²´ì˜ **ìœ¤ê³½ì„ ê²°ì •í•˜ëŠ” ê·¹ë‹¨ì ì¸ ë¶€ë¶„(extremities)**ì— ì£¼ëª©í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ì¸ì½”ë”ê°€ ì´ë¯¸ \u0026ldquo;ì—¬ê¸°ì— ì–¼ë£©ë§ì´ ìˆë‹¤\u0026quot;ê³  ì•Œë ¤ì£¼ë©´, ë””ì½”ë”ëŠ” ê·¸ ê²½ê³„ë§Œ ì •í™•íˆ ê·¸ë¦¬ëŠ” ë° ì§‘ì¤‘í•œë‹¤ëŠ” ê°€ì„¤ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.\nFigure 7ì€ 100ê°œì˜ ê°ì²´ ì¿¼ë¦¬(ìŠ¬ë¡¯)ê°€ ê°ê° ì–´ë–¤ ì¢…ë¥˜ì˜ ë°•ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ìŠ¬ë¡¯ì€ íŠ¹ì • **ìœ„ì¹˜(area)ì™€ í¬ê¸°(box size)**ë¥¼ ì „ë‹´í•˜ë„ë¡ íŠ¹í™”ë˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ ìŠ¬ë¡¯ì€ ì´ë¯¸ì§€ ì¤‘ì•™ì˜ í° ë¬¼ì²´ë¥¼, ë‹¤ë¥¸ ìŠ¬ë¡¯ì€ ì™¼ìª½ í•˜ë‹¨ì˜ ì‘ì€ ë¬¼ì²´ë¥¼ ì£¼ë¡œ ì°¾ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.\ní•™ìŠµ ë°ì´í„°ì—ëŠ” ê¸°ë¦°ì´ 13ë§ˆë¦¬ ì´ìƒ ìˆëŠ” ì´ë¯¸ì§€ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ìƒí™©ì—ì„œë„ ì˜ ë™ì‘í•˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´, ì¸ê³µì ìœ¼ë¡œ ê¸°ë¦° 24ë§ˆë¦¬ê°€ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.\nFigure 5ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, DETRì€ ë†€ëê²Œë„ 24ë§ˆë¦¬ì˜ ê¸°ë¦°ì„ ëª¨ë‘ ì •í™•í•˜ê²Œ ì°¾ì•„ëƒˆìŠµë‹ˆë‹¤. ì´ëŠ” 100ê°œì˜ ê°ì²´ ì¿¼ë¦¬ê°€ íŠ¹ì • í´ë˜ìŠ¤(ì˜ˆ: \u0026lsquo;ê¸°ë¦° ì „ìš© ì¿¼ë¦¬\u0026rsquo;)ì— ê³¼ì í•©ë˜ì§€ ì•Šê³ , ì¼ë°˜ì ì¸ ë¬¼ì²´ë¥¼ ì°¾ëŠ” ëŠ¥ë ¥ì„ í•™ìŠµí–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nPanoptic Segmentationìœ¼ë¡œì˜ í™•ì¥ Panoptic Segmentationì€ ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ì„ \u0026ldquo;ì–´ë–¤ ë¬¼ì²´ì— ì†í•˜ëŠ”ì§€(things)\u0026rdquo; ë˜ëŠ” \u0026ldquo;ì–´ë–¤ ë°°ê²½ì— ì†í•˜ëŠ”ì§€(stuff)\u0026ldquo;ë¡œ êµ¬ë¶„í•˜ëŠ” ë” ë³µì¡í•œ ì‘ì—…ì…ë‹ˆë‹¤.\nFigure 8ì€ DETRì— ê°„ë‹¨í•œ **ë§ˆìŠ¤í¬ í—¤ë“œ(mask head)**ë¥¼ ì¶”ê°€í•˜ì—¬ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë””ì½”ë”ì˜ ì¶œë ¥ ê°ê°ì— ëŒ€í•´ í•´ë‹¹ ë¬¼ì²´ì˜ ë§ˆìŠ¤í¬ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ í•©ì³ ìµœì¢… ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.\nTable 5ëŠ” DETRì´ PanopticFPNê³¼ ê°™ì€ ê¸°ì¡´ì˜ ê°•ë ¥í•œ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŠ¹íˆ ë°°ê²½(stuff) í´ë˜ìŠ¤ì—ì„œ ê°•ì ì„ ë³´ì´ëŠ”ë°, ì´ëŠ” ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë³´ëŠ” ì¸ì½”ë”ì˜ í˜ ë•ë¶„ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\nFigure 9ëŠ” DETRì´ ìƒì„±í•œ Panoptic Segmentationì˜ ì˜ˆì‹œ ì´ë¯¸ì§€ë¡œ, ë¬¼ì²´ì™€ ë°°ê²½ ëª¨ë‘ì— ëŒ€í•´ ê¹”ë”í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n4. ê²°ë¡  (Conclusion) ì´ ë…¼ë¬¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ì´ë¶„ ë§¤ì¹­ ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ **\u0026lsquo;ì§ì ‘ì ì¸ ì§‘í•© ì˜ˆì¸¡ ë¬¸ì œ\u0026rsquo;**ë¡œ í’€ì–´ë‚´ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì¸ DETRì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. DETRì€ ê¸°ì¡´ì˜ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸(ì•µì»¤, NMS ë“±)ì„ ì œê±°í•˜ë©´ì„œë„, ê³ ë„ë¡œ ìµœì í™”ëœ Faster R-CNNê³¼ ëŒ€ë“±í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\në˜í•œ, DETRì˜ êµ¬ì¡°ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ê³  ìœ ì—°í•˜ì—¬ Panoptic Segmentationê³¼ ê°™ì€ ë” ë³µì¡í•œ ë¬¸ì œë¡œë„ ì‰½ê²Œ í™•ì¥ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ì˜ ì „ì—­ì ì¸ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ëŠ¥ë ¥ ë•ë¶„ì— í° ê°ì²´ íƒì§€ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n","permalink":"https://mookjsi.github.io/posts/paper-review-detr/","summary":"\u0026lsquo;End-to-End Object Detection with Transformers\u0026rsquo; ë…¼ë¬¸ ì‹¬ì¸µ ë¦¬ë·°","title":"DETR: End-to-End Object Detection with Transformers"},{"content":"CVPR 2016ì—ì„œ ë°œí‘œë˜ì–´ ë”¥ëŸ¬ë‹ ì—­ì‚¬ì— í•œ íšì„ ê·¸ì€ ë…¼ë¬¸, Deep Residual Learning for Image Recognitionì— ëŒ€í•œ ë¦¬ë·°ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì€ ë‹¹ì‹œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í° ë‚œì œì˜€ë˜ \u0026lsquo;ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ”\u0026rsquo; ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ì €ìë“¤ì˜ í•´ë²•ì¸ **ResNet (Residual Network)**ì€ \u0026lsquo;ì”ì°¨ í•™ìŠµ\u0026rsquo;ì´ë¼ëŠ” í˜ì‹ ì ì¸ êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ì´ì „ì—ëŠ” ë¶ˆê°€ëŠ¥í–ˆë˜ 100ì¸µ ì´ìƒì˜ ì´ˆì‹¬ì¸µ ì‹ ê²½ë§ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.\nì„œë¡  ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ë¥¼ ì¸ì‹í•˜ëŠ” ê¸°ìˆ , ì˜ˆë¥¼ ë“¤ì–´ ì‚¬ì§„ ì†ì˜ ê³ ì–‘ì´ë¥¼ ì•Œì•„ë³´ëŠ” ì¸ê³µì§€ëŠ¥(AI)ì€ \u0026lsquo;ì‹¬ì¸µ ì‹ ê²½ë§(Deep Neural Network)\u0026lsquo;ì´ë¼ëŠ” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì‹ ê²½ë§ì€ ì¸ê°„ì˜ ë‡Œê°€ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ëª¨ë°©í•œ ê²ƒìœ¼ë¡œ, ì—¬ëŸ¬ ê°œì˜ ì¸µ(layer)ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œëŠ” ì´ ì¸µì„ ë§ì´ ìŒ“ì„ìˆ˜ë¡, ì¦‰ ë„¤íŠ¸ì›Œí¬ê°€ \u0026lsquo;ê¹Šì–´ì§ˆìˆ˜ë¡\u0026rsquo; ë” ë˜‘ë˜‘í•´ì ¸ì•¼ í•©ë‹ˆë‹¤. ë§ˆì¹˜ ìš°ë¦¬ê°€ ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì‚¬ê³ ë¥¼ ê±°ì³ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒê³¼ ê°™ì£ .\ní•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¬´ì‘ì • ì¸µì„ ê¹Šê²Œ ìŒ“ê¸°ë§Œ í•˜ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥ ì €í•˜ (Degradation) ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‹ ê¸°í•˜ê²Œë„ ì´ëŠ” ë‹¨ìˆœíˆ ê³„ì‚°ì´ ë„ˆë¬´ ë³µì¡í•´ì ¸ì„œ ìƒê¸°ëŠ” ê³¼ì í•©(overfitting) ë¬¸ì œë„ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ë” ê¹Šì€ ëª¨ë¸ì´ ê·¸ë³´ë‹¤ ì–•ì€ ëª¨ë¸ë³´ë‹¤ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ë¥˜ìœ¨(training error)ì´ ë” ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ì´ìƒí•œ í˜„ìƒì´ì—ˆìŠµë‹ˆë‹¤.\nìœ„ ê·¸ë¦¼ì€ ì´ ë¬¸ì œë¥¼ ëª…í™•íˆ ë³´ì—¬ì¤ë‹ˆë‹¤. CIFAR-10ì´ë¼ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ 20ì¸µ ë„¤íŠ¸ì›Œí¬ì™€ 56ì¸µ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¨ ê²°ê³¼ì…ë‹ˆë‹¤. ì™¼ìª½ ê·¸ë˜í”„(training error)ë¥¼ ë³´ë©´, ë” ê¹Šì€ 56ì¸µ ë„¤íŠ¸ì›Œí¬(ë¶‰ì€ìƒ‰ ì„ )ê°€ 20ì¸µ ë„¤íŠ¸ì›Œí¬(ë…¸ë€ìƒ‰ ì„ )ë³´ë‹¤ í›ˆë ¨ ì˜¤ë¥˜ê°€ ë” ë†’ìŠµë‹ˆë‹¤. ë‹¹ì—°íˆ ì˜¤ë¥¸ìª½ ê·¸ë˜í”„(test error)ì—ì„œë„ 56ì¸µ ë„¤íŠ¸ì›Œí¬ì˜ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ê°€ ë” ë†’ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ìƒì‹ì ìœ¼ë¡œ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ê°€ ìµœì†Œí•œ ì–•ì€ ë„¤íŠ¸ì›Œí¬ë§Œí¼ì˜ ì„±ëŠ¥ì€ ë‚´ì•¼ í•˜ëŠ”ë°, ì‹¤ì œë¡œëŠ” ê·¸ë ‡ì§€ ëª»í–ˆë˜ ê²ƒì…ë‹ˆë‹¤.\nì´ ë…¼ë¬¸ì€ ë°”ë¡œ ì´ \u0026lsquo;ì„±ëŠ¥ ì €í•˜\u0026rsquo; ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ê¹Šì€ ì”ì°¨ í•™ìŠµ (Deep Residual Learning)**ì´ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•´, ë„¤íŠ¸ì›Œí¬ê°€ ì²˜ìŒë¶€í„° ì •ë‹µì„ ì™„ë²½í•˜ê²Œ ë§ì¶”ë ¤ê³  ì• ì“°ëŠ” ëŒ€ì‹ , ì´ë¯¸ ì•Œê³  ìˆëŠ” ì •ë³´(ì…ë ¥ê°’)ë¥¼ ë°”íƒ•ìœ¼ë¡œ \u0026lsquo;ì°¨ì´(residual)\u0026lsquo;ë§Œì„ í•™ìŠµí•˜ë„ë¡ êµ¬ì¡°ë¥¼ ë°”ê¾¼ ê²ƒì…ë‹ˆë‹¤. ì´ í˜ì‹ ì ì¸ ë°©ë²•ìœ¼ë¡œ ì €ìë“¤ì€ ì´ì „ì— ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ì—¬ê²¨ì¡Œë˜ 152ì¸µ ê¹Šì´ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ í›ˆë ¨ì‹œì¼°ê³ , ë‹¹ì‹œ ì´ë¯¸ì§€ ì¸ì‹ ëŒ€íšŒì¸ ILSVRC 2015ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í•˜ëŠ” ì¾Œê±°ë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.\nê¸°ìˆ  ì„¤ëª… ResNetì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì–´ë–»ê²Œ \u0026lsquo;ì°¨ì´\u0026rsquo;ë§Œì„ í•™ìŠµí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì¼ê¹Œìš”? ë°”ë¡œ **ì§€ë¦„ê¸¸ ì—°ê²° (Shortcut Connection)**ì´ë¼ëŠ” êµ¬ì¡°ë¥¼ í†µí•´ êµ¬í˜„ë©ë‹ˆë‹¤.\nê¸°ì¡´ì˜ ì‹ ê²½ë§ì€ ì…ë ¥ê°’ xê°€ ì—¬ëŸ¬ ì¸µì„ ìˆœì„œëŒ€ë¡œ í†µê³¼í•˜ë©° ë³µì¡í•œ í•¨ìˆ˜ H(x)ë¥¼ í•™ìŠµí•˜ë ¤ê³  í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ ì‚¬ì§„(x)ì„ ë³´ê³  \u0026lsquo;ê³ ì–‘ì´\u0026rsquo;ë¼ëŠ” ì •ë‹µ(H(x))ì„ ë°”ë¡œ ì°¾ì•„ë‚´ë ¤ëŠ” ë°©ì‹ì´ì—ˆì£ .\ní•˜ì§€ë§Œ ResNetì€ ìƒê°ì„ ë°”ê¿¨ìŠµë‹ˆë‹¤. \u0026lsquo;ì–´ì°¨í”¼ ì…ë ¥ê°’ xê°€ ìˆëŠ”ë°, êµ³ì´ H(x) ì „ì²´ë¥¼ ìƒˆë¡œ ë°°ìš¸ í•„ìš”ê°€ ìˆì„ê¹Œ? ê·¸ëƒ¥ xì—ë‹¤ê°€ ì•½ê°„ì˜ ì •ë³´ë§Œ ë”í•´ì„œ ì •ë‹µì„ ë§Œë“¤ë©´ ë˜ì§€ ì•Šì„ê¹Œ?\u0026rsquo; ë¼ëŠ” ì ‘ê·¼ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•´ì•¼ í•  ëª©í‘œë¥¼ H(x)ì—ì„œ F(x) = H(x) - xë¡œ ë°”ê¿‰ë‹ˆë‹¤. ì—¬ê¸°ì„œ F(x)ê°€ ë°”ë¡œ ì…ë ¥ê°’ê³¼ ì •ë‹µ ì‚¬ì´ì˜ \u0026lsquo;ì°¨ì´\u0026rsquo;, ì¦‰ **ì”ì°¨ (residual)**ì…ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ëŠ” ì´ ì”ì°¨ F(x)ë¥¼ í•™ìŠµí•œ ë’¤, ì›ë˜ ì…ë ¥ê°’ xë¥¼ ë”í•´ ìµœì¢… ê²°ê³¼(F(x) + x)ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.\nìœ„ ê·¸ë¦¼ì€ ì´ \u0026lsquo;ì”ì°¨ í•™ìŠµ\u0026rsquo;ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\nì…ë ¥ê°’ xê°€ ë‘ ê°ˆë˜ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. í•œìª½ì€ ê¸°ì¡´ì²˜ëŸ¼ ê°€ì¤‘ì¹˜ ì¸µ(weight layer)ì„ í†µê³¼í•˜ë©° ë³µì¡í•œ ë³€í™˜, ì¦‰ F(x)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ë‹¤ë¥¸ í•œìª½ì€ ì•„ë¬´ëŸ° ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ê±´ë„ˆë›°ëŠ” \u0026lsquo;ì§€ë¦„ê¸¸(shortcut)\u0026lsquo;ì„ ë”°ë¼ê°‘ë‹ˆë‹¤. ì´ë¥¼ **í•­ë“± ë§¤í•‘ (identity mapping)**ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— ë‘ ê²°ê³¼(F(x)ì™€ x)ê°€ ë”í•´ì ¸ ìµœì¢… ì¶œë ¥(F(x) + x)ì´ ë©ë‹ˆë‹¤. ì´ êµ¬ì¡°ê°€ ì™œ íš¨ê³¼ì ì¼ê¹Œìš”? ë§Œì•½ ì–´ë–¤ ì¸µì—ì„œ ì•„ë¬´ê²ƒë„ í•™ìŠµí•  í•„ìš” ì—†ì´ ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ìµœì„ ì¸ ìƒí™©ì´ë¼ë©´, ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ëŠ” ì—¬ëŸ¬ ì¸µì„ ê±°ì¹˜ë©° ì…ë ¥ê³¼ ì¶œë ¥ì´ ë˜‘ê°™ì•„ì§€ëŠ” ë³µì¡í•œ ë³€í™˜ì„ í•™ìŠµí•´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° ì–´ë ¤ìš´ ì¼ì´ì—ˆì£ . í•˜ì§€ë§Œ ResNet êµ¬ì¡°ì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ì”ì°¨ F(x)ë¥¼ ê·¸ëƒ¥ \u0026lsquo;0\u0026rsquo;ìœ¼ë¡œ ë§Œë“¤ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì¦‰, ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì•„ë¬´ê²ƒë„ ë°”ê¾¸ì§€ ì•Šìœ¼ë©´, ìì—°ìŠ¤ëŸ½ê²Œ ì…ë ¥ xê°€ ê·¸ëŒ€ë¡œ ì¶œë ¥ìœ¼ë¡œ ë‚˜ê°€ê²Œ ë©ë‹ˆë‹¤. í›¨ì”¬ ì‰¬ìš´ ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.\në‹¤ìŒì€ ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì…ë‹ˆë‹¤.\nVGG-19 (ì™¼ìª½): ë‹¹ì‹œ í‘œì¤€ìœ¼ë¡œ ì—¬ê²¨ì§€ë˜ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì…ë‹ˆë‹¤. 34-layer plain (ì¤‘ê°„): VGG-19ì˜ ëª¨ë¸ì„ ë”°ë¼ ë‹¨ìˆœíˆ ì¸µì„ ê¹Šê²Œ ìŒ“ì€ ì¼ë°˜ì ì¸ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤. ì´ êµ¬ì¡°ì—ì„œ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. 34-layer residual (ì˜¤ë¥¸ìª½): ì¤‘ê°„ì˜ \u0026lsquo;plain\u0026rsquo; ë„¤íŠ¸ì›Œí¬ì™€ ë˜‘ê°™ì€ êµ¬ì¡°ì— \u0026lsquo;ì§€ë¦„ê¸¸ ì—°ê²°(shortcut connection)\u0026lsquo;ë§Œ ì¶”ê°€í•œ ResNet êµ¬ì¡°ì…ë‹ˆë‹¤. êµ½ì€ í™”ì‚´í‘œë“¤ì´ ë°”ë¡œ ì´ ì§€ë¦„ê¸¸ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬(50ì¸µ ì´ìƒ)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ë³‘ëª© (Bottleneck) êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\nê¸°ì¡´ì˜ ë¸”ë¡(ì™¼ìª½)ì´ 3x3 í•„í„°ì˜ í•©ì„±ê³± ì¸µ ë‘ ê°œë¡œ ì´ë£¨ì–´ì¡Œë‹¤ë©´, ë³‘ëª© ë¸”ë¡(ì˜¤ë¥¸ìª½)ì€ 1x1, 3x3, 1x1 í•©ì„±ê³± ì¸µ ì„¸ ê°œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì²« ë²ˆì§¸ 1x1 í•©ì„±ê³±ì€ ì±„ë„ ìˆ˜ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ê³ (ë³‘ëª©ì²˜ëŸ¼ ì…êµ¬ê°€ ì¢ì•„ì§), 3x3 í•©ì„±ê³±ìœ¼ë¡œ í•µì‹¬ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•œ ë’¤, ë§ˆì§€ë§‰ 1x1 í•©ì„±ê³±ìœ¼ë¡œ ë‹¤ì‹œ ì±„ë„ ìˆ˜ë¥¼ ì›ë˜ëŒ€ë¡œ ë³µì›í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ êµ¬ì¡° ë•ë¶„ì— ì¸µì€ ë” ê¹Šê²Œ ìŒ“ìœ¼ë©´ì„œë„ ì „ì²´ì ì¸ ê³„ì‚° ë³µì¡ë„ëŠ” VGG ë„¤íŠ¸ì›Œí¬ë³´ë‹¤ ë‚®ê²Œ ìœ ì§€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ì €ìë“¤ì€ ì œì•ˆí•œ ResNetì˜ íš¨ê³¼ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë„·(ImageNet)ê³¼ CIFAR-10ì´ë¼ëŠ” ëŒ€í‘œì ì¸ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\nì´ë¯¸ì§€ë„·(ImageNet) ë¶„ë¥˜ ì‹¤í—˜ ì´ë¯¸ì§€ë„·ì€ 1000ê°œì˜ í´ë˜ìŠ¤(ì¢…ë¥˜)ë¡œ ì´ë£¨ì–´ì§„ 128ë§Œ ì¥ì˜ ë°©ëŒ€í•œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\nìœ„ ê²°ê³¼ë“¤ì€ ResNetì´ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\nì™¼ìª½ ê·¸ë˜í”„: ì¼ë°˜(plain) ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìš°, 34ì¸µ(ë¶‰ì€ìƒ‰ ì„ )ì´ 18ì¸µ(í•˜ëŠ˜ìƒ‰ ì„ )ë³´ë‹¤ í›ˆë ¨ ì˜¤ë¥˜ì™€ ê²€ì¦ ì˜¤ë¥˜ ëª¨ë‘ ë” ë†’ìŠµë‹ˆë‹¤. ì „í˜•ì ì¸ ì„±ëŠ¥ ì €í•˜ í˜„ìƒì…ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„: ResNetì˜ ê²½ìš°, ìƒí™©ì´ ì—­ì „ë©ë‹ˆë‹¤. 34ì¸µ ResNet(ë¶‰ì€ìƒ‰ ì„ )ì´ 18ì¸µ ResNet(í•˜ëŠ˜ìƒ‰ ì„ )ë³´ë‹¤ ì˜¤ë¥˜ìœ¨ì´ í›¨ì”¬ ë‚®ìŠµë‹ˆë‹¤. ê¹Šì´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœ ê²ƒì…ë‹ˆë‹¤. ìœ„ í‘œëŠ” ì´ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¼ë°˜ ë„¤íŠ¸ì›Œí¬ëŠ” 18ì¸µ(27.94%)ì—ì„œ 34ì¸µ(28.54%)ìœ¼ë¡œ ê°ˆ ë•Œ ì˜¤ë¥˜ìœ¨ì´ ë†’ì•„ì¡Œì§€ë§Œ, ResNetì€ 18ì¸µ(27.88%)ì—ì„œ 34ì¸µ(25.03%)ìœ¼ë¡œ ê°ˆ ë•Œ ì˜¤ë¥˜ìœ¨ì´ 2.8%ë‚˜ í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤.\në” ê¹Šì€ ResNet ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì€ ìœ„ ë‘ í‘œì™€ ê°™ìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ì•ì—ì„œ ì„¤ëª…í•œ \u0026lsquo;ë³‘ëª©\u0026rsquo; êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ 50ì¸µ, 101ì¸µ, ê·¸ë¦¬ê³  ë¬´ë ¤ 152ì¸µì— ë‹¬í•˜ëŠ” ResNetì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. 34ì¸µ(25.03%)ë¶€í„° ì‹œì‘í•´ 50ì¸µ(22.85%), 101ì¸µ(21.75%), 152ì¸µ(21.43%)ìœ¼ë¡œ ê¹Šì–´ì§ˆìˆ˜ë¡ top-1 ì˜¤ë¥˜ìœ¨ì´ ê¾¸ì¤€íˆ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ ë¬¸ì œì—†ì´ ê¹Šì´ì˜ ì´ì ì„ ì·¨í•œ ê²ƒì…ë‹ˆë‹¤.\níŠ¹íˆ 152ì¸µ ResNetì€ ë‹¨ì¼ ëª¨ë¸ë§Œìœ¼ë¡œ 4.49%ì˜ top-5 ê²€ì¦ ì˜¤ë¥˜ìœ¨ì„ ë‹¬ì„±í–ˆëŠ”ë°, ì´ëŠ” ë‹¹ì‹œ ë‹¤ë¥¸ ì—¬ëŸ¬ ëª¨ë¸ì„ í•©ì¹œ ì•™ìƒë¸”(ensemble) ê²°ê³¼ë³´ë‹¤ë„ ì¢‹ì€ ì„±ì ì´ì—ˆìŠµë‹ˆë‹¤.\nìµœì¢… ëŒ€íšŒ ê²°ê³¼, ì—¬ëŸ¬ ResNet ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ì—¬ ì´ë¯¸ì§€ë„· í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ **3.57%**ë¼ëŠ” ê²½ì´ë¡œìš´ top-5 ì˜¤ë¥˜ìœ¨ì„ ê¸°ë¡í•˜ë©° ILSVRC 2015 ëŒ€íšŒì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤.\nCIFAR-10 ë¶„ì„ ë° 1000ì¸µ ì´ìƒì˜ ë„¤íŠ¸ì›Œí¬ CIFAR-10 ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ì‹¤í—˜ì—ì„œë„ ë¹„ìŠ·í•œ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ìœ„ ê·¸ë˜í”„ëŠ” CIFAR-10ì—ì„œì˜ í›ˆë ¨ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nì™¼ìª½ (plain networks): ì¼ë°˜ ë„¤íŠ¸ì›Œí¬ëŠ” 20ì¸µì—ì„œ 56ì¸µìœ¼ë¡œ ê¹Šì–´ì§ˆìˆ˜ë¡ ì˜¤ë¥˜ìœ¨ì´ ì ì  ë†’ì•„ì§€ëŠ” ì„±ëŠ¥ ì €í•˜ í˜„ìƒì„ ë³´ì…ë‹ˆë‹¤. ì¤‘ê°„ (ResNets): ë°˜ë©´, ResNetì€ 20ì¸µë¶€í„° 110ì¸µê¹Œì§€ ê¹Šì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ì˜¤ë¥˜ìœ¨ì´ ê¾¸ì¤€íˆ ê°ì†Œí•©ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ (110-layer vs 1202-layer): ì €ìë“¤ì€ ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°€ 1202ì¸µì´ë¼ëŠ” ê·¹ë‹¨ì ìœ¼ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤. í›ˆë ¨ ì˜¤ë¥˜ëŠ” 0.1% ë¯¸ë§Œìœ¼ë¡œ ë§¤ìš° ë‚®ì•˜ì§€ë§Œ(ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ ì•„ë˜ìª½ ì„ ), í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜(7.93%)ëŠ” 110ì¸µ ëª¨ë¸(6.43%)ë³´ë‹¤ ë‹¤ì†Œ ë†’ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” ì‘ì€ ë°ì´í„°ì…‹ì— ë¹„í•´ ëª¨ë¸ì´ ë„ˆë¬´ ê±°ëŒ€í•´ì„œ ë°œìƒí•œ ê³¼ì í•©(overfitting) ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤. ìœ„ ê·¸ë˜í”„ëŠ” ê° ì¸µì˜ ì‘ë‹µ(ì¶œë ¥ê°’)ì˜ í‘œì¤€í¸ì°¨ë¥¼ ë¶„ì„í•œ ê²ƒì…ë‹ˆë‹¤.\nê·¸ë˜í”„ë¥¼ ë³´ë©´ ì „ë°˜ì ìœ¼ë¡œ ResNet(ë¶‰ì€ìƒ‰, ê²€ì€ìƒ‰ ì„ )ì˜ ì‘ë‹µê°’ì´ ì¼ë°˜ ë„¤íŠ¸ì›Œí¬(ë…¸ë€ìƒ‰, ë¶„í™ìƒ‰ ì„ )ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ì´ëŠ” ResNetì˜ ì”ì°¨ í•¨ìˆ˜ê°€ ì¼ë°˜ì ìœ¼ë¡œ \u0026lsquo;0\u0026rsquo;ì— ê°€ê¹Œìš´ ê°’ì„ ê°–ëŠ”ë‹¤ëŠ” ê°€ì„¤ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤. ì¦‰, ê° ì¸µì´ ì‹ í˜¸ë¥¼ í¬ê²Œ ë°”ê¾¸ê¸°ë³´ë‹¤ëŠ” ì¡°ê¸ˆì”©ë§Œ ìˆ˜ì •í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë˜í•œ ResNet-20, 56, 110ì„ ë¹„êµí•´ë³´ë©´, ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê° ì¸µì˜ ì‘ë‹µê°’ì´ ë” ì‘ì•„ì§€ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤. ë” ë§ì€ ì¸µì´ í˜‘ë ¥í•˜ì—¬ ì‹ í˜¸ë¥¼ ì¡°ê¸ˆì”© ì ì§„ì ìœ¼ë¡œ ë°”ê¾¼ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ê°ì²´ íƒì§€(Object Detection) ì‹¤í—˜ ResNetì€ ë‹¨ìˆœíˆ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì´ë¯¸ì§€ ì† íŠ¹ì • ë¬¼ì²´ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê°ì²´ íƒì§€ ê³¼ì œì—ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì•„ë˜ í‘œë“¤ì€ ê¸°ì¡´ì˜ VGG-16 ë„¤íŠ¸ì›Œí¬ë¥¼ ResNet-101ë¡œ êµì²´í–ˆì„ ë•Œì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ ì–´ë ¤ìš´ COCO ë°ì´í„°ì…‹ì—ì„œ mAP(ê°ì²´ íƒì§€ ì„±ëŠ¥ì˜ ì£¼ìš” ì²™ë„)ê°€ 28%ë‚˜ ìƒëŒ€ì ìœ¼ë¡œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ResNetì´ í•™ìŠµí•œ í‘œí˜„(representation) ìì²´ê°€ ë§¤ìš° ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•©ë‹ˆë‹¤.\nê²°ë¡  ì„±ëŠ¥ ì €í•˜ (Degradation) ë¬¸ì œ ì •ì˜ ë° í•´ê²°: ì´ì „ê¹Œì§€ëŠ” ëª…í™•í•˜ê²Œ ì„¤ëª…ë˜ì§€ ì•Šì•˜ë˜, ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ í›ˆë ¨ì´ ë” ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œë¥¼ \u0026lsquo;ì„±ëŠ¥ ì €í•˜\u0026rsquo;ë¡œ ëª…í™•íˆ ì •ì˜í•˜ê³ , ì´ë¥¼ \u0026lsquo;ì”ì°¨ í•™ìŠµ(Residual Learning)\u0026lsquo;ì´ë¼ëŠ” í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ì´ˆì‹¬ì¸µ ì‹ ê²½ë§ (Extremely Deep Network)ì˜ ê°€ëŠ¥ì„± ì œì‹œ: \u0026lsquo;ì§€ë¦„ê¸¸ ì—°ê²°(Shortcut Connection)\u0026lsquo;ì´ë¼ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ êµ¬ì¡°ë¥¼ í†µí•´ 152ì¸µ, ë‚˜ì•„ê°€ 1000ì¸µì´ ë„˜ëŠ” ë§¤ìš° ê¹Šì€ ì‹ ê²½ë§ì˜ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê¹Šì´ì— ëŒ€í•œ ê¸°ì¡´ì˜ í•œê³„ë¥¼ ì™„ì „íˆ ë¬´ë„ˆëœ¨ë¦° ê²ƒì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œì˜ SOTA (State-of-the-art) ë‹¬ì„±: ì œì•ˆëœ ResNetì€ ì´ë¯¸ì§€ ë¶„ë¥˜ë¿ë§Œ ì•„ë‹ˆë¼ ê°ì²´ íƒì§€, ë¶„í•  ë“± ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©° ìƒˆë¡œìš´ í‘œì¤€ ëª¨ë¸ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤. FYI ResNetì„ êµ¬í˜„í•œ ì¢‹ì€ ì˜ˆì‹œ ë§í¬ê°€ ìˆì–´ì„œ ê°€ì ¸ì™€ë´¤ìŠµë‹ˆë‹¤!\nhttps://github.com/JayPatwardhan/ResNet-PyTorch\n","permalink":"https://mookjsi.github.io/posts/paper-review-resnet/","summary":"CVPR 2016ì—ì„œ ë°œí‘œëœ \u0026lsquo;Deep Residual Learning for Image Recognition\u0026rsquo; ë…¼ë¬¸ì— ëŒ€í•œ ì‹¬ì¸µ ë¦¬ë·°ì…ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì˜ \u0026lsquo;ì„±ëŠ¥ ì €í•˜(Degradation)\u0026rsquo; ë¬¸ì œë¥¼ í•´ê²°í•œ ì”ì°¨ í•™ìŠµ(Residual Learning)ì˜ í•µì‹¬ ê°œë…, ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°, ê·¸ë¦¬ê³  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì•Œê¸° ì‰½ê²Œ ë¶„ì„í•©ë‹ˆë‹¤.","title":"ResNet - ë” ê¹Šì€ ì‹ ê²½ë§ì„ ìœ„í•œ ì”ì°¨ í•™ìŠµ"},{"content":"\nSlide 1: Title Slide Title: Advanced Policy Gradient Methods: TRPO \u0026amp; PPO Session: YBIGTA SUMMER SESSION Presenter: DS 26 Jungmook Kang Date: 2025.08.26 Slide 2-5: Why does Reinforcement Learning sometimes fail? Main Question: \u0026ldquo;Why does reinforcement learning sometimes break down?\u0026rdquo; Problem: Basic policy gradient methods (like A2C) have limitations. Making drastic changes to the policyâ€”the agent\u0026rsquo;s decision-making criteriaâ€”can cause significant problems. Key Issues: Unstable Learning: If an update step is too large and in the wrong direction, the agent\u0026rsquo;s performance can collapse, making recovery impossible. This is more damaging than in supervised learning because a bad policy affects the distribution of states and rewards the agent will see in the future. Changing Data Distribution: As the policy changes, the data (states, actions, rewards) it collects also changes. This non-stationarity of the input data makes learning difficult. Example Analogy: A robot learning to walk. If it\u0026rsquo;s walking well and tries a new strategy by swinging its legs too wide, it will fall. Once it has fallen, it can no longer collect useful data about walking, and learning stops. Slide 6-8: What is TRPO? \u0026ldquo;Let\u0026rsquo;s not change the policy too much!\u0026rdquo; Concept: Trust Region Policy Optimization (TRPO). Core Idea: When updating the policy, TRPO establishes a \u0026ldquo;trust region\u0026rdquo; or a \u0026ldquo;safe zone\u0026rdquo; to ensure the new policy does not stray too far from the old one. Goal: \u0026ldquo;Maximize policy performance under the constraint that the difference (distance) between the old and new policies does not exceed a certain value, Î´.\u0026rdquo; This prevents the destructive, large policy updates that can lead to performance collapse. Slide 9-11: The Mathematical Expression of TRPO This slide introduces the core optimization problem of TRPO. The goal is to maximize an objective function, subject to a constraint.\nObjective Function (What to Maximize): The expression $\\mathbb{E}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}\\hat{A}]$ represents the expected advantage of the new policy $\\pi_{\\theta}$ relative to the old policy $\\pi_{\\theta_{old}}$. This is a surrogate objective that uses importance sampling to estimate the performance of the new policy using data collected from the old one. The term $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ is the importance sampling ratio. The full objective function is: $$ L_{\\theta_{old}}^{IS}(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}\\mid s_{t})}{\\pi_{\\theta_{old}}(a_{t}\\mid s_{t})}\\hat{A}_{t}\\right] $$ Constraint (The \u0026ldquo;Trust Region\u0026rdquo;): The term $\\mathbb{E}[D_{KL}(\\pi_{\\theta_{old}}||\\pi_{\\theta})]\\le\\delta$ is the constraint that defines the trust region. $D_{KL}$ stands for Kullback-Leibler (KL) Divergence. It\u0026rsquo;s a way to measure the \u0026ldquo;distance\u0026rdquo; or difference between two probability distributions. Here, it measures how much the new policy $\\pi_{\\theta}$ has changed from the old policy $\\pi_{\\theta_{old}}$. By keeping this value less than a small constant $\\delta$, we ensure the policy update is small and stays within a \u0026ldquo;trusted\u0026rdquo; region where the performance estimate is reliable. The full optimization problem solved by TRPO is:\n$$ \\underset{\\theta}{\\text{maximize}} \\quad \\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}\\hat{A}_{t}\\right] $$ $$ \\text{subject to} \\quad \\hat{\\mathbb{E}}_{t}\\left[KL\\big(\\pi_{\\theta_{old}}(\\cdot|s_{t}),\\,\\pi_{\\theta}(\\cdot|s_{t})\\big)\\right]\\le\\delta $$ Slide 12: Limitations of TRPO Problem: While TRPO\u0026rsquo;s core idea was groundbreaking for stabilizing RL, it wasn\u0026rsquo;t widely adopted in practice. Major Drawbacks: High Computational Cost: It uses a complex, second-order optimization technique called the \u0026ldquo;conjugate gradient method\u0026rdquo; which is computationally heavy. Difficult to Implement: The algorithm is complex to code and debug. Poor Performance on Certain Tasks: It struggled with tasks involving complex visual inputs (requiring deep CNNs), such as the Atari benchmark. It is also hard to use with architectures that have multiple outputs (e.g., a policy and a value function). Slide 13-14: PPO: The Practical Version of TRPO Concept: Proximal Policy Optimization (PPO). Core Idea: PPO inherits TRPO\u0026rsquo;s philosophy of \u0026ldquo;let\u0026rsquo;s change the policy a little at a time\u0026rdquo; but replaces the complex and strict trust region constraint with a simpler mechanism that is easier to implement and more efficient. How it Improves on TRPO: Instead of using a hard constraint, PPO modifies the objective function to penalize policies that move too far away from the old policy. The most common technique for this is called Clipping. Impact: PPO is now one of the most widely used policy optimization algorithms due to its simplicity, stability, and strong performance. Slide 15-19: PPO\u0026rsquo;s Core Idea: Clipping This section details the PPO-Clip objective function, which is the key to its success.\nPPO-Clip Objective Function: $$ L^{CLIP}(\\theta)=\\hat{\\mathbb{E}}\\left[\\min\\big(r_{t}(\\theta)\\hat{A}_{t},\\ \\operatorname{clip}(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t}\\big)\\right] $$ Breakdown of Terms:\n$r_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$: This is the same probability ratio used in TRPO. It indicates whether an action is more or less likely under the new policy compared to the old one. $clip(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)$: This is the clipping mechanism. It forces the probability ratio $r_t(\\theta)$ to stay within a small range, $[1-\\epsilon, 1+\\epsilon]$ (e.g., [0.8, 1.2]). This acts as a safety rail, preventing the ratio from becoming too large or too small. $min(\u0026hellip;)$: The final objective takes the minimum of two values: The original objective ($r_{t}(\\theta)\\hat{A}_{t}$) The clipped objective ($clip(\u0026hellip;)\\hat{A}_{t}$) By taking the minimum, PPO creates a pessimistic, lower-bound estimate of the policy\u0026rsquo;s performance. This effectively puts a cap on the benefit you can get from a single update, which discourages excessively large changes to the policy. Analogy: It\u0026rsquo;s like setting a study plan. No matter how much you enjoy one subject, you cap its study time at \u0026ldquo;a maximum of 3 hours per day.\u0026rdquo; This prevents you from neglecting other subjects and keeps your overall learning balanced and stable.\nSlide 20-21: The Power and Application of PPO Why it\u0026rsquo;s popular: PPO is simple, powerful, and stable, making it the de-facto standard for reinforcement learning. Where is PPO used?: Robotics: Training stable and precise movements. Game Playing: Achieving superhuman performance in complex games. Large Language Models (LLMs): Fine-tuning models like ChatGPT to produce more helpful and safer responses (a process known as Reinforcement Learning from Human Feedback or RLHF). Major Users: Almost all leading AI companies, including OpenAI, DeepMind, Meta, and NVIDIA, use PPO as a core algorithm. ","permalink":"https://mookjsi.github.io/posts/trpoppo/","summary":"Session notes that explain TRPO\u0026rsquo;s trust region and PPO\u0026rsquo;s clipping with intuition and equations, and summarize limitations and practical applications.","title":"Advanced Policy Gradient Methods: TRPO \u0026 PPO"},{"content":"I\u0026rsquo;m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025.\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like \u0026ldquo;not A, but B\u0026rdquo;). The authors\u0026rsquo; solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nThis is my presentation on \u0026ldquo;Promptriever: Instruction-Trained Retrievers,\u0026rdquo; which I put together for the Information Theory and Machine Learning Lab.\nFirst, let\u0026rsquo;s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It\u0026rsquo;s also worth noting this work was accepted as a poster at ICLR 2025.\nI want to start with the paper\u0026rsquo;s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be \u0026ldquo;intelligent, instruction-following partners, not just data finders.\u0026rdquo;\nHere are the other co-authors who contributed to this research.\nI\u0026rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We\u0026rsquo;ll start with the motivation behind the research.\nSo, how do current search engines \u0026ldquo;think\u0026rdquo;? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what\u0026rsquo;s the problem?\nThis example makes the problem obvious. Imagine you need a laptop that is not a MacBook and costs under $1000. A standard retriever sees the keywords \u0026ldquo;MacBook\u0026rdquo; and \u0026ldquo;under $1000\u0026rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant.\nThis leads to a frustrating user experience. You\u0026rsquo;re forced to keep tweaking keywords and filters just to find what you want.\nThis is where Promptriever comes in. It doesn\u0026rsquo;t just use semantic similarity. Instead, it operates on \u0026ldquo;dynamic relevance definitions,\u0026rdquo; which allows for a much more intelligent process.\nLet\u0026rsquo;s look at the same query again, but with Promptriever. It correctly understands the instructionsâ€”the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.\nThe key idea here is that Promptriever \u0026ldquo;dynamically adjusts relevance based on your natural language instructions.\u0026rdquo; It\u0026rsquo;s not just matching words; it\u0026rsquo;s understanding commands.\nWe\u0026rsquo;ve seen what it does, which leads to the next question: \u0026ldquo;But how on earth was this made?\u0026rdquo; Let\u0026rsquo;s get into the technical details.\nNow, we\u0026rsquo;ll dive into the second section, where I\u0026rsquo;ll break down the Promptriever model\u0026rsquo;s architecture and training.\nThe architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder.\nThe main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model\u0026rsquo;s instruction-following ability. So how did they keep the model intelligent?\nThe answer, as they stated in their core message, lies in their \u0026ldquo;novel training data, which makes ignoring commands impossible for correct answers.\u0026rdquo;\nFor context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.\nPromptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. This is what lets them train the model on complex, instruction-based prompts.\nThe most clever part of their training data is the Instruction-Negative. This is a document that\u0026rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.\nHereâ€™s a perfect example. For the query \u0026ldquo;What is the capital of France?,\u0026rdquo; a general article about Paris is a good result. But if you add the instruction \u0026ldquo;mention its average annual rainfall,\u0026rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.\nThrough this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.\nThe authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.\nCreating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.\nNow for the final section, \u0026ldquo;Experiments,\u0026rdquo; where we\u0026rsquo;ll look at the results.\nFor a fair comparison, they ran an \u0026ldquo;Apples-to-Apples\u0026rdquo; test against RepLLaMA, using the exact same data and hyperparameters.\nThey used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions.\nThe results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.\nThis table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability.\nOn the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nThis is where it gets interesting. When given a helpful prompt, Promptriever\u0026rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely \u0026ldquo;promptable.\u0026rdquo;\nThis table looks at the standard deviation of scores across different prompts. Promptriever\u0026rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nThe ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nThe authors also proved their training \u0026ldquo;recipe\u0026rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, \u0026ldquo;A golden recipe doesn\u0026rsquo;t discriminate against ingredients!\u0026rdquo;\nFinally, let\u0026rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper\u0026rsquo;s scope.\nIn the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","permalink":"https://mookjsi.github.io/posts/paper-review-promptriever/","summary":"A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","title":"Promptriever - Instruction-Trained Retrievers"},{"content":"This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the geometry of the loss landscape and a model\u0026rsquo;s ability to generalize.\nThe core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, flat regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.\nThis is the title slide for my presentation on \u0026ldquo;Simplifying neural nets by discovering flat minima,\u0026rdquo; which I prepared for our lab meeting.\nFirst, let\u0026rsquo;s credit the authors: Sepp Hochreiter and JÃ¼rgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?\nHere are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).\nThis quote from the authors perfectly captures the paper\u0026rsquo;s central thesis: \u0026ldquo;Find wide, not sharp, minima - and your network generalizes for free.\u0026rdquo; It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.\nMy presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.\nSo, let\u0026rsquo;s start with the motivation. Why was this research necessary?\nTo understand the paper\u0026rsquo;s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network\u0026rsquo;s weights (so-called \u0026ldquo;priors\u0026rdquo;). This paper marked a shift, arguing that we should focus on the geometry of the error surface rather than imposing assumptions about the weights themselves.\nThis 3D plot illustrates the core concept perfectly. On the right, we see a \u0026ldquo;sharp\u0026rdquo; minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a \u0026ldquo;flat\u0026rdquo; minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.\nHere\u0026rsquo;s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It\u0026rsquo;s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It\u0026rsquo;s much easier to find a good spot, and moving around a little doesn\u0026rsquo;t drastically change your altitude (or your model\u0026rsquo;s error). This robustness is linked to lower model complexity.\nSo, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a \u0026ldquo;good\u0026rdquo; prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.\nThe \u0026ldquo;Flat Minima\u0026rdquo; approach offers solutions to these problems. First, it doesn\u0026rsquo;t require any pre-chosen priors; the geometry of the solution space itself defines what a \u0026ldquo;simple\u0026rdquo; model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.\nNow, let\u0026rsquo;s formalize the problem by defining the tasks and architectures this method applies to.\nThe basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.\nThe model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).\nBuilding on that, we can define the set of \u0026ldquo;acceptable\u0026rdquo; solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors w for which the training MSE is less than or equal to this tolerance.\nNow we get to the core of the algorithm\u0026rsquo;s construction. For a given weight vector w, we define a \u0026ldquo;box\u0026rdquo; around it. For each individual weight $w_{ij}$, we find the maximum amount $\\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\\Delta w_{ij}$ for each weight.\nThese intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the \u0026ldquo;Flat Minima\u0026rdquo; as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.\nSo, how do we actually find these large-volume minima? That brings us to the algorithm itself.\nThe main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\\Delta w$. A larger volume signifies a flatter, more desirable minimum.\nThis slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.\nMaximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.\nThis new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.\nTo build the algorithm, we first need to mathematically define \u0026ldquo;flatness\u0026rdquo;. We start by defining the change in the network\u0026rsquo;s output, $ED(w, \\delta w)$, that results from a small change in weights, $\\delta w$.\nTo make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \\delta w)$, in terms of the original output and the first derivatives (gradients).\nSubstituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.\nThis leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don\u0026rsquo;t lead to large output changes.\nThe second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as \u0026ldquo;spherical\u0026rdquo; as possible by ensuring that perturbations to each weight contribute equally to the total output change.\nHere is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.\nBy rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).\nThis slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.\nBy solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\\Delta w_{ij}|$. This formula depends on the network\u0026rsquo;s output gradients.\nLet\u0026rsquo;s quickly recap the algorithm\u0026rsquo;s goal. We aim to maximize the box volume $\\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.\nNow that we have a formula for the size of the box edges, $\\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network\u0026rsquo;s derivatives. This slide shows that connection, approximating the log of $\\Delta w_{ij}$ with the log of the derived formula.\nThis slide restates the formula for the size of the perturbation $|\\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.\nPlugging the expression for $\\Delta w_{ij}$ into our cost function $B(w, D_0) = -\\sum \\log \\Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the \u0026ldquo;flatness\u0026rdquo; of the minimum.\nThe complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\\lambda$ balances the two.\nTo minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network\u0026rsquo;s output, which would typically be very expensive to compute.\nHowever, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$â€”the same as standard backpropagationâ€”using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.\nTo recap the key advantages: the \u0026ldquo;Flat Minima\u0026rdquo; method is appealing because it uses geometry instead of priors to define simplicity, it\u0026rsquo;s computationally efficient, and it naturally performs network pruning for better generalization.\nThe paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.\nIn the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.\nThe results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.\nThe second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.\nThe final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.\nThe results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.\n","permalink":"https://mookjsi.github.io/posts/paper-review-flatminima/","summary":"A detailed review of the classic 1994 paper by Hochreiter \u0026amp; Schmidhuber, \u0026lsquo;Simplifying Neural Nets by Discovering Flat Minima\u0026rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization.","title":"Simplifying Neural Nets by Discovering Flat Minima"},{"content":"Hello everyone, I\u0026rsquo;m Jungmook Kang at Yonsei University. Today, I\u0026rsquo;m excited to share a review of a fascinating paper that was recently accepted to NeurIPS, which delves into the theoretical underpinnings of the attention mechanism. Let\u0026rsquo;s get started.\nThe title of the paper is \u0026ldquo;Max-Margin Token Selection in Attention Mechanism.\u0026rdquo; This work comes from our lab here at Yonsei University, and it explores the question of why attention works the way it does.\nSo, let\u0026rsquo;s begin with the motivation for this research. What prompted us to look into this?\nWe all know that the attention mechanism is incredibly effective. For a sentence like \u0026ldquo;The pizza came out of the oven and it tasted good!\u0026rdquo;, attention can correctly associate \u0026ldquo;it\u0026rdquo; with \u0026ldquo;pizza.\u0026rdquo; It works remarkably well\u0026hellip; but the story doesn\u0026rsquo;t end there.\nWhen we look at the standard Transformer architecture, which powers models like BERT and GPT, we see a complex system of encoders and decoders with multiple layers of multi-head attention. This raises some fundamental questions: Why is this architecture so successful? And more specifically, what is happening to the model\u0026rsquo;s weights during the optimization process that leads to this success?\nAnalyzing this architecture theoretically is very challenging. The optimization problem is both non-linear, due to functions like Softmax, and non-convex. This makes it extremely difficult to describe the training dynamics with traditional methods.\nTo tackle this, we turn to the concept of \u0026ldquo;Implicit Bias.\u0026rdquo; This is a phenomenon where the optimization algorithm itselfâ€”like Gradient Descentâ€”has a preference for a certain type of solution, even when there\u0026rsquo;s no explicit regularization term in the loss function telling it to do so.\nThis leads us to the central question of our research: What is the implicit bias of the attention model? What kind of solution does it naturally favor?\nWe can get a clue from simpler models. It\u0026rsquo;s a known result from Soudry et al. (2018) that when you train a linear model with logistic loss using gradient descent, the solution tends to converge towards the one you\u0026rsquo;d get from a hard-margin Support Vector Machine (SVM). The goal of a hard-margin SVM is to find the hyperplane that maximizes the distance, or margin, to the nearest data points.\nSo, we formed a hypothesis: Could it be that the implicit bias of the much more complex attention model is also related to a hard-margin SVM solution?\nHere\u0026rsquo;s a core idea. We observe that attention has a tendency to focus on a small number of important tokens. We believe the principle guiding this selection is related to maximizing a margin. For instance, to distinguish between a cat and a dog, you might focus on the shape of the \u0026rsquo;ears\u0026rsquo; because that feature provides the clearest separation, the largest \u0026lsquo;margin,\u0026rsquo; between the two classes.\nThis brings us to the first main part of the analysis, where we explore the idea of global and local margin maximization within the attention mechanism.\nTo make the analysis tractable, we start with a simplified Softmax Attention model. The function f(X) takes an input sequence X and computes a final output. The key learnable parameters are p, the query embedding, and W, the key-query weights. In Transformers, p can be thought of as the embedding for the [CLS] token.\nOur training objective is to minimize the empirical risk for a binary classification task. We use a standard setup with input sequences X and labels Y, and a decreasing loss function like logistic loss. The model\u0026rsquo;s prediction f(Xi) is a function of the learnable parameters v, p, and W.\nHere, I\u0026rsquo;ll just quickly go over some of the mathematical notation we use throughout the paper. We use standard conventions for vectors and matrices, and we\u0026rsquo;ll use simplified notations like L(p) to denote the risk when v and W are fixed.\nOur approach involves studying the regularization path, which is the path of the solution p as we increase its allowed norm, R. This path mirrors the trajectory of gradient descent. We analyze a final attention model where the trainable parameters W and p jointly influence the softmax, leading to similar optimization dynamics.\nThis brings us to Lemma 1, which is a crucial simplification. It states that the weight matrix W is effectively a rank-1 matrix and its learning dynamics are completely determined by the vector p. This is very important because it means we can fix W and focus our entire analysis on the optimization of p.\nSo, we can formally define our problem. We are exploring the training risk L(v, p) where the key embeddings Ki are derived from the input Xi. For the analysis, we can even treat Xi and Ki as separate entities.\nTo ensure our proofs hold, we need Assumption A. This is a standard assumption that requires our loss function l to be well-behavedâ€”specifically, it must be strictly decreasing and have a Lipschitz-continuous derivative. Common functions like logistic loss and exponential loss satisfy this condition.\nNow, we introduce the central piece of our theory: a hard-margin SVM problem, which we call ATT-SVM. The goal here is to find a direction p that, for each input sequence, separates one chosen token (k_iÎ±i) from all other tokens (k_it) by a margin of at least 1. We will show that the optimization of softmax attention is effectively trying to solve this problem.\nTo formalize this, we define the score of a token (Î³_it) as its contribution to the final correct prediction. The optimal tokens are naturally the ones with the highest scores for each input. The Globally-Optimal Max-Margin (GMM) direction, denoted p^mm*, is then the solution to our ATT-SVM problem when we choose these optimal tokens.\nTheorem 1 is our first main result. It states that the regularization pathâ€”the sequence of solutions as we increase the norm Râ€”converges in direction to this GMM solution, p^mm*. In simple terms, as the optimization progresses and the norm of p grows, its direction aligns with the direction that best separates the highest-scoring tokens from the rest.\nTheorem 2 looks at the convergence of gradient descent directly. It says that under our assumptions, the norm of the weight vector p will grow towards infinity during training. Critically, for the simple case of a single training example (n=1), the direction of p converges to the GMM direction. However, for n \u0026gt; 1, it\u0026rsquo;s possible for the optimization to get trapped in a local minimum.\nTo test these theorems, we designed a simple experiment. We created synthetic data with three tokens, where we can visualize their key embeddings in 2D and independently control their scores using a third dimension. This lets us clearly see what\u0026rsquo;s happening.\nThese plots show the results. In Figure (a), where the conditions for global convergence are met, we see that all gradient descent trajectories (the grey arrows) correctly converge to the GMM direction (the red dashed line). In (b), we\u0026rsquo;ve changed the scores so that a locally optimal solution exists, and we see some trajectories get stuck there instead. Figure (c) shows the more complex scenario with multiple inputs, where finding the joint GMM solution is harder.\nThis brings us to the idea of locally-optimal tokens. These are tokens that might not be the highest-scoring ones globally, but they form a stable solution for the ATT-SVM problem. The direction associated with them is a locally-optimal max-margin (LMM) direction.\nTo talk about local convergence, we introduce the geometric concept of a cone. A cone around a direction q is simply the set of all vectors that are \u0026ldquo;close\u0026rdquo; in angle to q. The diagram illustrates an initialization p(0) that lies within the cone of an LMM direction p^mm.\nTheorem 3 formalizes local convergence. It states that if you initialize the gradient descent p(0) within the cone of an LMM direction, the optimization will stay within that cone and ultimately converge to that specific LMM direction.\nLooking back at our experiment in Figure (b), we can now understand it better through Theorem 3. The trajectories that don\u0026rsquo;t find the global optimum are the ones that were initialized inside the cone of the locally-optimal solution (the blue square), and as the theorem predicts, they converge to it.\nTheorem 4 provides a tightness guarantee. It essentially says that the LMM directions are the only stable convergence points. If you start in any direction q that is not an LMM direction, the optimization path will eventually move away from q.\nThis diagram illustrates Theorem 4. If our initial parameter p(0) is in a cone (the grey one) that does not contain any LMM or GMM direction, the optimization process will not converge in that direction. It is implicitly forced to seek out one of the valid max-margin solutions.\nTo summarize the first half: we\u0026rsquo;ve shown that when training only the attention parameter p, the implicit bias of gradient descent drives the solution towards a hard-margin SVM that separates tokens. Now, what happens if we also learn the prediction head v at the same time?\nThis leads to the second part of our analysis: the joint convergence of the prediction head v and the attention weights p.\nThe high-level intuition is that the model is linear in v, so the optimization with respect to v also has an implicit bias towards a standard max-margin classifier solution. The features for this classifier, x_i^p, are the outputs of the attention layer.\nThe challenge here is that the features r that v sees are determined by p, which itself is changing during training. This creates a coupled dynamic. We analyze this by separating the problem into cases based on whether the selected features are support vectors for the v-classifier or not.\nTheorem 5 addresses the case where we assume the selected tokens are all support vectors. We introduce Assumption C, which formalizes that if the attention doesn\u0026rsquo;t perfectly select the optimal token, the classification margin for v shrinks. Under this assumption, we find that v converges to the direction of a standard SVM solution, and p converges to the direction of our ATT-SVM solution.\nThis raises a natural question: Does every selected token really need to be a support vector, sitting right on the margin? That seems like a very strict condition. Intuitively, for the data points that are not support vectors, we don\u0026rsquo;t need to maximize their margin. We just need to make sure they are on the correct side of the decision boundary. This allows us to define a \u0026ldquo;relaxed\u0026rdquo; version of our ATT-SVM problem.\nTheorem 6 presents this more general result. The ATT-SVM problem is relaxed: for tokens corresponding to support vectors (i âˆˆ S), we require a margin of 1, but for non-support vectors (i âˆˆ S-bar), we only require a margin of 0 (correct classification). We only need Assumption C to hold for the support vectors. The result is that v still converges to the max-margin v^mm, and p converges to the solution of this new, relaxed SVM problem, p^relax.\nWe ran experiments for this joint convergence case as well. The plots show the trajectories for the attention weights p and the classifier head v. Plot (a) shows a case aligning with Theorem 5, where all inputs are support vectors. Plot (b) shows a case for Theorem 6, where one input is not a support vector. Plot (c) shows that the softmax probability for the optimal token quickly goes to 1, confirming that the attention is learning to select specific tokens as predicted.\nSo, to summarize the entire theoretical part: Whether we train the attention weights p alone or jointly with the classifier head v, the implicit bias of the optimization consistently pushes the model towards a hard-margin SVM solution. This provides a strong theoretical explanation for why attention learns to become sparse and focus on the most discriminative tokens.\nFinally, let\u0026rsquo;s look at a few more experiments to see these principles in action on more complex tasks.\nThis slide provides an experimental comparison between Normalized Gradient Descent (GD) and Vanilla GD, validating the paper\u0026rsquo;s theory. Plot (a) shows that while both methods learn to focus on a single token (softmax probability approaches 1.0), Normalized GD achieves this sparse attention much faster. Plot (b) demonstrates a key theoretical prediction: the norm of the attention weights ||p|| diverges (grows linearly without bound) when using Normalized GD. This behavior is characteristic of an optimization process seeking a max-margin solution.\nHere, we trained a vision transformer on an image classification task. Figure 7 shows that as training progresses over epochs, the sparsity of the attention map increases (red curve goes down), meaning the model learns to focus on fewer, more important image patches. At the same time, the norm of the attention weights ||W|| steadily increases (blue curve), which is exactly what our theory predicts will happen as the solution converges towards an infinite-norm, max-margin boundary. The images in Figure 6 visually show this focusing effect over time.\nThis final experiment shows how the choice of loss function affects the dynamics. We compare a correlation loss (l(x) = -x) with the logistic loss. The gradient\u0026rsquo;s magnitude depends on the token\u0026rsquo;s score Î³ differently for each loss. For correlation loss, larger scores get larger gradients, while for logistic loss, the gradient is largest for scores near zero. This results in different trajectories, but as you can see, both are ultimately guided by the same underlying max-margin principle, pushing towards a separating hyperplane.\n","permalink":"https://mookjsi.github.io/posts/paper-review-maxtoken/","summary":"This post reviews the NeurIPS 2025 paper \u0026lsquo;Max-Margin Token Selection in Attention Mechanism.\u0026rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs.","title":"Max-Margin Token Selection in Attention Mechanism"},{"content":"This is my detailed slide-by-slide analysis of the paper Stochastic Approximation to Contrastive Learning, which was submitted to the ICLR 2025 conference.\nThe paper tackles a critical challenge in contrastive learning: its heavy reliance on large batch sizes and the associated computational cost. The authors introduce SACLR, a novel framework inspired by Stochastic Cluster Embedding (SCE) that reformulates the objective using I-divergence. The goal was to enable efficient training with as little as one negative sample, a significant departure from methods like SimCLR.\nWhile the premise is compelling, the paper faced substantial criticism during the open review process regarding its experimental comparisons, novelty, and the substantiation of its claims. Despite rebuttals and additional experiments, it was ultimately rejected. In this review, I\u0026rsquo;ll walk through the method as presented and layer in the context from the public reviews to provide a complete picture of its strengths and weaknesses.\nThis is the title slide for my review of the paper \u0026ldquo;Stochastic Approximation to Contrastive Learning.\u0026rdquo; This presentation was prepared for the Information Theory and Machine Learning Lab at Yonsei University.\nTo begin, I\u0026rsquo;ll recap the core concept of Representation Learning. This field is all about how we represent data, which leads to the fundamental question: what exactly is a \u0026ldquo;representation\u0026rdquo; in the context of machine learning?\nI\u0026rsquo;ll illustrate this with a simple task. Imagine you need to solve the division problem CCV / VI. For most people, this is not immediately obvious.\nNow, consider this problem: 210 / 6. This is likely much easier, and you can quickly determine the answer is 35. The interesting part is that both problems represent the exact same calculation.\nThe key difference lies in the representation of the same numerical information. The first task used Roman numerals, while the second used Arabic numerals. The choice of representation dramatically changes the difficulty of the task.\nThis analogy illustrates a critical point that is central to representation learning: the right representation can make a complex task much easier to solve.\nTo summarize this introductory point: the difficulty of many information processing tasks is highly dependent on how that information is represented.\nSo, the crucial question for us is: how can we obtain a \u0026ldquo;good representation\u0026rdquo; for various machine learning tasks? Today, I\u0026rsquo;ll focus on one prominent approach for achieving this: Self-Supervised Learning.\nSelf-Supervised Learning is a subset of the broader field of Representation Learning.\nA primary motivation for the development of self-supervised methods is the immense cost and effort required to obtain large-scale labeled datasets for traditional supervised learning.\nThe solution that Self-Supervised Learning proposes is to find a way to learn a \u0026ldquo;good representation\u0026rdquo; that captures the essential features of the data using only an unlabeled dataset.\nThis slide illustrates the typical self-supervised learning pipeline. First, a model is trained on a \u0026ldquo;pretext task\u0026rdquo; using a large amount of unlabeled data. The learned model is then transferred and fine-tuned on a \u0026ldquo;downstream task\u0026rdquo; using task-specific (and often limited) labeled data.\nThe goal of the pre-training stage is to transform raw data, which can be a poor representation for a computer (like a raw image of a dog), into a feature vector that is a much better representation for downstream tasks.\nThe central question this paper investigates is the pre-training step: how exactly does the model learn a good representation from unlabeled data?\nWithin the realm of Self-Supervised Learning, I will now narrow the focus to Contrastive Learning. This approach is based on making \u0026ldquo;inter-sample\u0026rdquo; predictions.\nHere is the core mechanic of contrastive learning. We start with an \u0026ldquo;anchor\u0026rdquo; image. We create two different augmented versions, or \u0026ldquo;views,\u0026rdquo; of this anchor, which form a \u0026ldquo;positive pair.\u0026rdquo; We then contrast this with a \u0026ldquo;negative pair,\u0026rdquo; which is formed by the anchor and a view from a completely different image.\nIn summary, the objective of contrastive learning is to learn effective representations by mapping similar data points close to each other in the representation space, while simultaneously pushing dissimilar data points far apart.\nThis slide outlines the flow of the main arguments in this review. I\u0026rsquo;ll begin by discussing the high computational costs that arise from the conventional definition of positive and negative pairs. Then, I will introduce the paper\u0026rsquo;s proposed method, which uses matrix approximation with I-divergence to create a decomposable and stochastic loss, ultimately achieving competitive results with a low batch size and fewer negative pairs.\nNow, we move from the recap to the main introduction of the paper\u0026rsquo;s contribution.\nThe core problem is that while supervised learning is effective, it depends on having extensive labeled data. Self-Supervised Learning is the alternative we are exploring.\nHowever, popular contrastive learning methods like SimCLR require very large batch sizes to ensure a sufficient balance of positive and negative examples. This expends a large amount of computational resources, particularly on the negative pairs. A method called Sog-CLR attempted to address this by mixing an EMA of image similarities into the denominator of the InfoNCE loss.\nTo be more specific, SimCLR\u0026rsquo;s InfoNCE loss operates in a full-batch mode, which is not decomposable in a mini-batch setting. Sog-CLR showed improvement by incorporating a running average for the negative pair estimations.\nThis paper poses the question: even with Sog-CLR\u0026rsquo;s improvements, is there still room for further optimization? It introduces SACLR, a method that uses I-divergence to reformulate the objective into a matrix approximation problem that is decomposable across instance pairs.\nThe key idea behind SACLR is this reformulation using I-divergence, which allows the objective to be decomposed and approximated stochastically. This is presented as an advancement over Sog-CLR\u0026rsquo;s EMA mixing approach.\nThe paper\u0026rsquo;s central claim is that its method, SACLR, can learn high-quality representations with a small batch size and very few negative pairs. While reviewers found the core idea of tackling the large batch size problem to be an interesting and valuable contribution, and saw the novel formulation inspired by Stochastic Cluster Embedding as a strength, they heavily scrutinized the experimental evidence supporting these claims.\nTo understand SACLR, we first need to look at its theoretical foundation: Stochastic Cluster Embedding (SCE). I will now detail the matrix approximation with I-divergence that underpins the method.\nIn SCE, we have embedded data points, like $y_i$ and $y_j$. We define a similarity kernel $q_{ij}$ between these points in the embedded space, typically using a Gaussian or a Student\u0026rsquo;s t-distribution kernel. This value should be close to 1 for similar points.\nWe also define a target similarity matrix, P, which represents the desired similarities in the embedded space. For example, perfectly clustered data would have a block-diagonal P matrix.\nThe goal is to make the learned similarity matrix Q as close as possible to our desirable target matrix P. The question is, how do we measure this closeness?\nThis is where the choice of divergence metric is crucial. t-SNE uses the KL-divergence. In contrast, SCE uses the I-divergence, which includes an additional scaling factor \u0026rsquo;s\u0026rsquo; and linear terms.\nThis slide highlights the formulas for both KL-divergence, used in t-SNE, and the I-divergence with a scaling factor, used in SCE.\nAn important property is that the I-divergence can reduce to the KL-divergence. This happens if the scaling factor \u0026rsquo;s\u0026rsquo; is set to be the inverse of the sum of all kernel similarities, effectively normalizing the Q matrix.\nSCE defines the scaling factor \u0026rsquo;s\u0026rsquo; using a weighted sum controlled by the parameter Î±, which introduces additional repulsion to improve cluster quality. A key methodological point, raised by Reviewer C43T, was why \u0026rsquo;s\u0026rsquo; is treated as a constant during optimization when it is a function of the embeddings. The authors clarified that they use an interleaving optimization strategy: the model parameters are optimized while \u0026rsquo;s\u0026rsquo; is fixed, and then \u0026rsquo;s\u0026rsquo; is periodically updated based on the new embeddings.\nBy plugging this definition of the weights $w_{ij}$ into the formula for \u0026rsquo;s\u0026rsquo;, the denominator term can be rewritten as a sum of two expectations.\nThese two expectations have clear interpretations: $E_1$ is the expected similarity for pairs drawn from the target distribution P, while $E_2$ is the expected similarity for pairs drawn uniformly at random from all possible pairs.\nThis formulation allows the I-divergence objective to be rewritten in a stochastic form, separating it into an attraction term based on the target distribution and a repulsion term based on the uniform distribution. The terms themselves are simple functions of the similarity $q_{ij}$.\nNow, let\u0026rsquo;s apply this SCE framework to Contrastive Learning. In our setting, for each input $x_i$, we generate two augmented views, $\\tilde{x}{i}^{(1)}$ and $\\tilde{x}{i}^{(2)}$. We denote their embeddings as $\\tilde{y}{i}^{(u)}$ and the similarity between any two embeddings as $q{ij}^{(u,v)}$.\nThe goal in contrastive learning is to make embeddings from the same instance ($i=j$) similar, and embeddings from different instances ($i \\ne j$) dissimilar. We can formally define this as a target tensor $p$, where the target similarity is 1 only for positive pairs ($p_{ii}^{(1,2)}$ and $p_{ii}^{(2,1)}$) and 0 for all other pairs.\nThis slide provides a concrete example of the target tensor $P$ for a case with N=3 instances. The matrices for same-view similarities ($P^{(1,1)}, P^{(2,2)}$) are all zeros, while the matrices for cross-view similarities ($P^{(1,2)}, P^{(2,1)}$) are identity matrices, capturing the positive pair targets.\nTo simplify the math, we can flatten this four-dimensional tensor structure into standard 2D matrices. The $2N \\times 2N$ target matrix $\\psi$ is formed by reorganizing the elements of the tensor $p$. This results in a sparse matrix where the identity matrices from the tensor become off-diagonal blocks.\nFor notational simplicity, we neglect the diagonal elements of the matrices $\\psi$ and $\\phi$ in the approximation, as they are constant for the kernels used and do not affect the optimization.\nNow we apply the SCE I-divergence formula, but to our new flattened matrices $\\psi$ and $\\phi$ which represent the contrastive learning problem.\nApplying the I-divergence $D_{I}(\\psi||s\\phi)$ and using the specific structure of our target matrix $\\psi$ (which is mostly zeros), the loss function simplifies significantly. The scaling factor $s$ is updated periodically, with its weights $w_{ij}^{u,v}$ also adapted for the contrastive case.\nThe full loss function $\\mathcal{L}{CLR}(\\theta)$ can be expressed as an expectation over the data indices. This form consists of a term for positive pairs ($log~q{ii}^{1,2}$) and a repulsion term involving a sum of similarities over all pairs.\nTo make this computationally feasible, we use a Monte Carlo approximation. This gives us the final SACLR objective, $\\mathcal{L}{SACLR}(\\theta)$, which is calculated over a mini-batch $\\mathcal{B}$ and a set of M negative samples $\\mathcal{M}{i}$. The paper studies two variants: SACLR-1 (M=1) and SACLR-all (M=B).\nThe scaling factor $s$ is also estimated stochastically. Its inverse, $s^{-1}$, is approximated using samples from the mini-batch, and this estimate is updated smoothly using an exponential moving average (EMA) after each batch.\nThe paper also explores a row-wise decomposition of the matrix approximation. Instead of one global scaling factor \u0026rsquo;s\u0026rsquo;, each row \u0026lsquo;a\u0026rsquo; of the similarity matrix gets its own scaling factor $s_a$. This results in the loss function $\\mathcal{L}_{CLR-row}(\\theta)$.\nThis slide provides proof for the simplified row-wise SACLR loss function. By substituting the sparse target matrix $\\psi$ into the general I-divergence formula and summing over all rows, we arrive at the expression shown.\nA key theoretical finding presented in the paper is that under specific conditions, the SACLR-row objective becomes equivalent to the SimCLR loss. This link, however, became a point of discussion during the review. Reviewer gz9D argued that this equivalence doesn\u0026rsquo;t explain why SACLR should be expected to outperform SimCLR. The authors countered that the method\u0026rsquo;s advantage comes not from this specific condition, but from using a different, more flexible weighting scheme for the scaling factor.\nHere is the proof of Theorem 3.1. By substituting the condition for the scaling factors into the loss function, the repulsion term simplifies to a constant, and the remaining terms can be rearranged to form precisely the SimCLR objective.\nJust as with the full matrix version, the row-wise loss can be approximated stochastically for mini-batch training. This gives us the $\\mathcal{L}_{SACLR-row}(\\theta)$ objective.\nSimilarly, the row-wise scaling factors $s_{2(i-1)+u}$ are estimated stochastically within each mini-batch and updated using an EMA rule.\nThis diagram provides a summary of the theoretical framework I have just presented. We started with the concept of I-divergence and a scaling factor, which we used to build a matrix approximation. This was then decomposed row-wise, and both versions were made practical via stochastic approximation. Now, it\u0026rsquo;s time to see the experimental results.\nThis slide presents the full pseudocode for the SACLR algorithm. A major point of contention in the initial review was that the paper claimed to be \u0026ldquo;more computationally efficient\u0026rdquo; without providing empirical data on runtime or memory. The detailed ablation studies on computational cost, shown later in the presentation, were added during the rebuttal period as a direct response to this criticism from the reviewers.\nNow we move to the experiments section. The standard evaluation protocol for self-supervised methods is used: first, a model is pre-trained without labels on a dataset like ImageNet or CIFAR. The learned weights from this backbone are then used to initialize a new network, a linear layer is added, and this new network is trained with labels to perform a classification task.\nTable 1 shows the Top-1 linear classification accuracies on ImageNet. While SACLR-ALL is shown to be competitive with some methods like MoCo v2, this comparison drew significant criticism during the review process. The Area Chair and multiple reviewers stated that the baseline methods used for comparison were \u0026ldquo;weak\u0026rdquo; and the performance gains \u0026ldquo;marginal,\u0026rdquo; noting that many stronger, more recent methods were omitted.\nThis table shows results for longer training, with SACLR-MIX surpassing SimCLR and SogCLR in this setup. However, reviewers found this comparison inadequate as well. Reviewer gz9D pointed out that results for top-performing methods like VICReg and Barlow Twins from other benchmark papers were significantly higher. The authors argued their goal was primarily efficiency without heavy tuning, but this did not overcome the concerns about the weak comparison set.\nThis experiment in Table 3 specifically investigates performance when using only a single negative sample (M=1) per image. SACLR-1 achieves a Top-1 accuracy of 65.3%, significantly outperforming classical contrastive losses like Triplet and Logistic loss under the same constraint.\nThis table evaluates semi-supervised learning performance, showing SACLR\u0026rsquo;s strength in data-limited regimes. It is worth noting that the initial submission focused almost exclusively on linear evaluation. Reviewers Tx4K and gz9D strongly recommended including more comprehensive evaluations like semi-supervised fine-tuning to provide a more complete picture, and the additional kNN and fine-tuning results were added to the paper in response.\nThe learned representations are also evaluated on transfer learning classification tasks. As shown in Table 5, the representations learned by SACLR-ALL and SACLR-MIX transfer very well to other datasets like VOC07 and especially iNaturalist18, where they significantly outperform SimCLR and MoCo v2.\nTable 6 shows transfer learning results on more complex downstream tasks: object detection and segmentation on VOC and COCO. Across all metrics, the SACLR variants are highly competitive and often outperform strong baselines like SimCLR, BYOL, and MoCo v2, with SACLR-MIX showing the strongest results overall.\nThis table provides a direct comparison with other stochastic estimation-based contrastive methods, using an architecture similar to the iSog-CLR paper for a fair comparison. The results on CIFAR10, CIFAR100, and ImageNet100 show that both the matrix and row versions of SACLR are highly competitive, often achieving the best or second-best performance in this specific class of methods.\nThis slide presents several ablation studies on computational complexity and robustness. These experiments were largely added in response to direct reviewer feedback. Reviewers requested empirical data on runtime and memory to substantiate the paper\u0026rsquo;s efficiency claims, as well as a study on the impact of batch size to support the claim that SACLR performs well in small-batch settings. These tables represent the authors\u0026rsquo; attempt to provide that missing evidence.\n","permalink":"https://mookjsi.github.io/posts/paper-review-stochastic/","summary":"This post reviews \u0026lsquo;Stochastic Approximation to Contrastive Learning,\u0026rsquo; a paper submitted to ICLR 2025. While ultimately rejected, the paper proposed an interesting approach to make contrastive learning more efficient. I\u0026rsquo;ll break down its core ideas, the community\u0026rsquo;s feedback, and why it fell short.","title":"Stochastic Approximation to Contrastive Learning"},{"content":"This is a detailed slide-by-slide review of the paper When to Retrieve?, which was submitted to ACL 2024 but was ultimately rejected.\nThe paper questions the efficiency of the standard Retrieval-Augmented Generation (RAG) framework, which naively performs retrieval for every query. The authors propose an adaptive retrieval model (RET indicator) that dynamically decides, for each query, whether external knowledge retrieval is necessary. If the LLM\u0026rsquo;s parametric memory is sufficient, retrieval is skipped; otherwise, external information is fetched.\nThis is the title slide for my presentation on the paper \u0026ldquo;When to Retrieve?\u0026rdquo;, which I delivered at the Yonsei University Machine Learning Lab on February 28, 2025.\nTo set the stage, I\u0026rsquo;m introducing a real-world application I developed, \u0026ldquo;momugo,\u0026rdquo; a restaurant recommendation app. This slide shows the user input screen, where a user is asking for a recommendation for a quiet place to talk with a friend while enjoying soju and hot fish cake soup.\nThis slide demonstrates the detailed output of the \u0026ldquo;momugo\u0026rdquo; app. When a user clicks on a recommended restaurant, it displays more information, including relevant review snippets that match the user\u0026rsquo;s query, providing a justification for the recommendation.\nHere, I\u0026rsquo;m breaking down the initial data filtering process in my application. The system first performs a primary filtering based on structured data like category and business hours. Then, it moves to a more detailed secondary filtering based on the specifics of the user\u0026rsquo;s natural language query.\nThis slide illustrates the core retrieval and generation pipeline. After the initial filtering, the user\u0026rsquo;s query is used to find the top 10 most similar review embeddings from a vector database. The top 3 restaurants are then selected, and an LLM generates a descriptive recommendation reason for each.\nI\u0026rsquo;m detailing the \u0026ldquo;Detailed Filtering\u0026rdquo; step. An LLM is prompted with a system message to analyze the user\u0026rsquo;s query and extract specific requirements, such as \u0026ldquo;corkage available\u0026rdquo; or \u0026ldquo;pet-friendly,\u0026rdquo; into a structured JSON format.\nThis slide provides a high-level overview, separating the entire process into two main phases: Retrieval and Generation. The retrieval part finds the most relevant restaurants, and the generation part creates the user-facing explanation.\nI\u0026rsquo;m explaining the embedding process for the retrieval phase. The descriptions and latest reviews for each restaurant are converted into a 1536-dimensional vector using OpenAI\u0026rsquo;s text-embedding-3-small model and stored in a Pinecone vector database.\nThis slide details the retrieval mechanism. We embed all reviews and the user\u0026rsquo;s query. Using LangChain, we retrieve the top 10 review vectors based on cosine similarity. Finally, we determine the top 3 restaurants by calculating the average vector similarity for each.\nThis slide outlines the generation phase. The top 3 restaurants identified during retrieval, along with their metadata and the original user query, are used to create three separate prompts for the LLM.\nI\u0026rsquo;m showcasing the prompt engineering aspect for the generation phase. A structured prompt, including a system message and a human message with the query and restaurant context, is fed to the LLM to generate a tailored recommendation reason for each of the top 3 restaurants.\nThis slide concludes the overview of my application\u0026rsquo;s architecture. It shows how the generated recommendation reasons (from the LLM) and the filtered restaurant data are combined to produce the final, detailed output card shown to the user.\nShifting from my personal project to the main topic, this slide introduces the standard Retrieval-Augmented Generation (RAG) process, breaking it down into the \u0026lsquo;Retrieval\u0026rsquo; and \u0026lsquo;Generation\u0026rsquo; stages.\nHere, I pose a critical question about the standard RAG framework: Is the naive approach of always retrieving information for every query the most effective or efficient method?\nThis slide frames the core problem investigated in the paper. I\u0026rsquo;m questioning the fundamental assumption of RAG: \u0026ldquo;Should we always retrieve, regardless of the query?\u0026rdquo; This sets the stage for a more nuanced approach.\nI\u0026rsquo;m introducing the central concept of the paper: an adaptive retrieval model. The key idea is to use an indicator, which the authors call \u0026ldquo;RET,\u0026rdquo; to decide whether to retrieve external information or to answer directly from the LLM\u0026rsquo;s parametric memory.\nNow that we\u0026rsquo;ve established the need for a decision mechanism, the key research question becomes: \u0026ldquo;Where to place a RET?\u0026rdquo; In other words, how does the model learn when it\u0026rsquo;s appropriate to trigger the retrieval step?\nThis slide presents the core hypothesis of the paper. The authors believe that retrieval is unnecessary for \u0026ldquo;popular entities\u0026rdquo; (information likely stored in the LLM\u0026rsquo;s parameters) but crucial for \u0026ldquo;non-popular entities.\u0026rdquo;\nBuilding on the hypothesis, I\u0026rsquo;m highlighting the practical challenge: How can a model quantitatively identify if an entity is \u0026ldquo;popular\u0026rdquo;? The paper proposes a model that can learn this distinction to enhance the efficiency and accuracy of the RAG process.\nThis slide formally introduces the paper I\u0026rsquo;m reviewing: \u0026ldquo;When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\u0026rdquo; by Labruna, Campos, and Azkune. They name their proposed model \u0026ldquo;ADAPT-LLM.\u0026rdquo;\nI\u0026rsquo;m providing context with related works. This slide contrasts \u0026ldquo;Closed-book QA,\u0026rdquo; which relies solely on an LLM\u0026rsquo;s internal knowledge, with \u0026ldquo;Open-book QA,\u0026rdquo; which always uses an external information retrieval system.\nThis slide summarizes key issues in the field and their corresponding solutions. It covers the high cost of retraining models, the lexical gap in keyword-based retrieval, and the latency costs associated with retrieval, positioning an adaptive approach as a solution.\nContinuing with the related works, this slide discusses the problem of tool overuse in models like Toolformer and the unrealistic nature of previous popularity-based retrieval methods. It positions ADAPT-LLM as a solution that provides a robust baseline for comparison.\nHere, I\u0026rsquo;m outlining the four-step process of the ADAPT-LLM framework: 1) A query is sent to the model. 2) The model decides whether to retrieve or not. 3) If needed, it retrieves information. 4) It generates the final answer.\nThis slide focuses on the critical decision-making step. The central question is how the model determines whether to retrieve information. The paper\u0026rsquo;s answer is to fine-tune the LLM on a specially curated dataset to learn this behavior.\nI\u0026rsquo;m illustrating the data preparation process for fine-tuning ADAPT-LLM. The process starts with QA data, which is fed to a base LLM. The model\u0026rsquo;s answers are classified as correct or incorrect, which then informs the creation of different types of training prompts.\nThis slide provides a concrete example of the data preparation pipeline. Using a QA pair about the capital of South Korea, I show how the query is extracted from the source data.\nContinuing the example, this slide shows the base LLM generating both a correct answer (\u0026ldquo;Seoul\u0026rdquo;) and an incorrect answer (\u0026ldquo;Busan\u0026rdquo;) to the query, which is a crucial step for creating the fine-tuning dataset.\nThis slide details how the fine-tuning dataset, named DS_adapt, is constructed. Based on the LLM\u0026rsquo;s correctness, three types of prompts are generated: a direct answer for correct parametric knowledge, a \u0026lt;RET\u0026gt; token for incorrect answers, and a context-based prompt for incorrect answers.\nThis slide visualizes the final step of the model creation process. The DS_adapt dataset, with its varied prompt structures, is used to fine-tune a base LLM, resulting in the final ADAPT-LLM.\nI\u0026rsquo;m presenting the headline results from the paper. The table shows that ADAPT-LLM, trained on both NQ and SQuAD datasets, outperforms both the NEVER RETRIEVE (NR-LLM) and ALWAYS RETRIEVE (AR-LLM) baselines in terms of accuracy on the PopQA test set.\nThis slide outlines the structure of the experiments section of my review. I will cover three key areas: comparison to baselines, the model\u0026rsquo;s ability to determine when context is needed, and a comparison with the state-of-the-art approach for the PopQA dataset.\nI\u0026rsquo;m beginning the detailed experimental setup. This slide specifies the training datasets used for the fine-tuning process: Natural Questions (NQ) and Stanford Question Answering Dataset (SQuAD).\nThis slide specifies the base Large Language Model used in the experiments. The authors chose the Llama-2-7B model as the foundation for their fine-tuning.\nHere, I specify the dataset used for evaluating the models at test time. All configurations are evaluated on the PopQA dataset, which is designed to test knowledge of popular entities.\nI\u0026rsquo;m explaining how the baseline models are created for comparison. The NR-LLM (NEVER RETRIEVE) is fine-tuned only on prompts that require direct, parametric answers.\nSimilarly, this slide explains the creation of the AR-LLM (ALWAYS RETRIEVE) baseline. This model is fine-tuned exclusively on prompts that include retrieved context, forcing it to always rely on external information.\nThis slide recaps the training process for the main model, ADAPT-LLM. It is trained on the comprehensive DS_adapt dataset, which includes a mix of parametric, retrieval-triggering, and context-aware prompts.\nI\u0026rsquo;m presenting the main results table again, this time to emphasize the direct performance comparison. ADAPT-LLM consistently achieves the highest accuracy, demonstrating the effectiveness of its selective retrieval strategy.\nTo provide more context on the datasets, this slide presents a table comparing the statistics of NQ, SQuAD, and PopQA, including the number of questions and the average length of questions and answers.\nNow, I\u0026rsquo;m moving to the second part of the experimental analysis: evaluating ADAPT-LLM\u0026rsquo;s ability to correctly decide when to retrieve. This slide sets up the analysis of the model\u0026rsquo;s decision-making accuracy.\nThis slide presents a detailed breakdown of ADAPT-LLM\u0026rsquo;s performance. It analyzes the accuracy of the model in four scenarios: when it correctly decides to retrieve (and is given context), when it incorrectly decides not to retrieve (and isn\u0026rsquo;t given context), and so on.\nI\u0026rsquo;m highlighting a key observation from the results table. The accuracy for questions where the model chose to retrieve (Acc. w/ context for (RET)) seems quite low, around 33%. This prompts a deeper investigation.\nThis slide provides an explanation for the previously noted low accuracy. The authors point out that the performance of the underlying Information Retrieval (IR) system itself was a limiting factor.\nTo support the claim about poor IR performance, this slide presents results from Table 4 of the paper. It shows a massive accuracy gap when using the gold standard passages versus passages retrieved by the Contriever system, confirming the IR bottleneck.\nDespite the issues with the IR component, this slide shows that the model\u0026rsquo;s decision-making process is sound. The histograms show a clear inverse correlation between entity popularity and the usage of the \u0026lt;RET\u0026gt; token, confirming the model learned the core hypothesis.\nI\u0026rsquo;m now moving to the final experimental comparison: ADAPT-LLM versus the previous state-of-the-art method for the PopQA dataset. This slide visually contrasts the two approaches, highlighting differences in training data and thresholding methods.\nThis slide presents the results of the state-of-the-art comparison. While the accuracy is comparable, I\u0026rsquo;m highlighting the authors\u0026rsquo; claims that ADAPT-LLM is a more generalizable and efficient approach due to its lower reliance on the IR system and its independence from dataset-specific features like popularity scores.\nTo begin the conclusion, I\u0026rsquo;m bringing back the diagram illustrating the creation of the DS_adapt fine-tuning dataset. This serves as a reminder of the core technical contribution of the paper.\nThis slide recaps the fine-tuning process itself, showing how the diverse set of prompts from DS_adapt is used to train the base LLM into the final, adaptive ADAPT-LLM.\nIn my final slide, I\u0026rsquo;m summarizing the key takeaways from the paper. The experiments demonstrate that ADAPT-LLM is a robust model that successfully learns when to retrieve information, outperforming standard baselines and showing strong potential as a general, efficient approach to adaptive RAG.\n","permalink":"https://mookjsi.github.io/posts/paper-review-whentoretrieve/","summary":"This post reviews \u0026lsquo;When to Retrieve?\u0026rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community\u0026rsquo;s feedback, and the reasons it was not accepted.","title":"When to Retrieve?"},{"content":"Education Yonsei University, Seoul, Korea Senior @ Dept. of Applied Statistics GPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics) Research Interests Statistical Machine Learning Large Language Model Retrieval Method Research Experience Undergraduate Researcher, ITML @ Yonsei (Jan. 2025 â€“ Jun. 2025) Conducting undergraduate research under the supervision of Prof. Jy-yong Sohn. Engaged in ongoing projects related to RAG. Skills Programming: R, Python, Frontend, Git, MATLAB Languages: Korean (Native), English (Intermediate), Chinese (Beginner) ","permalink":"https://mookjsi.github.io/about/","summary":"\u003ch2 id=\"education\"\u003eEducation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eYonsei University\u003c/strong\u003e, Seoul, Korea\n\u003cul\u003e\n\u003cli\u003eSenior @ Dept. of Applied Statistics\u003c/li\u003e\n\u003cli\u003eGPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-interests\"\u003eResearch Interests\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical Machine Learning\u003c/li\u003e\n\u003cli\u003eLarge Language Model\u003c/li\u003e\n\u003cli\u003eRetrieval Method\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-experience\"\u003eResearch Experience\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUndergraduate Researcher\u003c/strong\u003e, ITML @ Yonsei (Jan. 2025 â€“ Jun. 2025)\n\u003cul\u003e\n\u003cli\u003eConducting undergraduate research under the supervision of Prof. Jy-yong Sohn.\u003c/li\u003e\n\u003cli\u003eEngaged in ongoing projects related to RAG.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming:\u003c/strong\u003e R, Python, Frontend, Git, MATLAB\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLanguages:\u003c/strong\u003e Korean (Native), English (Intermediate), Chinese (Beginner)\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"}]