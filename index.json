[{"content":"\nSlide 1: Title Slide Title: Advanced Policy Gradient Methods: TRPO \u0026amp; PPO Session: YBIGTA SUMMER SESSION Presenter: DS 26 Jungmook Kang Date: 2025.08.26 Slide 2-5: Why does Reinforcement Learning sometimes fail? Main Question: \u0026ldquo;Why does reinforcement learning sometimes break down?\u0026rdquo; Problem: Basic policy gradient methods (like A2C) have limitations. Making drastic changes to the policy—the agent\u0026rsquo;s decision-making criteria—can cause significant problems. Key Issues: Unstable Learning: If an update step is too large and in the wrong direction, the agent\u0026rsquo;s performance can collapse, making recovery impossible. This is more damaging than in supervised learning because a bad policy affects the distribution of states and rewards the agent will see in the future. Changing Data Distribution: As the policy changes, the data (states, actions, rewards) it collects also changes. This non-stationarity of the input data makes learning difficult. Example Analogy: A robot learning to walk. If it\u0026rsquo;s walking well and tries a new strategy by swinging its legs too wide, it will fall. Once it has fallen, it can no longer collect useful data about walking, and learning stops. Slide 6-8: What is TRPO? \u0026ldquo;Let\u0026rsquo;s not change the policy too much!\u0026rdquo; Concept: Trust Region Policy Optimization (TRPO). Core Idea: When updating the policy, TRPO establishes a \u0026ldquo;trust region\u0026rdquo; or a \u0026ldquo;safe zone\u0026rdquo; to ensure the new policy does not stray too far from the old one. Goal: \u0026ldquo;Maximize policy performance under the constraint that the difference (distance) between the old and new policies does not exceed a certain value, δ.\u0026rdquo; This prevents the destructive, large policy updates that can lead to performance collapse. Slide 9-11: The Mathematical Expression of TRPO This slide introduces the core optimization problem of TRPO. The goal is to maximize an objective function, subject to a constraint.\nObjective Function (What to Maximize):\nThe expression $\\mathbb{E}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}\\hat{A}]$ represents the expected advantage of the new policy $\\pi_{\\theta}$ relative to the old policy $\\pi_{\\theta_{old}}$. This is a surrogate objective that uses importance sampling to estimate the performance of the new policy using data collected from the old one. The term $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ is the importance sampling ratio. The full objective function is: $$L_{\\theta_{old}}^{IS}(\\theta)=\\hat{\\mathbb{E}}{t}[\\frac{\\pi{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|S_{t})}\\hat{A}_{t}]$$ Constraint (The \u0026ldquo;Trust Region\u0026rdquo;):\nThe term $\\mathbb{E}[D_{KL}(\\pi_{\\theta_{old}}||\\pi_{\\theta})]\\le\\delta$ is the constraint that defines the trust region. $D_{KL}$ stands for Kullback-Leibler (KL) Divergence. It\u0026rsquo;s a way to measure the \u0026ldquo;distance\u0026rdquo; or difference between two probability distributions. Here, it measures how much the new policy $\\pi_{\\theta}$ has changed from the old policy $\\pi_{\\theta_{old}}$. By keeping this value less than a small constant $\\delta$, we ensure the policy update is small and stays within a \u0026ldquo;trusted\u0026rdquo; region where the performance estimate is reliable. The Complete TRPO Problem: The full optimization problem solved by TRPO is: $$ \\underset{\\theta}{\\text{maximize}} \\quad \\hat{\\mathbb{E}}{t}[\\frac{\\pi{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}\\hat{A}{t}] $$ $$ \\text{subject to} \\quad \\hat{\\mathbb{E}}{t}[KL[\\pi_{\\theta_{old}}(\\cdot|s_{t}),\\pi_{\\theta}(\\cdot|s_{t})]]\\le\\delta $$\nSlide 12: Limitations of TRPO Problem: While TRPO\u0026rsquo;s core idea was groundbreaking for stabilizing RL, it wasn\u0026rsquo;t widely adopted in practice. Major Drawbacks: High Computational Cost: It uses a complex, second-order optimization technique called the \u0026ldquo;conjugate gradient method\u0026rdquo; which is computationally heavy. Difficult to Implement: The algorithm is complex to code and debug. Poor Performance on Certain Tasks: It struggled with tasks involving complex visual inputs (requiring deep CNNs), such as the Atari benchmark. It is also hard to use with architectures that have multiple outputs (e.g., a policy and a value function). Slide 13-14: PPO: The Practical Version of TRPO Concept: Proximal Policy Optimization (PPO). Core Idea: PPO inherits TRPO\u0026rsquo;s philosophy of \u0026ldquo;let\u0026rsquo;s change the policy a little at a time\u0026rdquo; but replaces the complex and strict trust region constraint with a simpler mechanism that is easier to implement and more efficient. How it Improves on TRPO: Instead of using a hard constraint, PPO modifies the objective function to penalize policies that move too far away from the old policy. The most common technique for this is called Clipping. Impact: PPO is now one of the most widely used policy optimization algorithms due to its simplicity, stability, and strong performance. Slide 15-19: PPO\u0026rsquo;s Core Idea: Clipping This section details the PPO-Clip objective function, which is the key to its success.\nPPO-Clip Objective Function: $$L^{CLIP}(\\theta)=\\hat{\\mathbb{E}}[min(r_{t}(\\theta)\\hat{A}{t}, clip(r{t}(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_{t})]$$\nBreakdown of Terms:\n$r_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$: This is the same probability ratio used in TRPO. It indicates whether an action is more or less likely under the new policy compared to the old one. $clip(r_{t}(\\theta),1-\\epsilon,1+\\epsilon)$: This is the clipping mechanism. It forces the probability ratio $r_t(\\theta)$ to stay within a small range, $[1-\\epsilon, 1+\\epsilon]$ (e.g., [0.8, 1.2]). This acts as a safety rail, preventing the ratio from becoming too large or too small. $min(\u0026hellip;)$: The final objective takes the minimum of two values: The original objective ($r_{t}(\\theta)\\hat{A}_{t}$) The clipped objective ($clip(\u0026hellip;)\\hat{A}_{t}$) By taking the minimum, PPO creates a pessimistic, lower-bound estimate of the policy\u0026rsquo;s performance. This effectively puts a cap on the benefit you can get from a single update, which discourages excessively large changes to the policy. Analogy: It\u0026rsquo;s like setting a study plan. No matter how much you enjoy one subject, you cap its study time at \u0026ldquo;a maximum of 3 hours per day.\u0026rdquo; This prevents you from neglecting other subjects and keeps your overall learning balanced and stable.\nSlide 20-21: The Power and Application of PPO Why it\u0026rsquo;s popular: PPO is simple, powerful, and stable, making it the de-facto standard for reinforcement learning. Where is PPO used?: Robotics: Training stable and precise movements. Game Playing: Achieving superhuman performance in complex games. Large Language Models (LLMs): Fine-tuning models like ChatGPT to produce more helpful and safer responses (a process known as Reinforcement Learning from Human Feedback or RLHF). Major Users: Almost all leading AI companies, including OpenAI, DeepMind, Meta, and NVIDIA, use PPO as a core algorithm. Slide 22: A2C vs. PPO at a Glance ","permalink":"https://mookjsi.github.io/posts/trpoppo/","summary":"Session notes that explain TRPO\u0026rsquo;s trust region and PPO\u0026rsquo;s clipping with intuition and equations, and summarize limitations and practical applications.","title":"Advanced Policy Gradient Methods: TRPO \u0026 PPO"},{"content":"I\u0026rsquo;m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025.\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like \u0026ldquo;not A, but B\u0026rdquo;). The authors\u0026rsquo; solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nThis is my presentation on \u0026ldquo;Promptriever: Instruction-Trained Retrievers,\u0026rdquo; which I put together for the Information Theory and Machine Learning Lab.\nFirst, let\u0026rsquo;s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It\u0026rsquo;s also worth noting this work was accepted as a poster at ICLR 2025.\nI want to start with the paper\u0026rsquo;s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be \u0026ldquo;intelligent, instruction-following partners, not just data finders.\u0026rdquo;\nHere are the other co-authors who contributed to this research.\nI\u0026rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We\u0026rsquo;ll start with the motivation behind the research.\nSo, how do current search engines \u0026ldquo;think\u0026rdquo;? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what\u0026rsquo;s the problem?\nThis example makes the problem obvious. Imagine you need a laptop that is not a MacBook and costs under $1000. A standard retriever sees the keywords \u0026ldquo;MacBook\u0026rdquo; and \u0026ldquo;under $1000\u0026rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant.\nThis leads to a frustrating user experience. You\u0026rsquo;re forced to keep tweaking keywords and filters just to find what you want.\nThis is where Promptriever comes in. It doesn\u0026rsquo;t just use semantic similarity. Instead, it operates on \u0026ldquo;dynamic relevance definitions,\u0026rdquo; which allows for a much more intelligent process.\nLet\u0026rsquo;s look at the same query again, but with Promptriever. It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.\nThe key idea here is that Promptriever \u0026ldquo;dynamically adjusts relevance based on your natural language instructions.\u0026rdquo; It\u0026rsquo;s not just matching words; it\u0026rsquo;s understanding commands.\nWe\u0026rsquo;ve seen what it does, which leads to the next question: \u0026ldquo;But how on earth was this made?\u0026rdquo; Let\u0026rsquo;s get into the technical details.\nNow, we\u0026rsquo;ll dive into the second section, where I\u0026rsquo;ll break down the Promptriever model\u0026rsquo;s architecture and training.\nThe architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder.\nThe main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model\u0026rsquo;s instruction-following ability. So how did they keep the model intelligent?\nThe answer, as they stated in their core message, lies in their \u0026ldquo;novel training data, which makes ignoring commands impossible for correct answers.\u0026rdquo;\nFor context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.\nPromptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. This is what lets them train the model on complex, instruction-based prompts.\nThe most clever part of their training data is the Instruction-Negative. This is a document that\u0026rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.\nHere’s a perfect example. For the query \u0026ldquo;What is the capital of France?,\u0026rdquo; a general article about Paris is a good result. But if you add the instruction \u0026ldquo;mention its average annual rainfall,\u0026rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.\nThrough this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.\nThe authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.\nCreating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.\nNow for the final section, \u0026ldquo;Experiments,\u0026rdquo; where we\u0026rsquo;ll look at the results.\nFor a fair comparison, they ran an \u0026ldquo;Apples-to-Apples\u0026rdquo; test against RepLLaMA, using the exact same data and hyperparameters.\nThey used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions.\nThe results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.\nThis table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability.\nOn the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nThis is where it gets interesting. When given a helpful prompt, Promptriever\u0026rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely \u0026ldquo;promptable.\u0026rdquo;\nThis table looks at the standard deviation of scores across different prompts. Promptriever\u0026rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nThe ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nThe authors also proved their training \u0026ldquo;recipe\u0026rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, \u0026ldquo;A golden recipe doesn\u0026rsquo;t discriminate against ingredients!\u0026rdquo;\nFinally, let\u0026rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper\u0026rsquo;s scope.\nIn the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","permalink":"https://mookjsi.github.io/posts/paper-review-promptriever/","summary":"A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","title":"Promptriever - Instruction-Trained Retrievers"},{"content":"This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the geometry of the loss landscape and a model\u0026rsquo;s ability to generalize.\nThe core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, flat regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.\nThis is the title slide for my presentation on \u0026ldquo;Simplifying neural nets by discovering flat minima,\u0026rdquo; which I prepared for our lab meeting.\nFirst, let\u0026rsquo;s credit the authors: Sepp Hochreiter and Jürgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?\nHere are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).\nThis quote from the authors perfectly captures the paper\u0026rsquo;s central thesis: \u0026ldquo;Find wide, not sharp, minima - and your network generalizes for free.\u0026rdquo; It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.\nMy presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.\nSo, let\u0026rsquo;s start with the motivation. Why was this research necessary?\nTo understand the paper\u0026rsquo;s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network\u0026rsquo;s weights (so-called \u0026ldquo;priors\u0026rdquo;). This paper marked a shift, arguing that we should focus on the geometry of the error surface rather than imposing assumptions about the weights themselves.\nThis 3D plot illustrates the core concept perfectly. On the right, we see a \u0026ldquo;sharp\u0026rdquo; minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a \u0026ldquo;flat\u0026rdquo; minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.\nHere\u0026rsquo;s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It\u0026rsquo;s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It\u0026rsquo;s much easier to find a good spot, and moving around a little doesn\u0026rsquo;t drastically change your altitude (or your model\u0026rsquo;s error). This robustness is linked to lower model complexity.\nSo, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a \u0026ldquo;good\u0026rdquo; prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.\nThe \u0026ldquo;Flat Minima\u0026rdquo; approach offers solutions to these problems. First, it doesn\u0026rsquo;t require any pre-chosen priors; the geometry of the solution space itself defines what a \u0026ldquo;simple\u0026rdquo; model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.\nNow, let\u0026rsquo;s formalize the problem by defining the tasks and architectures this method applies to.\nThe basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.\nThe model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).\nBuilding on that, we can define the set of \u0026ldquo;acceptable\u0026rdquo; solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors w for which the training MSE is less than or equal to this tolerance.\nNow we get to the core of the algorithm\u0026rsquo;s construction. For a given weight vector w, we define a \u0026ldquo;box\u0026rdquo; around it. For each individual weight $w_{ij}$, we find the maximum amount $\\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\\Delta w_{ij}$ for each weight.\nThese intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the \u0026ldquo;Flat Minima\u0026rdquo; as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.\nSo, how do we actually find these large-volume minima? That brings us to the algorithm itself.\nThe main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\\Delta w$. A larger volume signifies a flatter, more desirable minimum.\nThis slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.\nMaximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.\nThis new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.\nTo build the algorithm, we first need to mathematically define \u0026ldquo;flatness\u0026rdquo;. We start by defining the change in the network\u0026rsquo;s output, $ED(w, \\delta w)$, that results from a small change in weights, $\\delta w$.\nTo make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \\delta w)$, in terms of the original output and the first derivatives (gradients).\nSubstituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.\nThis leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don\u0026rsquo;t lead to large output changes.\nThe second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as \u0026ldquo;spherical\u0026rdquo; as possible by ensuring that perturbations to each weight contribute equally to the total output change.\nHere is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.\nBy rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).\nThis slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.\nBy solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\\Delta w_{ij}|$. This formula depends on the network\u0026rsquo;s output gradients.\nLet\u0026rsquo;s quickly recap the algorithm\u0026rsquo;s goal. We aim to maximize the box volume $\\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.\nNow that we have a formula for the size of the box edges, $\\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network\u0026rsquo;s derivatives. This slide shows that connection, approximating the log of $\\Delta w_{ij}$ with the log of the derived formula.\nThis slide restates the formula for the size of the perturbation $|\\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.\nPlugging the expression for $\\Delta w_{ij}$ into our cost function $B(w, D_0) = -\\sum \\log \\Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the \u0026ldquo;flatness\u0026rdquo; of the minimum.\nThe complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\\lambda$ balances the two.\nTo minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network\u0026rsquo;s output, which would typically be very expensive to compute.\nHowever, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$—the same as standard backpropagation—using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.\nTo recap the key advantages: the \u0026ldquo;Flat Minima\u0026rdquo; method is appealing because it uses geometry instead of priors to define simplicity, it\u0026rsquo;s computationally efficient, and it naturally performs network pruning for better generalization.\nThe paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.\nIn the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.\nThe results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.\nThe second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.\nThe final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.\nThe results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.\n","permalink":"https://mookjsi.github.io/posts/paper-review-flatminima/","summary":"A detailed review of the classic 1994 paper by Hochreiter \u0026amp; Schmidhuber, \u0026lsquo;Simplifying Neural Nets by Discovering Flat Minima\u0026rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization.","title":"Simplifying Neural Nets by Discovering Flat Minima"},{"content":"Hello everyone, I\u0026rsquo;m Jungmook Kang at Yonsei University. Today, I\u0026rsquo;m excited to share a review of a fascinating paper that was recently accepted to NeurIPS, which delves into the theoretical underpinnings of the attention mechanism. Let\u0026rsquo;s get started.\nThe title of the paper is \u0026ldquo;Max-Margin Token Selection in Attention Mechanism.\u0026rdquo; This work comes from our lab here at Yonsei University, and it explores the question of why attention works the way it does.\nSo, let\u0026rsquo;s begin with the motivation for this research. What prompted us to look into this?\nWe all know that the attention mechanism is incredibly effective. For a sentence like \u0026ldquo;The pizza came out of the oven and it tasted good!\u0026rdquo;, attention can correctly associate \u0026ldquo;it\u0026rdquo; with \u0026ldquo;pizza.\u0026rdquo; It works remarkably well\u0026hellip; but the story doesn\u0026rsquo;t end there.\nWhen we look at the standard Transformer architecture, which powers models like BERT and GPT, we see a complex system of encoders and decoders with multiple layers of multi-head attention. This raises some fundamental questions: Why is this architecture so successful? And more specifically, what is happening to the model\u0026rsquo;s weights during the optimization process that leads to this success?\nAnalyzing this architecture theoretically is very challenging. The optimization problem is both non-linear, due to functions like Softmax, and non-convex. This makes it extremely difficult to describe the training dynamics with traditional methods.\nTo tackle this, we turn to the concept of \u0026ldquo;Implicit Bias.\u0026rdquo; This is a phenomenon where the optimization algorithm itself—like Gradient Descent—has a preference for a certain type of solution, even when there\u0026rsquo;s no explicit regularization term in the loss function telling it to do so.\nThis leads us to the central question of our research: What is the implicit bias of the attention model? What kind of solution does it naturally favor?\nWe can get a clue from simpler models. It\u0026rsquo;s a known result from Soudry et al. (2018) that when you train a linear model with logistic loss using gradient descent, the solution tends to converge towards the one you\u0026rsquo;d get from a hard-margin Support Vector Machine (SVM). The goal of a hard-margin SVM is to find the hyperplane that maximizes the distance, or margin, to the nearest data points.\nSo, we formed a hypothesis: Could it be that the implicit bias of the much more complex attention model is also related to a hard-margin SVM solution?\nHere\u0026rsquo;s a core idea. We observe that attention has a tendency to focus on a small number of important tokens. We believe the principle guiding this selection is related to maximizing a margin. For instance, to distinguish between a cat and a dog, you might focus on the shape of the \u0026rsquo;ears\u0026rsquo; because that feature provides the clearest separation, the largest \u0026lsquo;margin,\u0026rsquo; between the two classes.\nThis brings us to the first main part of the analysis, where we explore the idea of global and local margin maximization within the attention mechanism.\nTo make the analysis tractable, we start with a simplified Softmax Attention model. The function f(X) takes an input sequence X and computes a final output. The key learnable parameters are p, the query embedding, and W, the key-query weights. In Transformers, p can be thought of as the embedding for the [CLS] token.\nOur training objective is to minimize the empirical risk for a binary classification task. We use a standard setup with input sequences X and labels Y, and a decreasing loss function like logistic loss. The model\u0026rsquo;s prediction f(Xi) is a function of the learnable parameters v, p, and W.\nHere, I\u0026rsquo;ll just quickly go over some of the mathematical notation we use throughout the paper. We use standard conventions for vectors and matrices, and we\u0026rsquo;ll use simplified notations like L(p) to denote the risk when v and W are fixed.\nOur approach involves studying the regularization path, which is the path of the solution p as we increase its allowed norm, R. This path mirrors the trajectory of gradient descent. We analyze a final attention model where the trainable parameters W and p jointly influence the softmax, leading to similar optimization dynamics.\nThis brings us to Lemma 1, which is a crucial simplification. It states that the weight matrix W is effectively a rank-1 matrix and its learning dynamics are completely determined by the vector p. This is very important because it means we can fix W and focus our entire analysis on the optimization of p.\nSo, we can formally define our problem. We are exploring the training risk L(v, p) where the key embeddings Ki are derived from the input Xi. For the analysis, we can even treat Xi and Ki as separate entities.\nTo ensure our proofs hold, we need Assumption A. This is a standard assumption that requires our loss function l to be well-behaved—specifically, it must be strictly decreasing and have a Lipschitz-continuous derivative. Common functions like logistic loss and exponential loss satisfy this condition.\nNow, we introduce the central piece of our theory: a hard-margin SVM problem, which we call ATT-SVM. The goal here is to find a direction p that, for each input sequence, separates one chosen token (k_iαi) from all other tokens (k_it) by a margin of at least 1. We will show that the optimization of softmax attention is effectively trying to solve this problem.\nTo formalize this, we define the score of a token (γ_it) as its contribution to the final correct prediction. The optimal tokens are naturally the ones with the highest scores for each input. The Globally-Optimal Max-Margin (GMM) direction, denoted p^mm*, is then the solution to our ATT-SVM problem when we choose these optimal tokens.\nTheorem 1 is our first main result. It states that the regularization path—the sequence of solutions as we increase the norm R—converges in direction to this GMM solution, p^mm*. In simple terms, as the optimization progresses and the norm of p grows, its direction aligns with the direction that best separates the highest-scoring tokens from the rest.\nTheorem 2 looks at the convergence of gradient descent directly. It says that under our assumptions, the norm of the weight vector p will grow towards infinity during training. Critically, for the simple case of a single training example (n=1), the direction of p converges to the GMM direction. However, for n \u0026gt; 1, it\u0026rsquo;s possible for the optimization to get trapped in a local minimum.\nTo test these theorems, we designed a simple experiment. We created synthetic data with three tokens, where we can visualize their key embeddings in 2D and independently control their scores using a third dimension. This lets us clearly see what\u0026rsquo;s happening.\nThese plots show the results. In Figure (a), where the conditions for global convergence are met, we see that all gradient descent trajectories (the grey arrows) correctly converge to the GMM direction (the red dashed line). In (b), we\u0026rsquo;ve changed the scores so that a locally optimal solution exists, and we see some trajectories get stuck there instead. Figure (c) shows the more complex scenario with multiple inputs, where finding the joint GMM solution is harder.\nThis brings us to the idea of locally-optimal tokens. These are tokens that might not be the highest-scoring ones globally, but they form a stable solution for the ATT-SVM problem. The direction associated with them is a locally-optimal max-margin (LMM) direction.\nTo talk about local convergence, we introduce the geometric concept of a cone. A cone around a direction q is simply the set of all vectors that are \u0026ldquo;close\u0026rdquo; in angle to q. The diagram illustrates an initialization p(0) that lies within the cone of an LMM direction p^mm.\nTheorem 3 formalizes local convergence. It states that if you initialize the gradient descent p(0) within the cone of an LMM direction, the optimization will stay within that cone and ultimately converge to that specific LMM direction.\nLooking back at our experiment in Figure (b), we can now understand it better through Theorem 3. The trajectories that don\u0026rsquo;t find the global optimum are the ones that were initialized inside the cone of the locally-optimal solution (the blue square), and as the theorem predicts, they converge to it.\nTheorem 4 provides a tightness guarantee. It essentially says that the LMM directions are the only stable convergence points. If you start in any direction q that is not an LMM direction, the optimization path will eventually move away from q.\nThis diagram illustrates Theorem 4. If our initial parameter p(0) is in a cone (the grey one) that does not contain any LMM or GMM direction, the optimization process will not converge in that direction. It is implicitly forced to seek out one of the valid max-margin solutions.\nTo summarize the first half: we\u0026rsquo;ve shown that when training only the attention parameter p, the implicit bias of gradient descent drives the solution towards a hard-margin SVM that separates tokens. Now, what happens if we also learn the prediction head v at the same time?\nThis leads to the second part of our analysis: the joint convergence of the prediction head v and the attention weights p.\nThe high-level intuition is that the model is linear in v, so the optimization with respect to v also has an implicit bias towards a standard max-margin classifier solution. The features for this classifier, x_i^p, are the outputs of the attention layer.\nThe challenge here is that the features r that v sees are determined by p, which itself is changing during training. This creates a coupled dynamic. We analyze this by separating the problem into cases based on whether the selected features are support vectors for the v-classifier or not.\nTheorem 5 addresses the case where we assume the selected tokens are all support vectors. We introduce Assumption C, which formalizes that if the attention doesn\u0026rsquo;t perfectly select the optimal token, the classification margin for v shrinks. Under this assumption, we find that v converges to the direction of a standard SVM solution, and p converges to the direction of our ATT-SVM solution.\nThis raises a natural question: Does every selected token really need to be a support vector, sitting right on the margin? That seems like a very strict condition. Intuitively, for the data points that are not support vectors, we don\u0026rsquo;t need to maximize their margin. We just need to make sure they are on the correct side of the decision boundary. This allows us to define a \u0026ldquo;relaxed\u0026rdquo; version of our ATT-SVM problem.\nTheorem 6 presents this more general result. The ATT-SVM problem is relaxed: for tokens corresponding to support vectors (i ∈ S), we require a margin of 1, but for non-support vectors (i ∈ S-bar), we only require a margin of 0 (correct classification). We only need Assumption C to hold for the support vectors. The result is that v still converges to the max-margin v^mm, and p converges to the solution of this new, relaxed SVM problem, p^relax.\nWe ran experiments for this joint convergence case as well. The plots show the trajectories for the attention weights p and the classifier head v. Plot (a) shows a case aligning with Theorem 5, where all inputs are support vectors. Plot (b) shows a case for Theorem 6, where one input is not a support vector. Plot (c) shows that the softmax probability for the optimal token quickly goes to 1, confirming that the attention is learning to select specific tokens as predicted.\nSo, to summarize the entire theoretical part: Whether we train the attention weights p alone or jointly with the classifier head v, the implicit bias of the optimization consistently pushes the model towards a hard-margin SVM solution. This provides a strong theoretical explanation for why attention learns to become sparse and focus on the most discriminative tokens.\nFinally, let\u0026rsquo;s look at a few more experiments to see these principles in action on more complex tasks.\nThis slide provides an experimental comparison between Normalized Gradient Descent (GD) and Vanilla GD, validating the paper\u0026rsquo;s theory. Plot (a) shows that while both methods learn to focus on a single token (softmax probability approaches 1.0), Normalized GD achieves this sparse attention much faster. Plot (b) demonstrates a key theoretical prediction: the norm of the attention weights ||p|| diverges (grows linearly without bound) when using Normalized GD. This behavior is characteristic of an optimization process seeking a max-margin solution.\nHere, we trained a vision transformer on an image classification task. Figure 7 shows that as training progresses over epochs, the sparsity of the attention map increases (red curve goes down), meaning the model learns to focus on fewer, more important image patches. At the same time, the norm of the attention weights ||W|| steadily increases (blue curve), which is exactly what our theory predicts will happen as the solution converges towards an infinite-norm, max-margin boundary. The images in Figure 6 visually show this focusing effect over time.\nThis final experiment shows how the choice of loss function affects the dynamics. We compare a correlation loss (l(x) = -x) with the logistic loss. The gradient\u0026rsquo;s magnitude depends on the token\u0026rsquo;s score γ differently for each loss. For correlation loss, larger scores get larger gradients, while for logistic loss, the gradient is largest for scores near zero. This results in different trajectories, but as you can see, both are ultimately guided by the same underlying max-margin principle, pushing towards a separating hyperplane.\n","permalink":"https://mookjsi.github.io/posts/paper-review-maxtoken/","summary":"This post reviews the NeurIPS 2025 paper \u0026lsquo;Max-Margin Token Selection in Attention Mechanism.\u0026rsquo; The paper analyzes the theoretical foundations and implicit bias of the attention mechanism, explaining why attention focuses on important tokens and how this process is connected to margin maximization in SVMs.","title":"Max-Margin Token Selection in Attention Mechanism"},{"content":"This is my detailed slide-by-slide analysis of the paper Stochastic Approximation to Contrastive Learning, which was submitted to the ICLR 2025 conference.\nThe paper tackles a critical challenge in contrastive learning: its heavy reliance on large batch sizes and the associated computational cost. The authors introduce SACLR, a novel framework inspired by Stochastic Cluster Embedding (SCE) that reformulates the objective using I-divergence. The goal was to enable efficient training with as little as one negative sample, a significant departure from methods like SimCLR.\nWhile the premise is compelling, the paper faced substantial criticism during the open review process regarding its experimental comparisons, novelty, and the substantiation of its claims. Despite rebuttals and additional experiments, it was ultimately rejected. In this review, I\u0026rsquo;ll walk through the method as presented and layer in the context from the public reviews to provide a complete picture of its strengths and weaknesses.\nThis is the title slide for my review of the paper \u0026ldquo;Stochastic Approximation to Contrastive Learning.\u0026rdquo; This presentation was prepared for the Information Theory and Machine Learning Lab at Yonsei University.\nTo begin, I\u0026rsquo;ll recap the core concept of Representation Learning. This field is all about how we represent data, which leads to the fundamental question: what exactly is a \u0026ldquo;representation\u0026rdquo; in the context of machine learning?\nI\u0026rsquo;ll illustrate this with a simple task. Imagine you need to solve the division problem CCV / VI. For most people, this is not immediately obvious.\nNow, consider this problem: 210 / 6. This is likely much easier, and you can quickly determine the answer is 35. The interesting part is that both problems represent the exact same calculation.\nThe key difference lies in the representation of the same numerical information. The first task used Roman numerals, while the second used Arabic numerals. The choice of representation dramatically changes the difficulty of the task.\nThis analogy illustrates a critical point that is central to representation learning: the right representation can make a complex task much easier to solve.\nTo summarize this introductory point: the difficulty of many information processing tasks is highly dependent on how that information is represented.\nSo, the crucial question for us is: how can we obtain a \u0026ldquo;good representation\u0026rdquo; for various machine learning tasks? Today, I\u0026rsquo;ll focus on one prominent approach for achieving this: Self-Supervised Learning.\nSelf-Supervised Learning is a subset of the broader field of Representation Learning.\nA primary motivation for the development of self-supervised methods is the immense cost and effort required to obtain large-scale labeled datasets for traditional supervised learning.\nThe solution that Self-Supervised Learning proposes is to find a way to learn a \u0026ldquo;good representation\u0026rdquo; that captures the essential features of the data using only an unlabeled dataset.\nThis slide illustrates the typical self-supervised learning pipeline. First, a model is trained on a \u0026ldquo;pretext task\u0026rdquo; using a large amount of unlabeled data. The learned model is then transferred and fine-tuned on a \u0026ldquo;downstream task\u0026rdquo; using task-specific (and often limited) labeled data.\nThe goal of the pre-training stage is to transform raw data, which can be a poor representation for a computer (like a raw image of a dog), into a feature vector that is a much better representation for downstream tasks.\nThe central question this paper investigates is the pre-training step: how exactly does the model learn a good representation from unlabeled data?\nWithin the realm of Self-Supervised Learning, I will now narrow the focus to Contrastive Learning. This approach is based on making \u0026ldquo;inter-sample\u0026rdquo; predictions.\nHere is the core mechanic of contrastive learning. We start with an \u0026ldquo;anchor\u0026rdquo; image. We create two different augmented versions, or \u0026ldquo;views,\u0026rdquo; of this anchor, which form a \u0026ldquo;positive pair.\u0026rdquo; We then contrast this with a \u0026ldquo;negative pair,\u0026rdquo; which is formed by the anchor and a view from a completely different image.\nIn summary, the objective of contrastive learning is to learn effective representations by mapping similar data points close to each other in the representation space, while simultaneously pushing dissimilar data points far apart.\nThis slide outlines the flow of the main arguments in this review. I\u0026rsquo;ll begin by discussing the high computational costs that arise from the conventional definition of positive and negative pairs. Then, I will introduce the paper\u0026rsquo;s proposed method, which uses matrix approximation with I-divergence to create a decomposable and stochastic loss, ultimately achieving competitive results with a low batch size and fewer negative pairs.\nNow, we move from the recap to the main introduction of the paper\u0026rsquo;s contribution.\nThe core problem is that while supervised learning is effective, it depends on having extensive labeled data. Self-Supervised Learning is the alternative we are exploring.\nHowever, popular contrastive learning methods like SimCLR require very large batch sizes to ensure a sufficient balance of positive and negative examples. This expends a large amount of computational resources, particularly on the negative pairs. A method called Sog-CLR attempted to address this by mixing an EMA of image similarities into the denominator of the InfoNCE loss.\nTo be more specific, SimCLR\u0026rsquo;s InfoNCE loss operates in a full-batch mode, which is not decomposable in a mini-batch setting. Sog-CLR showed improvement by incorporating a running average for the negative pair estimations.\nThis paper poses the question: even with Sog-CLR\u0026rsquo;s improvements, is there still room for further optimization? It introduces SACLR, a method that uses I-divergence to reformulate the objective into a matrix approximation problem that is decomposable across instance pairs.\nThe key idea behind SACLR is this reformulation using I-divergence, which allows the objective to be decomposed and approximated stochastically. This is presented as an advancement over Sog-CLR\u0026rsquo;s EMA mixing approach.\nThe paper\u0026rsquo;s central claim is that its method, SACLR, can learn high-quality representations with a small batch size and very few negative pairs. While reviewers found the core idea of tackling the large batch size problem to be an interesting and valuable contribution, and saw the novel formulation inspired by Stochastic Cluster Embedding as a strength, they heavily scrutinized the experimental evidence supporting these claims.\nTo understand SACLR, we first need to look at its theoretical foundation: Stochastic Cluster Embedding (SCE). I will now detail the matrix approximation with I-divergence that underpins the method.\nIn SCE, we have embedded data points, like $y_i$ and $y_j$. We define a similarity kernel $q_{ij}$ between these points in the embedded space, typically using a Gaussian or a Student\u0026rsquo;s t-distribution kernel. This value should be close to 1 for similar points.\nWe also define a target similarity matrix, P, which represents the desired similarities in the embedded space. For example, perfectly clustered data would have a block-diagonal P matrix.\nThe goal is to make the learned similarity matrix Q as close as possible to our desirable target matrix P. The question is, how do we measure this closeness?\nThis is where the choice of divergence metric is crucial. t-SNE uses the KL-divergence. In contrast, SCE uses the I-divergence, which includes an additional scaling factor \u0026rsquo;s\u0026rsquo; and linear terms.\nThis slide highlights the formulas for both KL-divergence, used in t-SNE, and the I-divergence with a scaling factor, used in SCE.\nAn important property is that the I-divergence can reduce to the KL-divergence. This happens if the scaling factor \u0026rsquo;s\u0026rsquo; is set to be the inverse of the sum of all kernel similarities, effectively normalizing the Q matrix.\nSCE defines the scaling factor \u0026rsquo;s\u0026rsquo; using a weighted sum controlled by the parameter α, which introduces additional repulsion to improve cluster quality. A key methodological point, raised by Reviewer C43T, was why \u0026rsquo;s\u0026rsquo; is treated as a constant during optimization when it is a function of the embeddings. The authors clarified that they use an interleaving optimization strategy: the model parameters are optimized while \u0026rsquo;s\u0026rsquo; is fixed, and then \u0026rsquo;s\u0026rsquo; is periodically updated based on the new embeddings.\nBy plugging this definition of the weights $w_{ij}$ into the formula for \u0026rsquo;s\u0026rsquo;, the denominator term can be rewritten as a sum of two expectations.\nThese two expectations have clear interpretations: $E_1$ is the expected similarity for pairs drawn from the target distribution P, while $E_2$ is the expected similarity for pairs drawn uniformly at random from all possible pairs.\nThis formulation allows the I-divergence objective to be rewritten in a stochastic form, separating it into an attraction term based on the target distribution and a repulsion term based on the uniform distribution. The terms themselves are simple functions of the similarity $q_{ij}$.\nNow, let\u0026rsquo;s apply this SCE framework to Contrastive Learning. In our setting, for each input $x_i$, we generate two augmented views, $\\tilde{x}{i}^{(1)}$ and $\\tilde{x}{i}^{(2)}$. We denote their embeddings as $\\tilde{y}{i}^{(u)}$ and the similarity between any two embeddings as $q{ij}^{(u,v)}$.\nThe goal in contrastive learning is to make embeddings from the same instance ($i=j$) similar, and embeddings from different instances ($i \\ne j$) dissimilar. We can formally define this as a target tensor $p$, where the target similarity is 1 only for positive pairs ($p_{ii}^{(1,2)}$ and $p_{ii}^{(2,1)}$) and 0 for all other pairs.\nThis slide provides a concrete example of the target tensor $P$ for a case with N=3 instances. The matrices for same-view similarities ($P^{(1,1)}, P^{(2,2)}$) are all zeros, while the matrices for cross-view similarities ($P^{(1,2)}, P^{(2,1)}$) are identity matrices, capturing the positive pair targets.\nTo simplify the math, we can flatten this four-dimensional tensor structure into standard 2D matrices. The $2N \\times 2N$ target matrix $\\psi$ is formed by reorganizing the elements of the tensor $p$. This results in a sparse matrix where the identity matrices from the tensor become off-diagonal blocks.\nFor notational simplicity, we neglect the diagonal elements of the matrices $\\psi$ and $\\phi$ in the approximation, as they are constant for the kernels used and do not affect the optimization.\nNow we apply the SCE I-divergence formula, but to our new flattened matrices $\\psi$ and $\\phi$ which represent the contrastive learning problem.\nApplying the I-divergence $D_{I}(\\psi||s\\phi)$ and using the specific structure of our target matrix $\\psi$ (which is mostly zeros), the loss function simplifies significantly. The scaling factor $s$ is updated periodically, with its weights $w_{ij}^{u,v}$ also adapted for the contrastive case.\nThe full loss function $\\mathcal{L}{CLR}(\\theta)$ can be expressed as an expectation over the data indices. This form consists of a term for positive pairs ($log~q{ii}^{1,2}$) and a repulsion term involving a sum of similarities over all pairs.\nTo make this computationally feasible, we use a Monte Carlo approximation. This gives us the final SACLR objective, $\\mathcal{L}{SACLR}(\\theta)$, which is calculated over a mini-batch $\\mathcal{B}$ and a set of M negative samples $\\mathcal{M}{i}$. The paper studies two variants: SACLR-1 (M=1) and SACLR-all (M=B).\nThe scaling factor $s$ is also estimated stochastically. Its inverse, $s^{-1}$, is approximated using samples from the mini-batch, and this estimate is updated smoothly using an exponential moving average (EMA) after each batch.\nThe paper also explores a row-wise decomposition of the matrix approximation. Instead of one global scaling factor \u0026rsquo;s\u0026rsquo;, each row \u0026lsquo;a\u0026rsquo; of the similarity matrix gets its own scaling factor $s_a$. This results in the loss function $\\mathcal{L}_{CLR-row}(\\theta)$.\nThis slide provides proof for the simplified row-wise SACLR loss function. By substituting the sparse target matrix $\\psi$ into the general I-divergence formula and summing over all rows, we arrive at the expression shown.\nA key theoretical finding presented in the paper is that under specific conditions, the SACLR-row objective becomes equivalent to the SimCLR loss. This link, however, became a point of discussion during the review. Reviewer gz9D argued that this equivalence doesn\u0026rsquo;t explain why SACLR should be expected to outperform SimCLR. The authors countered that the method\u0026rsquo;s advantage comes not from this specific condition, but from using a different, more flexible weighting scheme for the scaling factor.\nHere is the proof of Theorem 3.1. By substituting the condition for the scaling factors into the loss function, the repulsion term simplifies to a constant, and the remaining terms can be rearranged to form precisely the SimCLR objective.\nJust as with the full matrix version, the row-wise loss can be approximated stochastically for mini-batch training. This gives us the $\\mathcal{L}_{SACLR-row}(\\theta)$ objective.\nSimilarly, the row-wise scaling factors $s_{2(i-1)+u}$ are estimated stochastically within each mini-batch and updated using an EMA rule.\nThis diagram provides a summary of the theoretical framework I have just presented. We started with the concept of I-divergence and a scaling factor, which we used to build a matrix approximation. This was then decomposed row-wise, and both versions were made practical via stochastic approximation. Now, it\u0026rsquo;s time to see the experimental results.\nThis slide presents the full pseudocode for the SACLR algorithm. A major point of contention in the initial review was that the paper claimed to be \u0026ldquo;more computationally efficient\u0026rdquo; without providing empirical data on runtime or memory. The detailed ablation studies on computational cost, shown later in the presentation, were added during the rebuttal period as a direct response to this criticism from the reviewers.\nNow we move to the experiments section. The standard evaluation protocol for self-supervised methods is used: first, a model is pre-trained without labels on a dataset like ImageNet or CIFAR. The learned weights from this backbone are then used to initialize a new network, a linear layer is added, and this new network is trained with labels to perform a classification task.\nTable 1 shows the Top-1 linear classification accuracies on ImageNet. While SACLR-ALL is shown to be competitive with some methods like MoCo v2, this comparison drew significant criticism during the review process. The Area Chair and multiple reviewers stated that the baseline methods used for comparison were \u0026ldquo;weak\u0026rdquo; and the performance gains \u0026ldquo;marginal,\u0026rdquo; noting that many stronger, more recent methods were omitted.\nThis table shows results for longer training, with SACLR-MIX surpassing SimCLR and SogCLR in this setup. However, reviewers found this comparison inadequate as well. Reviewer gz9D pointed out that results for top-performing methods like VICReg and Barlow Twins from other benchmark papers were significantly higher. The authors argued their goal was primarily efficiency without heavy tuning, but this did not overcome the concerns about the weak comparison set.\nThis experiment in Table 3 specifically investigates performance when using only a single negative sample (M=1) per image. SACLR-1 achieves a Top-1 accuracy of 65.3%, significantly outperforming classical contrastive losses like Triplet and Logistic loss under the same constraint.\nThis table evaluates semi-supervised learning performance, showing SACLR\u0026rsquo;s strength in data-limited regimes. It is worth noting that the initial submission focused almost exclusively on linear evaluation. Reviewers Tx4K and gz9D strongly recommended including more comprehensive evaluations like semi-supervised fine-tuning to provide a more complete picture, and the additional kNN and fine-tuning results were added to the paper in response.\nThe learned representations are also evaluated on transfer learning classification tasks. As shown in Table 5, the representations learned by SACLR-ALL and SACLR-MIX transfer very well to other datasets like VOC07 and especially iNaturalist18, where they significantly outperform SimCLR and MoCo v2.\nTable 6 shows transfer learning results on more complex downstream tasks: object detection and segmentation on VOC and COCO. Across all metrics, the SACLR variants are highly competitive and often outperform strong baselines like SimCLR, BYOL, and MoCo v2, with SACLR-MIX showing the strongest results overall.\nThis table provides a direct comparison with other stochastic estimation-based contrastive methods, using an architecture similar to the iSog-CLR paper for a fair comparison. The results on CIFAR10, CIFAR100, and ImageNet100 show that both the matrix and row versions of SACLR are highly competitive, often achieving the best or second-best performance in this specific class of methods.\nThis slide presents several ablation studies on computational complexity and robustness. These experiments were largely added in response to direct reviewer feedback. Reviewers requested empirical data on runtime and memory to substantiate the paper\u0026rsquo;s efficiency claims, as well as a study on the impact of batch size to support the claim that SACLR performs well in small-batch settings. These tables represent the authors\u0026rsquo; attempt to provide that missing evidence.\n","permalink":"https://mookjsi.github.io/posts/paper-review-stochastic/","summary":"This post reviews \u0026lsquo;Stochastic Approximation to Contrastive Learning,\u0026rsquo; a paper submitted to ICLR 2025. While ultimately rejected, the paper proposed an interesting approach to make contrastive learning more efficient. I\u0026rsquo;ll break down its core ideas, the community\u0026rsquo;s feedback, and why it fell short.","title":"Stochastic Approximation to Contrastive Learning"},{"content":"This is a detailed slide-by-slide review of the paper When to Retrieve?, which was submitted to ACL 2024 but was ultimately rejected.\nThe paper questions the efficiency of the standard Retrieval-Augmented Generation (RAG) framework, which naively performs retrieval for every query. The authors propose an adaptive retrieval model (RET indicator) that dynamically decides, for each query, whether external knowledge retrieval is necessary. If the LLM\u0026rsquo;s parametric memory is sufficient, retrieval is skipped; otherwise, external information is fetched.\nThis is the title slide for my presentation on the paper \u0026ldquo;When to Retrieve?\u0026rdquo;, which I delivered at the Yonsei University Machine Learning Lab on February 28, 2025.\nTo set the stage, I\u0026rsquo;m introducing a real-world application I developed, \u0026ldquo;momugo,\u0026rdquo; a restaurant recommendation app. This slide shows the user input screen, where a user is asking for a recommendation for a quiet place to talk with a friend while enjoying soju and hot fish cake soup.\nThis slide demonstrates the detailed output of the \u0026ldquo;momugo\u0026rdquo; app. When a user clicks on a recommended restaurant, it displays more information, including relevant review snippets that match the user\u0026rsquo;s query, providing a justification for the recommendation.\nHere, I\u0026rsquo;m breaking down the initial data filtering process in my application. The system first performs a primary filtering based on structured data like category and business hours. Then, it moves to a more detailed secondary filtering based on the specifics of the user\u0026rsquo;s natural language query.\nThis slide illustrates the core retrieval and generation pipeline. After the initial filtering, the user\u0026rsquo;s query is used to find the top 10 most similar review embeddings from a vector database. The top 3 restaurants are then selected, and an LLM generates a descriptive recommendation reason for each.\nI\u0026rsquo;m detailing the \u0026ldquo;Detailed Filtering\u0026rdquo; step. An LLM is prompted with a system message to analyze the user\u0026rsquo;s query and extract specific requirements, such as \u0026ldquo;corkage available\u0026rdquo; or \u0026ldquo;pet-friendly,\u0026rdquo; into a structured JSON format.\nThis slide provides a high-level overview, separating the entire process into two main phases: Retrieval and Generation. The retrieval part finds the most relevant restaurants, and the generation part creates the user-facing explanation.\nI\u0026rsquo;m explaining the embedding process for the retrieval phase. The descriptions and latest reviews for each restaurant are converted into a 1536-dimensional vector using OpenAI\u0026rsquo;s text-embedding-3-small model and stored in a Pinecone vector database.\nThis slide details the retrieval mechanism. We embed all reviews and the user\u0026rsquo;s query. Using LangChain, we retrieve the top 10 review vectors based on cosine similarity. Finally, we determine the top 3 restaurants by calculating the average vector similarity for each.\nThis slide outlines the generation phase. The top 3 restaurants identified during retrieval, along with their metadata and the original user query, are used to create three separate prompts for the LLM.\nI\u0026rsquo;m showcasing the prompt engineering aspect for the generation phase. A structured prompt, including a system message and a human message with the query and restaurant context, is fed to the LLM to generate a tailored recommendation reason for each of the top 3 restaurants.\nThis slide concludes the overview of my application\u0026rsquo;s architecture. It shows how the generated recommendation reasons (from the LLM) and the filtered restaurant data are combined to produce the final, detailed output card shown to the user.\nShifting from my personal project to the main topic, this slide introduces the standard Retrieval-Augmented Generation (RAG) process, breaking it down into the \u0026lsquo;Retrieval\u0026rsquo; and \u0026lsquo;Generation\u0026rsquo; stages.\nHere, I pose a critical question about the standard RAG framework: Is the naive approach of always retrieving information for every query the most effective or efficient method?\nThis slide frames the core problem investigated in the paper. I\u0026rsquo;m questioning the fundamental assumption of RAG: \u0026ldquo;Should we always retrieve, regardless of the query?\u0026rdquo; This sets the stage for a more nuanced approach.\nI\u0026rsquo;m introducing the central concept of the paper: an adaptive retrieval model. The key idea is to use an indicator, which the authors call \u0026ldquo;RET,\u0026rdquo; to decide whether to retrieve external information or to answer directly from the LLM\u0026rsquo;s parametric memory.\nNow that we\u0026rsquo;ve established the need for a decision mechanism, the key research question becomes: \u0026ldquo;Where to place a RET?\u0026rdquo; In other words, how does the model learn when it\u0026rsquo;s appropriate to trigger the retrieval step?\nThis slide presents the core hypothesis of the paper. The authors believe that retrieval is unnecessary for \u0026ldquo;popular entities\u0026rdquo; (information likely stored in the LLM\u0026rsquo;s parameters) but crucial for \u0026ldquo;non-popular entities.\u0026rdquo;\nBuilding on the hypothesis, I\u0026rsquo;m highlighting the practical challenge: How can a model quantitatively identify if an entity is \u0026ldquo;popular\u0026rdquo;? The paper proposes a model that can learn this distinction to enhance the efficiency and accuracy of the RAG process.\nThis slide formally introduces the paper I\u0026rsquo;m reviewing: \u0026ldquo;When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\u0026rdquo; by Labruna, Campos, and Azkune. They name their proposed model \u0026ldquo;ADAPT-LLM.\u0026rdquo;\nI\u0026rsquo;m providing context with related works. This slide contrasts \u0026ldquo;Closed-book QA,\u0026rdquo; which relies solely on an LLM\u0026rsquo;s internal knowledge, with \u0026ldquo;Open-book QA,\u0026rdquo; which always uses an external information retrieval system.\nThis slide summarizes key issues in the field and their corresponding solutions. It covers the high cost of retraining models, the lexical gap in keyword-based retrieval, and the latency costs associated with retrieval, positioning an adaptive approach as a solution.\nContinuing with the related works, this slide discusses the problem of tool overuse in models like Toolformer and the unrealistic nature of previous popularity-based retrieval methods. It positions ADAPT-LLM as a solution that provides a robust baseline for comparison.\nHere, I\u0026rsquo;m outlining the four-step process of the ADAPT-LLM framework: 1) A query is sent to the model. 2) The model decides whether to retrieve or not. 3) If needed, it retrieves information. 4) It generates the final answer.\nThis slide focuses on the critical decision-making step. The central question is how the model determines whether to retrieve information. The paper\u0026rsquo;s answer is to fine-tune the LLM on a specially curated dataset to learn this behavior.\nI\u0026rsquo;m illustrating the data preparation process for fine-tuning ADAPT-LLM. The process starts with QA data, which is fed to a base LLM. The model\u0026rsquo;s answers are classified as correct or incorrect, which then informs the creation of different types of training prompts.\nThis slide provides a concrete example of the data preparation pipeline. Using a QA pair about the capital of South Korea, I show how the query is extracted from the source data.\nContinuing the example, this slide shows the base LLM generating both a correct answer (\u0026ldquo;Seoul\u0026rdquo;) and an incorrect answer (\u0026ldquo;Busan\u0026rdquo;) to the query, which is a crucial step for creating the fine-tuning dataset.\nThis slide details how the fine-tuning dataset, named DS_adapt, is constructed. Based on the LLM\u0026rsquo;s correctness, three types of prompts are generated: a direct answer for correct parametric knowledge, a \u0026lt;RET\u0026gt; token for incorrect answers, and a context-based prompt for incorrect answers.\nThis slide visualizes the final step of the model creation process. The DS_adapt dataset, with its varied prompt structures, is used to fine-tune a base LLM, resulting in the final ADAPT-LLM.\nI\u0026rsquo;m presenting the headline results from the paper. The table shows that ADAPT-LLM, trained on both NQ and SQuAD datasets, outperforms both the NEVER RETRIEVE (NR-LLM) and ALWAYS RETRIEVE (AR-LLM) baselines in terms of accuracy on the PopQA test set.\nThis slide outlines the structure of the experiments section of my review. I will cover three key areas: comparison to baselines, the model\u0026rsquo;s ability to determine when context is needed, and a comparison with the state-of-the-art approach for the PopQA dataset.\nI\u0026rsquo;m beginning the detailed experimental setup. This slide specifies the training datasets used for the fine-tuning process: Natural Questions (NQ) and Stanford Question Answering Dataset (SQuAD).\nThis slide specifies the base Large Language Model used in the experiments. The authors chose the Llama-2-7B model as the foundation for their fine-tuning.\nHere, I specify the dataset used for evaluating the models at test time. All configurations are evaluated on the PopQA dataset, which is designed to test knowledge of popular entities.\nI\u0026rsquo;m explaining how the baseline models are created for comparison. The NR-LLM (NEVER RETRIEVE) is fine-tuned only on prompts that require direct, parametric answers.\nSimilarly, this slide explains the creation of the AR-LLM (ALWAYS RETRIEVE) baseline. This model is fine-tuned exclusively on prompts that include retrieved context, forcing it to always rely on external information.\nThis slide recaps the training process for the main model, ADAPT-LLM. It is trained on the comprehensive DS_adapt dataset, which includes a mix of parametric, retrieval-triggering, and context-aware prompts.\nI\u0026rsquo;m presenting the main results table again, this time to emphasize the direct performance comparison. ADAPT-LLM consistently achieves the highest accuracy, demonstrating the effectiveness of its selective retrieval strategy.\nTo provide more context on the datasets, this slide presents a table comparing the statistics of NQ, SQuAD, and PopQA, including the number of questions and the average length of questions and answers.\nNow, I\u0026rsquo;m moving to the second part of the experimental analysis: evaluating ADAPT-LLM\u0026rsquo;s ability to correctly decide when to retrieve. This slide sets up the analysis of the model\u0026rsquo;s decision-making accuracy.\nThis slide presents a detailed breakdown of ADAPT-LLM\u0026rsquo;s performance. It analyzes the accuracy of the model in four scenarios: when it correctly decides to retrieve (and is given context), when it incorrectly decides not to retrieve (and isn\u0026rsquo;t given context), and so on.\nI\u0026rsquo;m highlighting a key observation from the results table. The accuracy for questions where the model chose to retrieve (Acc. w/ context for (RET)) seems quite low, around 33%. This prompts a deeper investigation.\nThis slide provides an explanation for the previously noted low accuracy. The authors point out that the performance of the underlying Information Retrieval (IR) system itself was a limiting factor.\nTo support the claim about poor IR performance, this slide presents results from Table 4 of the paper. It shows a massive accuracy gap when using the gold standard passages versus passages retrieved by the Contriever system, confirming the IR bottleneck.\nDespite the issues with the IR component, this slide shows that the model\u0026rsquo;s decision-making process is sound. The histograms show a clear inverse correlation between entity popularity and the usage of the \u0026lt;RET\u0026gt; token, confirming the model learned the core hypothesis.\nI\u0026rsquo;m now moving to the final experimental comparison: ADAPT-LLM versus the previous state-of-the-art method for the PopQA dataset. This slide visually contrasts the two approaches, highlighting differences in training data and thresholding methods.\nThis slide presents the results of the state-of-the-art comparison. While the accuracy is comparable, I\u0026rsquo;m highlighting the authors\u0026rsquo; claims that ADAPT-LLM is a more generalizable and efficient approach due to its lower reliance on the IR system and its independence from dataset-specific features like popularity scores.\nTo begin the conclusion, I\u0026rsquo;m bringing back the diagram illustrating the creation of the DS_adapt fine-tuning dataset. This serves as a reminder of the core technical contribution of the paper.\nThis slide recaps the fine-tuning process itself, showing how the diverse set of prompts from DS_adapt is used to train the base LLM into the final, adaptive ADAPT-LLM.\nIn my final slide, I\u0026rsquo;m summarizing the key takeaways from the paper. The experiments demonstrate that ADAPT-LLM is a robust model that successfully learns when to retrieve information, outperforming standard baselines and showing strong potential as a general, efficient approach to adaptive RAG.\n","permalink":"https://mookjsi.github.io/posts/paper-review-whentoretrieve/","summary":"This post reviews \u0026lsquo;When to Retrieve?\u0026rsquo;, a paper submitted to ACL 2024 but ultimately rejected. I summarize its core ideas, the community\u0026rsquo;s feedback, and the reasons it was not accepted.","title":"When to Retrieve?"},{"content":"Education Yonsei University, Seoul, Korea Senior @ Dept. of Applied Statistics GPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics) Research Interests Statistical Machine Learning Large Language Model Retrieval Method Research Experience Undergraduate Researcher, ITML @ Yonsei (Jan. 2025 – Jun. 2025) Conducting undergraduate research under the supervision of Prof. Jy-yong Sohn. Engaged in ongoing projects related to RAG. Skills Programming: R, Python, Frontend, Git, MATLAB Languages: Korean (Native), English (Intermediate), Chinese (Beginner) ","permalink":"https://mookjsi.github.io/about/","summary":"\u003ch2 id=\"education\"\u003eEducation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eYonsei University\u003c/strong\u003e, Seoul, Korea\n\u003cul\u003e\n\u003cli\u003eSenior @ Dept. of Applied Statistics\u003c/li\u003e\n\u003cli\u003eGPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-interests\"\u003eResearch Interests\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical Machine Learning\u003c/li\u003e\n\u003cli\u003eLarge Language Model\u003c/li\u003e\n\u003cli\u003eRetrieval Method\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-experience\"\u003eResearch Experience\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUndergraduate Researcher\u003c/strong\u003e, ITML @ Yonsei (Jan. 2025 – Jun. 2025)\n\u003cul\u003e\n\u003cli\u003eConducting undergraduate research under the supervision of Prof. Jy-yong Sohn.\u003c/li\u003e\n\u003cli\u003eEngaged in ongoing projects related to RAG.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming:\u003c/strong\u003e R, Python, Frontend, Git, MATLAB\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLanguages:\u003c/strong\u003e Korean (Native), English (Intermediate), Chinese (Beginner)\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"}]