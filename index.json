[{"content":"I\u0026rsquo;m sharing my full slide-by-slide review of the paper Promptriever: Instruction-Trained Retrievers, which was presented at ICLR 2025.\nThe paper tackles a big problem in current search models: they often fail to understand complex requests, especially negative ones (like \u0026ldquo;not A, but B\u0026rdquo;). The authors\u0026rsquo; solution is Promptriever, a new model trained with a special dataset that forces it to actually follow instructions.\nThis is my presentation on \u0026ldquo;Promptriever: Instruction-Trained Retrievers,\u0026rdquo; which I put together for the Information Theory and Machine Learning Lab.\nFirst, let\u0026rsquo;s acknowledge the researchers. The lead author is Orion Weller, who is affiliated with Johns Hopkins and Samaya AI. It\u0026rsquo;s also worth noting this work was accepted as a poster at ICLR 2025.\nI want to start with the paper\u0026rsquo;s core claim, which I think is really powerful. The authors state that their new training method is the first to prove that search models can be \u0026ldquo;intelligent, instruction-following partners, not just data finders.\u0026rdquo;\nHere are the other co-authors who contributed to this research.\nI\u0026rsquo;ve structured this review into three parts: Motivation, the Promptriever model, and the Experiments. We\u0026rsquo;ll start with the motivation behind the research.\nSo, how do current search engines \u0026ldquo;think\u0026rdquo;? They mostly use a retriever based on semantic similarity to rank documents. On the surface, this seems fine, but what\u0026rsquo;s the problem?\nThis example makes the problem obvious. Imagine you need a laptop that is not a MacBook and costs under $1000. A standard retriever sees the keywords \u0026ldquo;MacBook\u0026rdquo; and \u0026ldquo;under $1000\u0026rdquo; in an article about the MacBook Air and incorrectly flags it as highly relevant.\nThis leads to a frustrating user experience. You\u0026rsquo;re forced to keep tweaking keywords and filters just to find what you want.\nThis is where Promptriever comes in. It doesn\u0026rsquo;t just use semantic similarity. Instead, it operates on \u0026ldquo;dynamic relevance definitions,\u0026rdquo; which allows for a much more intelligent process.\nLet\u0026rsquo;s look at the same query again, but with Promptriever. It correctly understands the instructions—the core topic, the exclusion of MacBooks, and the price constraint. Because of this, it successfully returns a relevant document about a Dell XPS 13.\nThe key idea here is that Promptriever \u0026ldquo;dynamically adjusts relevance based on your natural language instructions.\u0026rdquo; It\u0026rsquo;s not just matching words; it\u0026rsquo;s understanding commands.\nWe\u0026rsquo;ve seen what it does, which leads to the next question: \u0026ldquo;But how on earth was this made?\u0026rdquo; Let\u0026rsquo;s get into the technical details.\nNow, we\u0026rsquo;ll dive into the second section, where I\u0026rsquo;ll break down the Promptriever model\u0026rsquo;s architecture and training.\nThe architecture is a combination of the LLaMA-2 7B language model and a Bi-encoder.\nThe main technical hurdle they faced is a well-known one: standard fine-tuning for information retrieval often destroys a model\u0026rsquo;s instruction-following ability. So how did they keep the model intelligent?\nThe answer, as they stated in their core message, lies in their \u0026ldquo;novel training data, which makes ignoring commands impossible for correct answers.\u0026rdquo;\nFor context, standard retrieval models are trained on simple (Query, Document) pairs from datasets like MSMARCO.\nPromptriever, however, uses a much richer format: Query + Instruction paired with Synthetic documents. This is what lets them train the model on complex, instruction-based prompts.\nThe most clever part of their training data is the Instruction-Negative. This is a document that\u0026rsquo;s correct for the query alone, but becomes incorrect when the instruction is added. This is what forces the model to pay attention.\nHere’s a perfect example. For the query \u0026ldquo;What is the capital of France?,\u0026rdquo; a general article about Paris is a good result. But if you add the instruction \u0026ldquo;mention its average annual rainfall,\u0026rdquo; that article is now an instruction-negative, and a new document with rainfall data becomes the right answer.\nThrough this process, the model learns that if it ignores the instruction, it will retrieve the wrong results. To get the right answer, it has to carefully read and follow the command.\nThe authors were careful about quality control. They found that about 15% of their generated instructions made the original document irrelevant. For those cases, they used an LLM to generate a new, correct document as a substitute.\nCreating these instruction-negatives was absolutely essential. Without them, the model could have just learned to ignore the instructions and still perform well on the base dataset. The negatives guarantee true instruction-following.\nNow for the final section, \u0026ldquo;Experiments,\u0026rdquo; where we\u0026rsquo;ll look at the results.\nFor a fair comparison, they ran an \u0026ldquo;Apples-to-Apples\u0026rdquo; test against RepLLaMA, using the exact same data and hyperparameters.\nThey used a range of datasets and evaluated performance with metrics like NDCG@10, MRR, and importantly, p-MRR, which is designed to measure sensitivity to instructions.\nThe results were impressive. In short, Promptriever achieved state-of-the-art performance, showed better robustness, and could be improved zero-shot just by prompting.\nThis table gives a detailed breakdown. You can see that Promptriever gets high scores across the board, but it really shines in the p-MRR metric, which confirms its superior instruction-following ability.\nOn the in-domain MSMARCO dataset, the performance was on par with the strong RepLLaMA baseline. This is great because it shows that the model gained its new skills without sacrificing core retrieval performance.\nThis is where it gets interesting. When given a helpful prompt, Promptriever\u0026rsquo;s performance on out-of-domain datasets actually improves, while other models get worse. This proves that it is genuinely \u0026ldquo;promptable.\u0026rdquo;\nThis table looks at the standard deviation of scores across different prompts. Promptriever\u0026rsquo;s lower deviation means its performance is much more stable and consistent, regardless of how the query is phrased.\nThe ablation study confirms it all. The performance gains are a direct result of the instruction-based training with instruction-negatives, not because of other factors like longer queries.\nThe authors also proved their training \u0026ldquo;recipe\u0026rdquo; is general. It works well on other base models like Mistral and Llama 3, not just LLaMA-2. As they put it, \u0026ldquo;A golden recipe doesn\u0026rsquo;t discriminate against ingredients!\u0026rdquo;\nFinally, let\u0026rsquo;s look at the reviewer feedback. When one reviewer claimed the data comparison was unfair, the authors argued that the data generation method is their core contribution. They also clarified that comparing to techniques like query rewriting was out of the paper\u0026rsquo;s scope.\nIn the end, the authors addressed all concerns. They ran the requested statistical tests and added more real-world examples during the rebuttal period, which satisfied the reviewers and got the paper accepted.\n","permalink":"https://mookjsi.github.io/posts/paper-review-promptriever/","summary":"A review of the ICLR 2025 paper, Promptriever. In this post, I break down the core concepts, training methods, and results of a new retriever that\u0026rsquo;s trained to follow natural language instructions.","title":"Promptriever - Instruction-Trained Retrievers"},{"content":"This is a foundational paper from NIPS 1994 that introduced an idea that has become highly relevant again in the modern deep learning era: the connection between the geometry of the loss landscape and a model\u0026rsquo;s ability to generalize.\nThe core idea is simple yet powerful: instead of just finding the lowest point of the error function (the minimum), we should actively search for wide, flat regions. A model that corresponds to a flat minimum is less sensitive to small changes in its weights, which often translates to better performance on unseen data.\nThis is the title slide for my presentation on \u0026ldquo;Simplifying neural nets by discovering flat minima,\u0026rdquo; which I prepared for our lab meeting.\nFirst, let\u0026rsquo;s credit the authors: Sepp Hochreiter and Jürgen Schmidhuber. As you can see, this is a fairly old paper, but its insights are timeless and arguably more important than ever given the flood of modern research. The key question is, what can we learn from it today?\nHere are the authors of the paper. This work was originally presented at the NIPS 1994 conference (now known as NeurIPS).\nThis quote from the authors perfectly captures the paper\u0026rsquo;s central thesis: \u0026ldquo;Find wide, not sharp, minima - and your network generalizes for free.\u0026rdquo; It elegantly states that good generalization is a natural consequence of the shape of the solution space we find.\nMy presentation is structured into three main parts: First, the motivation for why we need a new approach. Second, a deeper dive into the core idea of flat minima. And finally, the specific algorithm they developed to find them.\nSo, let\u0026rsquo;s start with the motivation. Why was this research necessary?\nTo understand the paper\u0026rsquo;s contribution, it helps to look at the historical context. In the years leading up to 1994, popular techniques for improving generalization like Weight Decay and Optimal Brain Surgeon were based on making assumptions about the network\u0026rsquo;s weights (so-called \u0026ldquo;priors\u0026rdquo;). This paper marked a shift, arguing that we should focus on the geometry of the error surface rather than imposing assumptions about the weights themselves.\nThis 3D plot illustrates the core concept perfectly. On the right, we see a \u0026ldquo;sharp\u0026rdquo; minimum. While the loss is low at the very bottom, a small perturbation to the weights can cause a dramatic increase in loss. On the left, we have a \u0026ldquo;flat\u0026rdquo; minimum. Here, the loss remains low across a large connected region of the weight space. The hypothesis is that solutions in these flat regions are more robust and generalize better.\nHere\u0026rsquo;s a simple visual analogy. A sharp minimum is like a deep, narrow canyon. It\u0026rsquo;s difficult to land exactly at the bottom, and any small error puts you high up on the canyon walls. A flat minimum is like a wide, open valley. It\u0026rsquo;s much easier to find a good spot, and moving around a little doesn\u0026rsquo;t drastically change your altitude (or your model\u0026rsquo;s error). This robustness is linked to lower model complexity.\nSo, what was wrong with the existing methods? Weight-Decay, for instance, assumes a Gaussian prior and can sometimes shrink important weights too aggressively. Bayesian methods require you to hand-pick a \u0026ldquo;good\u0026rdquo; prior distribution. And methods like Optimal Brain Surgeon, while elegant, were very slow and memory-intensive because they required inverting the full Hessian matrix.\nThe \u0026ldquo;Flat Minima\u0026rdquo; approach offers solutions to these problems. First, it doesn\u0026rsquo;t require any pre-chosen priors; the geometry of the solution space itself defines what a \u0026ldquo;simple\u0026rdquo; model is. Second, it uses a clever computational method that makes it aware of second-order information but keeps the complexity on the same order as standard back-propagation. Finally, this process naturally prunes unnecessary weights, leading to a simpler model.\nNow, let\u0026rsquo;s formalize the problem by defining the tasks and architectures this method applies to.\nThe basic setup is a standard supervised learning problem. We have a set of inputs and outputs, and our training data consists of input-output pairs where the outputs have been perturbed by some noise.\nThe model is a neural network, represented by the function $f_w(x_p)$, which takes an input $x_p$ and produces an output, parameterized by a set of weights $W$. We measure its performance on the training set using the Mean Squared Error (MSE).\nBuilding on that, we can define the set of \u0026ldquo;acceptable\u0026rdquo; solutions. Given a tolerable error level, $E_{tol}$, the acceptable minimum is the entire set of weight vectors w for which the training MSE is less than or equal to this tolerance.\nNow we get to the core of the algorithm\u0026rsquo;s construction. For a given weight vector w, we define a \u0026ldquo;box\u0026rdquo; around it. For each individual weight $w_{ij}$, we find the maximum amount $\\delta$ it can be perturbed before the training error exceeds our tolerance $E_{tol}$. This gives us an interval $\\Delta w_{ij}$ for each weight.\nThese intervals for all the weights combine to form a high-dimensional hyper-cuboid in the weight space. The paper defines the \u0026ldquo;Flat Minima\u0026rdquo; as the volume of this box. The larger the volume, the flatter the minimum, and the more robust the solution.\nSo, how do we actually find these large-volume minima? That brings us to the algorithm itself.\nThe main objective of the algorithm is to maximize the volume of the box in weight space, which is represented as $\\Delta w$. A larger volume signifies a flatter, more desirable minimum.\nThis slide reiterates the goal, explicitly showing the formula for the box volume and reminding us that each edge of the box, $\\Delta w_{ij}$, is defined by how much a weight can change before the error surpasses a set tolerance.\nMaximizing a product of many terms is difficult. A standard trick is to instead minimize the negative logarithm of the value. Here, we shift our objective from maximizing the volume $\\Delta w$ to minimizing $B(w, D_0)$, which is proportional to the negative log of that volume.\nThis new objective function has a nice connection to the Minimum Description Length (MDL) principle. Minimizing this term is equivalent to finding a set of weights that can be described with the fewest number of bits, which corresponds to a simpler model.\nTo build the algorithm, we first need to mathematically define \u0026ldquo;flatness\u0026rdquo;. We start by defining the change in the network\u0026rsquo;s output, $ED(w, \\delta w)$, that results from a small change in weights, $\\delta w$.\nTo make this expression for output change usable, we approximate it using a first-order Taylor expansion. This allows us to express the new output, $o_k(w + \\delta w)$, in terms of the original output and the first derivatives (gradients).\nSubstituting the Taylor expansion back into our definition of output change gives us an approximate formula that depends on the sum of gradients multiplied by the weight changes.\nThis leads to our first flatness condition: for a minimum to be considered flat, the total change in output resulting from a weight perturbation must be less than or equal to some small constant, c. This ensures that small weight changes don\u0026rsquo;t lead to large output changes.\nThe second condition is designed to maximize the volume of our hyper-cuboid. To do this, we want to make the box as \u0026ldquo;spherical\u0026rdquo; as possible by ensuring that perturbations to each weight contribute equally to the total output change.\nHere is a clearer statement of Flatness Condition 2. It sets an equality: the output change caused by perturbing weight $w_{ij}$ should be equal to the output change caused by perturbing any other weight $w_{uv}$.\nBy rearranging the equation from Condition 2, we can express the allowable perturbation for one weight, $|\\delta w_{ij}|$, in terms of the perturbation of another weight and the ratio of their sensitivities (measured by their output gradients).\nThis slide simply presents both flatness conditions together, showing how they combine to define the properties of the solution we are seeking.\nBy solving the system of equations defined by both flatness conditions, we arrive at a final formula for the maximum allowable perturbation for any given weight, $|\\Delta w_{ij}|$. This formula depends on the network\u0026rsquo;s output gradients.\nLet\u0026rsquo;s quickly recap the algorithm\u0026rsquo;s goal. We aim to maximize the box volume $\\Delta w$, which is equivalent to minimizing the MDL cost function $B(w, D_0)$.\nNow that we have a formula for the size of the box edges, $\\Delta w_{ij}$, we can express our cost function $B(w, D_0)$ in terms of the network\u0026rsquo;s derivatives. This slide shows that connection, approximating the log of $\\Delta w_{ij}$ with the log of the derived formula.\nThis slide restates the formula for the size of the perturbation $|\\Delta w_{ij}|$, which is the key result from our derivation using the two flatness conditions.\nPlugging the expression for $\\Delta w_{ij}$ into our cost function $B(w, D_0) = -\\sum \\log \\Delta w_{ij}$ gives us this final, albeit complex-looking, penalty term that we need to minimize. This term explicitly captures the \u0026ldquo;flatness\u0026rdquo; of the minimum.\nThe complete objective function for training is then a combination of two parts: the standard MSE, which ensures the model fits the training data, and our new flatness penalty term, which encourages the model to find a simple, generalizable solution. A hyperparameter $\\lambda$ balances the two.\nTo minimize this objective function using gradient descent, we need to compute its gradient. The gradient of the flatness penalty term involves second-order derivatives of the network\u0026rsquo;s output, which would typically be very expensive to compute.\nHowever, the paper leverages a crucial insight. This complex gradient, which involves second-order information, can be calculated with a computational complexity of $O(W)$—the same as standard backpropagation—using a technique known as the Pearlmutter trick. This makes the entire algorithm practical and efficient.\nTo recap the key advantages: the \u0026ldquo;Flat Minima\u0026rdquo; method is appealing because it uses geometry instead of priors to define simplicity, it\u0026rsquo;s computationally efficient, and it naturally performs network pruning for better generalization.\nThe paper then presents three experiments to prove the effectiveness of the algorithm. These tests cover noisy classification, a recurrent network task, and a real-world regression problem using stock market data.\nIn the first experiment, the task was to classify a 2D point with both label and input noise. The network was trained on a small set of 200 samples and tested on a very large set of 120,000 samples to reliably measure generalization.\nThe results of the first experiment are shown here. This table presents 10 direct comparisons between conventional backprop and the new FMS approach. In every single run, the new method achieves a lower test error and gets significantly closer to the optimal error rate, clearly demonstrating superior generalization.\nThe second experiment used a recurrent neural network for a sequence classification task. The problem was designed to be solvable with just one hidden unit. While backprop failed to prune the redundant second unit, the FMS method successfully suppressed it, demonstrating its automatic model simplification capability.\nThe final experiment tackled a real-world problem: predicting directional changes in the DAX stock index. They used several sets of features, from fundamental economic indicators to technical trading signals.\nThe results from the stock market prediction task were compelling. The FMS method was benchmarked against standard Backprop, Optimal Brain Surgeon (OBS), and Weight Decay. FMS was better across all metrics and feature sets, achieving up to a 63% relative improvement over the best competitor, proving its value on noisy, real-world data.\n","permalink":"https://mookjsi.github.io/posts/paper-review-flatminima/","summary":"A detailed review of the classic 1994 paper by Hochreiter \u0026amp; Schmidhuber, \u0026lsquo;Simplifying Neural Nets by Discovering Flat Minima\u0026rsquo;. In this post, I break down the core concepts, the proposed algorithm, and the experimental results that demonstrate why seeking flat minima leads to better generalization.","title":"Simplifying Neural Nets by Discovering Flat Minima"},{"content":"Education Yonsei University, Seoul, Korea Senior @ Dept. of Applied Statistics GPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics) Research Interests Statistical Machine Learning Large Language Model Retrieval Method Research Experience Undergraduate Researcher, ITML @ Yonsei (Jan. 2025 – Jun. 2025) Conducting undergraduate research under the supervision of Prof. Jy-yong Sohn. Engaged in ongoing projects related to RAG. Skills Programming: R, Python, Frontend, Git, MATLAB Languages: Korean (Native), English (Intermediate), Chinese (Beginner) ","permalink":"https://mookjsi.github.io/about/","summary":"\u003ch2 id=\"education\"\u003eEducation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eYonsei University\u003c/strong\u003e, Seoul, Korea\n\u003cul\u003e\n\u003cli\u003eSenior @ Dept. of Applied Statistics\u003c/li\u003e\n\u003cli\u003eGPA: 4.16/4.3 (Overall), 4.24/4.3 (Statistics)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-interests\"\u003eResearch Interests\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical Machine Learning\u003c/li\u003e\n\u003cli\u003eLarge Language Model\u003c/li\u003e\n\u003cli\u003eRetrieval Method\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"research-experience\"\u003eResearch Experience\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUndergraduate Researcher\u003c/strong\u003e, ITML @ Yonsei (Jan. 2025 – Jun. 2025)\n\u003cul\u003e\n\u003cli\u003eConducting undergraduate research under the supervision of Prof. Jy-yong Sohn.\u003c/li\u003e\n\u003cli\u003eEngaged in ongoing projects related to RAG.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"skills\"\u003eSkills\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgramming:\u003c/strong\u003e R, Python, Frontend, Git, MATLAB\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLanguages:\u003c/strong\u003e Korean (Native), English (Intermediate), Chinese (Beginner)\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"}]